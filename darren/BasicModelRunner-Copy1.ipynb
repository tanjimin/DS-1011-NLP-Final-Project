{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BasicModel\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, tensorToList\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "zh, en = loadLangPairs(\"zh\")\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = langDataset([(zh.train_num[i], en.train_num[i]) for i in range(len(zh.train_num)) if (len(zh.train[i]) < zh.max_length) & (len(en.train[i]) < en.max_length)])\n",
    "# overfit_dataset = langDataset([(zh.train_num[i], en.train_num[i]) for i in range(32)])\n",
    "# overfit_loader = torch.utils.data.DataLoader(dataset=overfit_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=langCollateFn,\n",
    "#                                            shuffle=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "dev_dataset = langDataset([(zh.dev_num[i], en.dev_num[i]) for i in range(len(zh.dev_num)) if (len(zh.dev[i]) < zh.max_length) & (len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "## Add ignore index\n",
    "zh_criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "zh_encoder = BasicModel.EncoderRNN(input_size = zh.n_words, hidden_size = HIDDEN_SIZE, num_layers = 1, batch_size = BATCH_SIZE, raw_emb=zh.emb, learn_ids=zh.learn_ids).to(device)\n",
    "zh_decoder = BasicModel.DecoderRNN(hidden_size = HIDDEN_SIZE, output_size = en.n_words, num_layers = 1, batch_size = BATCH_SIZE, raw_emb=en.emb, learn_ids=en.learn_ids).to(device)\n",
    "\n",
    "zh_encoder_optimizer = optim.SGD(zh_encoder.parameters(), lr=learning_rate)\n",
    "zh_decoder_optimizer = optim.SGD(zh_decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, dev_loader, encoder, decoder, encoder_opt, decoder_opt, criterion, batch_size, epochs, print_every, hidden_size):\n",
    "    start = time.time()\n",
    "    print('Initializing Model Training + Eval...')\n",
    "    losses = []\n",
    "    train_scores = []\n",
    "    dev_scores = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "            if (len(inp[0]) != batch_size):\n",
    "                continue\n",
    "            inp.transpose_(0,1)\n",
    "            output.transpose_(0,1)\n",
    "            inp = inp.to(device)\n",
    "            output = output.to(device)\n",
    "            loss += BasicModel.train(inp, output, out_max, encoder, decoder, encoder_opt, decoder_opt, criterion, batch_size, hidden_size)\n",
    "            if i % print_every == 0 and i > 0:\n",
    "                losses.append(loss/i)\n",
    "                print(\"Time Elapsed: {} | Loss: {:.4}\".format(asMinutes(time.time() - start),\n",
    "                                                                                loss/i))\n",
    "                pkl.dump(encoder, open(\"./zh-g-base-encoder-sgd0.01.p\", \"wb\"))\n",
    "                pkl.dump(decoder, open(\"./zh-g-base-decoder-sgd0.01.p\", \"wb\"))\n",
    "        train_score = BasicModel.bleuEval(encoder, decoder, train_loader, batch_size, hidden_size)\n",
    "        train_scores.append(train_score)\n",
    "        print(\"Epoch: {} | Time Elapsed: {} | Loss: {:.4} | Train BLEU: {:.4}\".format(epoch + 1, \n",
    "                                                                                                        asMinutes(time.time() - start),\n",
    "                                                                                                        loss/len(train_loader), \n",
    "                                                                                                        train_score)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleuEval(encoder, decoder, data_loader, batch_size):\n",
    "    with torch.no_grad():\n",
    "        true_outputs = []\n",
    "        decoder_outputs = []\n",
    "        for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(data_loader):\n",
    "            if i * batch_size >= 10000 or len(inp[0]) != batch_size:\n",
    "                continue\n",
    "            inp = inp.transpose(0,1).to(device)\n",
    "            output = output.transpose(0,1).to(device)\n",
    "            true_outputs.append([[str(tok.item()) for tok in out if tok != 0] for out in output])\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            input_len = inp.shape[1]\n",
    "            encoder_outputs = torch.zeros(input_len, batch_size, 1, HIDDEN_SIZE, device=device)\n",
    "            encoder_hiddens = torch.zeros(input_len, 1, batch_size, HIDDEN_SIZE, device=device)\n",
    "\n",
    "            # Encode\n",
    "            for ec_idx in range(input_len):\n",
    "                # input batch_size * 1\n",
    "                encoder_output, encoder_hidden = encoder(inp[:, ec_idx].unsqueeze(1), encoder_hidden)\n",
    "                encoder_outputs[ec_idx] = encoder_output\n",
    "                encoder_hiddens[ec_idx] = encoder_hidden\n",
    "\n",
    "            # Decode\n",
    "            decoder_input = torch.tensor([SOS_ID] * batch_size, device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            # Greedy\n",
    "            for dc_idx in range(out_max):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input.unsqueeze(1), decoder_hidden)\n",
    "                decoder_output = decoder_output.squeeze(1).to(device) # get rid of the seq dimention\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = torch.LongTensor([topi[i][0] for i in range(inp.size(0))]).to(device)\n",
    "                ## Print Value\n",
    "                decoder_outputs.append(list(decoder_input.cpu().numpy()))\n",
    "            ## Print Value\n",
    "        predict = []\n",
    "        for seq in np.array(decoder_outputs).T.astype(str):\n",
    "            seq_toks = []\n",
    "            for tok in seq:\n",
    "                seq_toks.append(tok)\n",
    "                if tok == '3':\n",
    "                    break\n",
    "            predict.append(seq_toks)\n",
    "#         print('Sample True: ', ' '.join([en.id2word[int(i)] for i in true_outputs[0][0]]))\n",
    "#         print('Sample Predicted: ', ' '.join([en.id2word[int(i)] for i in predict[0]]))\n",
    "#         for seq in predict:\n",
    "#             print('Sample Predicted: ', ' '.join([en.id2word[int(i)] for i in seq]))\n",
    "        bleu_score = corpus_bleu(predict, true_outputs, 4)\n",
    "        return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi, en = loadLangPairs(\"vi\")\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(len(vi.train_num)) if (2 < len(vi.train[i]) < vi.max_length) & (2 < len(en.train[i]) < en.max_length)])\n",
    "overfit_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(32)])\n",
    "overfit_loader = torch.utils.data.DataLoader(dataset=overfit_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "dev_dataset = langDataset([(vi.dev_num[i], en.dev_num[i]) for i in range(len(vi.dev_num)) if (2 < len(vi.dev[i]) < vi.max_length) & (2 < len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'EncoderRNN' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-44b157477853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hybrid-vi-encoder.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hybrid-vi-decoder.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'EncoderRNN' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "encoder = pkl.load(open('hybrid-vi-encoder.p', 'rb'))\n",
    "decoder = pkl.load(open('hybrid-vi-decoder.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleuEval(encoder, decoder, dev_loader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleuEvalAttention(encoder, decoder, dev_loader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model Training + Eval...\n",
      "Time Elapsed: 4m 38s | Loss: 6.943\n",
      "Time Elapsed: 9m 13s | Loss: 6.334\n",
      "Time Elapsed: 13m 51s | Loss: 6.062\n",
      "Time Elapsed: 18m 27s | Loss: 5.899\n",
      "Time Elapsed: 23m 4s | Loss: 5.785\n",
      "Time Elapsed: 27m 40s | Loss: 5.693\n",
      "Time Elapsed: 32m 16s | Loss: 5.614\n",
      "Time Elapsed: 36m 54s | Loss: 5.549\n",
      "Time Elapsed: 41m 35s | Loss: 5.494\n",
      "Time Elapsed: 46m 13s | Loss: 5.444\n",
      "Time Elapsed: 50m 49s | Loss: 5.401\n",
      "Time Elapsed: 55m 25s | Loss: 5.363\n",
      "Time Elapsed: 60m 4s | Loss: 5.328\n",
      "Time Elapsed: 64m 41s | Loss: 5.296\n",
      "Time Elapsed: 69m 17s | Loss: 5.267\n",
      "Time Elapsed: 73m 55s | Loss: 5.24\n",
      "Time Elapsed: 78m 29s | Loss: 5.214\n",
      "Time Elapsed: 83m 4s | Loss: 5.191\n",
      "Epoch: 1 | Time Elapsed: 90m 44s | Loss: 5.187 | Train BLEU: 2.168\n",
      "Time Elapsed: 93m 54s | Loss: 4.779\n",
      "Time Elapsed: 97m 10s | Loss: 4.767\n",
      "Time Elapsed: 100m 25s | Loss: 4.757\n",
      "Time Elapsed: 104m 14s | Loss: 4.749\n",
      "Time Elapsed: 108m 50s | Loss: 4.744\n",
      "Time Elapsed: 113m 25s | Loss: 4.736\n",
      "Time Elapsed: 118m 3s | Loss: 4.729\n",
      "Time Elapsed: 122m 40s | Loss: 4.724\n",
      "Time Elapsed: 127m 17s | Loss: 4.717\n",
      "Time Elapsed: 131m 53s | Loss: 4.714\n",
      "Time Elapsed: 136m 30s | Loss: 4.71\n",
      "Time Elapsed: 141m 1s | Loss: 4.705\n",
      "Time Elapsed: 145m 38s | Loss: 4.699\n",
      "Time Elapsed: 150m 12s | Loss: 4.693\n",
      "Time Elapsed: 154m 48s | Loss: 4.688\n",
      "Time Elapsed: 159m 24s | Loss: 4.684\n",
      "Time Elapsed: 164m 0s | Loss: 4.68\n",
      "Time Elapsed: 168m 36s | Loss: 4.675\n",
      "Epoch: 2 | Time Elapsed: 177m 51s | Loss: 4.673 | Train BLEU: 1.129\n",
      "Time Elapsed: 182m 20s | Loss: 4.586\n",
      "Time Elapsed: 186m 57s | Loss: 4.577\n",
      "Time Elapsed: 191m 32s | Loss: 4.575\n",
      "Time Elapsed: 195m 0s | Loss: 4.572\n",
      "Time Elapsed: 198m 14s | Loss: 4.569\n",
      "Time Elapsed: 201m 27s | Loss: 4.563\n",
      "Time Elapsed: 204m 46s | Loss: 4.561\n",
      "Time Elapsed: 208m 1s | Loss: 4.558\n",
      "Time Elapsed: 212m 6s | Loss: 4.554\n",
      "Time Elapsed: 216m 41s | Loss: 4.552\n",
      "Time Elapsed: 221m 18s | Loss: 4.549\n",
      "Time Elapsed: 225m 56s | Loss: 4.547\n",
      "Time Elapsed: 230m 31s | Loss: 4.544\n",
      "Time Elapsed: 235m 6s | Loss: 4.54\n",
      "Time Elapsed: 239m 39s | Loss: 4.537\n",
      "Time Elapsed: 244m 13s | Loss: 4.534\n",
      "Time Elapsed: 248m 47s | Loss: 4.531\n",
      "Time Elapsed: 253m 22s | Loss: 4.528\n",
      "Epoch: 3 | Time Elapsed: 262m 36s | Loss: 4.526 | Train BLEU: 1.214\n",
      "Time Elapsed: 266m 51s | Loss: 4.469\n",
      "Time Elapsed: 271m 26s | Loss: 4.465\n",
      "Time Elapsed: 276m 1s | Loss: 4.469\n",
      "Time Elapsed: 280m 35s | Loss: 4.466\n",
      "Time Elapsed: 285m 10s | Loss: 4.463\n",
      "Time Elapsed: 289m 44s | Loss: 4.457\n",
      "Time Elapsed: 294m 19s | Loss: 4.456\n",
      "Time Elapsed: 298m 45s | Loss: 4.454\n",
      "Time Elapsed: 301m 59s | Loss: 4.451\n",
      "Time Elapsed: 305m 13s | Loss: 4.45\n",
      "Time Elapsed: 308m 26s | Loss: 4.448\n",
      "Time Elapsed: 311m 39s | Loss: 4.445\n",
      "Time Elapsed: 314m 58s | Loss: 4.443\n",
      "Time Elapsed: 319m 34s | Loss: 4.441\n",
      "Time Elapsed: 324m 10s | Loss: 4.44\n",
      "Time Elapsed: 328m 47s | Loss: 4.438\n",
      "Time Elapsed: 333m 22s | Loss: 4.434\n",
      "Time Elapsed: 337m 57s | Loss: 4.432\n",
      "Epoch: 4 | Time Elapsed: 347m 20s | Loss: 4.43 | Train BLEU: 0.919\n",
      "Time Elapsed: 351m 19s | Loss: 4.389\n",
      "Time Elapsed: 355m 57s | Loss: 4.387\n",
      "Time Elapsed: 360m 34s | Loss: 4.383\n",
      "Time Elapsed: 365m 11s | Loss: 4.376\n",
      "Time Elapsed: 369m 50s | Loss: 4.377\n",
      "Time Elapsed: 374m 28s | Loss: 4.371\n",
      "Time Elapsed: 379m 6s | Loss: 4.369\n",
      "Time Elapsed: 383m 41s | Loss: 4.367\n",
      "Time Elapsed: 388m 18s | Loss: 4.363\n",
      "Time Elapsed: 392m 54s | Loss: 4.36\n",
      "Time Elapsed: 397m 33s | Loss: 4.359\n",
      "Time Elapsed: 402m 11s | Loss: 4.357\n",
      "Time Elapsed: 406m 15s | Loss: 4.357\n",
      "Time Elapsed: 409m 32s | Loss: 4.356\n",
      "Time Elapsed: 412m 49s | Loss: 4.353\n",
      "Time Elapsed: 416m 5s | Loss: 4.352\n",
      "Time Elapsed: 419m 22s | Loss: 4.351\n",
      "Time Elapsed: 422m 56s | Loss: 4.349\n",
      "Epoch: 5 | Time Elapsed: 432m 56s | Loss: 4.347 | Train BLEU: 1.825\n",
      "Time Elapsed: 436m 41s | Loss: 4.314\n",
      "Time Elapsed: 441m 18s | Loss: 4.302\n",
      "Time Elapsed: 445m 56s | Loss: 4.304\n",
      "Time Elapsed: 450m 33s | Loss: 4.302\n",
      "Time Elapsed: 455m 11s | Loss: 4.302\n",
      "Time Elapsed: 459m 49s | Loss: 4.302\n",
      "Time Elapsed: 464m 27s | Loss: 4.298\n",
      "Time Elapsed: 469m 4s | Loss: 4.296\n",
      "Time Elapsed: 473m 39s | Loss: 4.294\n",
      "Time Elapsed: 478m 17s | Loss: 4.293\n",
      "Time Elapsed: 482m 55s | Loss: 4.291\n",
      "Time Elapsed: 487m 34s | Loss: 4.288\n",
      "Time Elapsed: 492m 9s | Loss: 4.288\n",
      "Time Elapsed: 496m 45s | Loss: 4.284\n",
      "Time Elapsed: 501m 19s | Loss: 4.282\n",
      "Time Elapsed: 505m 54s | Loss: 4.279\n",
      "Time Elapsed: 510m 27s | Loss: 4.278\n",
      "Time Elapsed: 513m 59s | Loss: 4.275\n",
      "Epoch: 6 | Time Elapsed: 519m 5s | Loss: 4.274 | Train BLEU: 2.201\n",
      "Time Elapsed: 522m 23s | Loss: 4.251\n",
      "Time Elapsed: 526m 10s | Loss: 4.244\n",
      "Time Elapsed: 530m 47s | Loss: 4.244\n",
      "Time Elapsed: 535m 21s | Loss: 4.237\n",
      "Time Elapsed: 539m 57s | Loss: 4.233\n",
      "Time Elapsed: 544m 31s | Loss: 4.234\n",
      "Time Elapsed: 549m 5s | Loss: 4.233\n",
      "Time Elapsed: 553m 39s | Loss: 4.232\n",
      "Time Elapsed: 558m 14s | Loss: 4.23\n",
      "Time Elapsed: 562m 47s | Loss: 4.23\n",
      "Time Elapsed: 567m 23s | Loss: 4.228\n",
      "Time Elapsed: 571m 58s | Loss: 4.227\n",
      "Time Elapsed: 576m 31s | Loss: 4.226\n",
      "Time Elapsed: 581m 5s | Loss: 4.226\n",
      "Time Elapsed: 585m 40s | Loss: 4.225\n",
      "Time Elapsed: 590m 14s | Loss: 4.225\n",
      "Time Elapsed: 594m 48s | Loss: 4.223\n",
      "Time Elapsed: 599m 25s | Loss: 4.222\n",
      "Epoch: 7 | Time Elapsed: 609m 51s | Loss: 4.22 | Train BLEU: 1.384\n",
      "Time Elapsed: 612m 6s | Loss: 4.198\n",
      "Time Elapsed: 615m 19s | Loss: 4.189\n",
      "Time Elapsed: 618m 37s | Loss: 4.183\n",
      "Time Elapsed: 623m 10s | Loss: 4.182\n",
      "Time Elapsed: 627m 43s | Loss: 4.184\n",
      "Time Elapsed: 632m 21s | Loss: 4.185\n",
      "Time Elapsed: 636m 55s | Loss: 4.181\n",
      "Time Elapsed: 641m 32s | Loss: 4.181\n",
      "Time Elapsed: 646m 5s | Loss: 4.181\n",
      "Time Elapsed: 650m 39s | Loss: 4.179\n",
      "Time Elapsed: 655m 14s | Loss: 4.179\n",
      "Time Elapsed: 659m 49s | Loss: 4.177\n",
      "Time Elapsed: 664m 22s | Loss: 4.176\n",
      "Time Elapsed: 668m 56s | Loss: 4.174\n",
      "Time Elapsed: 673m 30s | Loss: 4.174\n",
      "Time Elapsed: 678m 4s | Loss: 4.173\n",
      "Time Elapsed: 682m 39s | Loss: 4.172\n",
      "Time Elapsed: 687m 15s | Loss: 4.171\n",
      "Epoch: 8 | Time Elapsed: 698m 3s | Loss: 4.17 | Train BLEU: 1.615\n",
      "Time Elapsed: 701m 23s | Loss: 4.142\n",
      "Time Elapsed: 705m 45s | Loss: 4.141\n",
      "Time Elapsed: 709m 20s | Loss: 4.141\n",
      "Time Elapsed: 712m 34s | Loss: 4.141\n",
      "Time Elapsed: 715m 47s | Loss: 4.136\n",
      "Time Elapsed: 719m 0s | Loss: 4.137\n",
      "Time Elapsed: 722m 12s | Loss: 4.138\n",
      "Time Elapsed: 726m 6s | Loss: 4.134\n",
      "Time Elapsed: 730m 42s | Loss: 4.133\n",
      "Time Elapsed: 735m 16s | Loss: 4.133\n",
      "Time Elapsed: 739m 52s | Loss: 4.131\n",
      "Time Elapsed: 744m 28s | Loss: 4.13\n",
      "Time Elapsed: 749m 4s | Loss: 4.126\n",
      "Time Elapsed: 753m 38s | Loss: 4.125\n",
      "Time Elapsed: 758m 14s | Loss: 4.124\n",
      "Time Elapsed: 762m 50s | Loss: 4.123\n",
      "Time Elapsed: 767m 27s | Loss: 4.122\n",
      "Time Elapsed: 772m 3s | Loss: 4.12\n",
      "Epoch: 9 | Time Elapsed: 783m 27s | Loss: 4.119 | Train BLEU: 0.5835\n",
      "Time Elapsed: 786m 45s | Loss: 4.104\n",
      "Time Elapsed: 791m 12s | Loss: 4.092\n",
      "Time Elapsed: 795m 49s | Loss: 4.093\n",
      "Time Elapsed: 800m 24s | Loss: 4.09\n",
      "Time Elapsed: 805m 0s | Loss: 4.088\n",
      "Time Elapsed: 809m 37s | Loss: 4.089\n",
      "Time Elapsed: 813m 38s | Loss: 4.086\n",
      "Time Elapsed: 816m 53s | Loss: 4.085\n",
      "Time Elapsed: 820m 7s | Loss: 4.085\n",
      "Time Elapsed: 823m 22s | Loss: 4.081\n",
      "Time Elapsed: 826m 36s | Loss: 4.081\n",
      "Time Elapsed: 830m 11s | Loss: 4.08\n",
      "Time Elapsed: 834m 48s | Loss: 4.079\n",
      "Time Elapsed: 839m 25s | Loss: 4.077\n",
      "Time Elapsed: 844m 1s | Loss: 4.076\n",
      "Time Elapsed: 848m 37s | Loss: 4.076\n",
      "Time Elapsed: 853m 15s | Loss: 4.075\n",
      "Time Elapsed: 857m 50s | Loss: 4.075\n",
      "Epoch: 10 | Time Elapsed: 868m 47s | Loss: 4.074 | Train BLEU: 1.251\n",
      "Time Elapsed: 872m 5s | Loss: 4.07\n",
      "Time Elapsed: 876m 12s | Loss: 4.058\n",
      "Time Elapsed: 880m 47s | Loss: 4.053\n",
      "Time Elapsed: 885m 24s | Loss: 4.048\n",
      "Time Elapsed: 890m 0s | Loss: 4.046\n",
      "Time Elapsed: 894m 35s | Loss: 4.043\n",
      "Time Elapsed: 899m 10s | Loss: 4.045\n",
      "Time Elapsed: 903m 45s | Loss: 4.044\n",
      "Time Elapsed: 908m 20s | Loss: 4.041\n",
      "Time Elapsed: 912m 54s | Loss: 4.04\n",
      "Time Elapsed: 917m 30s | Loss: 4.038\n",
      "Time Elapsed: 920m 53s | Loss: 4.038\n",
      "Time Elapsed: 924m 10s | Loss: 4.037\n",
      "Time Elapsed: 927m 25s | Loss: 4.037\n",
      "Time Elapsed: 930m 42s | Loss: 4.035\n",
      "Time Elapsed: 933m 55s | Loss: 4.034\n",
      "Time Elapsed: 938m 5s | Loss: 4.035\n",
      "Time Elapsed: 942m 41s | Loss: 4.034\n",
      "Epoch: 11 | Time Elapsed: 954m 13s | Loss: 4.033 | Train BLEU: 2.536\n",
      "Time Elapsed: 957m 34s | Loss: 4.016\n",
      "Time Elapsed: 961m 28s | Loss: 4.006\n",
      "Time Elapsed: 966m 4s | Loss: 4.007\n",
      "Time Elapsed: 970m 39s | Loss: 4.006\n",
      "Time Elapsed: 975m 16s | Loss: 4.004\n",
      "Time Elapsed: 979m 51s | Loss: 4.004\n",
      "Time Elapsed: 984m 26s | Loss: 4.004\n",
      "Time Elapsed: 989m 2s | Loss: 4.002\n",
      "Time Elapsed: 993m 39s | Loss: 4.003\n",
      "Time Elapsed: 998m 15s | Loss: 4.002\n",
      "Time Elapsed: 1002m 49s | Loss: 4.0\n",
      "Time Elapsed: 1007m 24s | Loss: 3.998\n",
      "Time Elapsed: 1011m 59s | Loss: 3.997\n",
      "Time Elapsed: 1016m 34s | Loss: 3.997\n",
      "Time Elapsed: 1021m 8s | Loss: 3.996\n",
      "Time Elapsed: 1025m 8s | Loss: 3.996\n",
      "Time Elapsed: 1028m 23s | Loss: 3.996\n",
      "Time Elapsed: 1031m 36s | Loss: 3.995\n",
      "Epoch: 12 | Time Elapsed: 1039m 57s | Loss: 3.994 | Train BLEU: 2.956\n",
      "Time Elapsed: 1043m 16s | Loss: 3.979\n",
      "Time Elapsed: 1046m 34s | Loss: 3.975\n",
      "Time Elapsed: 1050m 12s | Loss: 3.974\n",
      "Time Elapsed: 1054m 44s | Loss: 3.969\n",
      "Time Elapsed: 1059m 21s | Loss: 3.967\n",
      "Time Elapsed: 1063m 55s | Loss: 3.964\n",
      "Time Elapsed: 1068m 29s | Loss: 3.962\n",
      "Time Elapsed: 1073m 4s | Loss: 3.964\n",
      "Time Elapsed: 1077m 38s | Loss: 3.966\n",
      "Time Elapsed: 1082m 15s | Loss: 3.965\n",
      "Time Elapsed: 1086m 52s | Loss: 3.964\n",
      "Time Elapsed: 1091m 27s | Loss: 3.963\n",
      "Time Elapsed: 1096m 2s | Loss: 3.962\n",
      "Time Elapsed: 1100m 39s | Loss: 3.96\n",
      "Time Elapsed: 1105m 13s | Loss: 3.96\n",
      "Time Elapsed: 1109m 52s | Loss: 3.96\n",
      "Time Elapsed: 1114m 28s | Loss: 3.961\n",
      "Time Elapsed: 1119m 3s | Loss: 3.96\n",
      "Epoch: 13 | Time Elapsed: 1128m 29s | Loss: 3.959 | Train BLEU: 1.59\n",
      "Time Elapsed: 1130m 53s | Loss: 3.953\n",
      "Time Elapsed: 1134m 13s | Loss: 3.94\n",
      "Time Elapsed: 1137m 32s | Loss: 3.934\n",
      "Time Elapsed: 1141m 41s | Loss: 3.932\n",
      "Time Elapsed: 1146m 17s | Loss: 3.932\n",
      "Time Elapsed: 1150m 54s | Loss: 3.931\n",
      "Time Elapsed: 1155m 30s | Loss: 3.931\n",
      "Time Elapsed: 1160m 7s | Loss: 3.929\n",
      "Time Elapsed: 1164m 42s | Loss: 3.927\n",
      "Time Elapsed: 1169m 18s | Loss: 3.927\n",
      "Time Elapsed: 1173m 51s | Loss: 3.927\n",
      "Time Elapsed: 1178m 27s | Loss: 3.926\n",
      "Time Elapsed: 1183m 3s | Loss: 3.926\n",
      "Time Elapsed: 1187m 37s | Loss: 3.926\n",
      "Time Elapsed: 1192m 11s | Loss: 3.926\n",
      "Time Elapsed: 1196m 44s | Loss: 3.925\n"
     ]
    }
   ],
   "source": [
    "fit(train_loader, dev_loader, zh_encoder, zh_decoder, zh_encoder_optimizer, zh_decoder_optimizer, zh_criterion, BATCH_SIZE, 15, 300, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^output got cut off due to accidentally exiting jupyter notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
