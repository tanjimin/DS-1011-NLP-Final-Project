{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, tensorToList\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dictionaries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pkl.load(f)\n",
    "        return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sentence given numbers\n",
    "def ids2sentence(sentence, dictionary):\n",
    "    return ' '.join([dictionary[i] for i in sentence])\n",
    "#ids2sentence(en_train_num[0], id2word_en_dic)\n",
    "\n",
    "def add_symbol(id2word_dic, word2id_dic):\n",
    "    symbols = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    for i, symbol in enumerate(symbols):\n",
    "        id2word_dic[i] = symbol\n",
    "        word2id_dic[symbol] = i\n",
    "    return id2word_dic, word2id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2word_vi_dic = load_zipped_pickle(\"../embeddings/id2word_vi_dic.p\")\n",
    "# word2id_vi_dic = load_zipped_pickle(\"../embeddings/word2id_vi_dic.p\")\n",
    "\n",
    "# id2word_en_dic = load_zipped_pickle(\"../embeddings/id2word_en_dic.p\")\n",
    "# word2id_en_dic = load_zipped_pickle(\"../embeddings/word2id_en_dic.p\")\n",
    "\n",
    "# id2word_vi_dic, word2id_vi_dic = add_symbol(id2word_vi_dic, word2id_vi_dic)\n",
    "# id2word_en_dic, word2id_en_dic = add_symbol(id2word_en_dic, word2id_en_dic)\n",
    "\n",
    "# vi_train = load_zipped_pickle(\"../data/vi-en-tokens/train_vi_tok.p\")\n",
    "# en_train = load_zipped_pickle(\"../data/vi-en-tokens/train_en_tok.p\") # Already Processed for symbols\n",
    "\n",
    "# vi_train_num = load_zipped_pickle(\"../data/vi-en-tokens/train_vi_tok_num.p\")\n",
    "# en_train_num = load_zipped_pickle(\"../data/vi-en-tokens/train_en_tok_num.p\") # Already Processed for symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi, en = loadLangPairs(\"vi\")\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(len(vi.train_num)) if (len(vi.train[i]) < vi.max_length) & (len(en.train[i]) < en.max_length)])\n",
    "overfit_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(32)])\n",
    "overfit_loader = torch.utils.data.DataLoader(dataset=overfit_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "dev_dataset = langDataset([(vi.dev_num[i], en.dev_num[i]) for i in range(len(vi.dev_num)) if (len(vi.dev[i]) < vi.max_length) & (len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by input data length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sort_by_length(data_input, target_data):\n",
    "#     input_size = [len(data) for data in data_input]\n",
    "#     size_index = np.argsort(input_size)\n",
    "#     return list(np.array(data_input)[size_index]), list(np.array(target_data)[size_index])\n",
    "\n",
    "# vi_train_num, en_train_num = sort_by_length(vi_train_num, en_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Data given batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(data, length):\n",
    "    for i, line in enumerate(data):\n",
    "        if len(line) < length:\n",
    "            for i in range(len(line), length):\n",
    "                line.append(0)\n",
    "        else:\n",
    "            data[i] = line[0:length]\n",
    "    return data\n",
    "\n",
    "# Return the batch data and target\n",
    "def get_batch(i, batch_size, train_data, train_target):\n",
    "    if i * batch_size > len(train_data):\n",
    "        raise Exception('Incorrect batch index')\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "    batch_data = list(np.array(train_data)[start_idx:end_idx])\n",
    "    batch_target = list(np.array(train_target)[start_idx:end_idx])\n",
    "    batch_data = pad(batch_data, len(batch_data[batch_size - 1]))\n",
    "    max_target = max([len(data) for data in batch_data])\n",
    "    batch_target = pad(batch_target, max_target)\n",
    "    return batch_data, batch_target\n",
    "\n",
    "# get_batch(5, 64, vi_train_num, en_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_size, raw_emb, learn_ids):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        # input_size: input dictionary size\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(self.hidden_size, \n",
    "                          hidden_size, \n",
    "                          num_layers= num_layers, \n",
    "                          batch_first = True) # BATCH FIRST\n",
    "\n",
    "    def forward(self, encoder_input, hidden_input):\n",
    "        # encoder_input: batch * 1 (for 1 word each time)\n",
    "        embedded_input = self.embedding(encoder_input)\n",
    "        # embedded_input: batch * 1 * emb_dim\n",
    "        # hidden_input: batch * 1(layer) * hidden_size\n",
    "        output, hidden = self.gru(embedded_input, hidden_input)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, max_length, batch_size, raw_emb, learn_ids, dropout_p=0.1):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        # Max length for a sentence\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, \n",
    "                          self.hidden_size,\n",
    "                          num_layers= num_layers, \n",
    "                          batch_first = True) # BATCH_FRIST)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, decoder_input, hidden_input, encoder_hiddens):\n",
    "        # hidden_input: 1 * batch * hidden_size\n",
    "        hidden_input = hidden_input.squeeze(0)\n",
    "        # decoder_input: batch * 1\n",
    "        embedded_input = self.embedding(decoder_input)\n",
    "        # embedded_input: batch * 1 * embed_size\n",
    "        embedded_input = self.dropout(embedded_input).squeeze(1)\n",
    "        \n",
    "        # embedded_input: batch * embed_size\n",
    "        # hidden_input: batch * hidden_size \n",
    "        # (Use input and newest hidden to decide which encoder hidden is important)\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded_input, hidden_input), 1)), dim=1).unsqueeze(1)\n",
    "        # encoder_output: max_length * batch * encoder_hidden_size\n",
    "        encoder_hiddens_t = encoder_hiddens.transpose(0, 1)\n",
    "        # attn_weights: batch * 1 * max_length(theoretical)\n",
    "        cropped_attn_weights = attn_weights[:, :, :encoder_hiddens_t.shape[1]]\n",
    "        # cropped_attn_weights: batch * 1 * max_length(actual)\n",
    "        # encoder_hiddens_t: batch * max_length(actual) * encoder_hidden_size\n",
    "        ## \n",
    "        attn_applied = torch.bmm(cropped_attn_weights, encoder_hiddens_t).squeeze(1)\n",
    "        \n",
    "        # embedded_input: batch * embed_size\n",
    "        # attn_applied: batch * encoder_hidden_size\n",
    "        output = torch.cat((embedded_input, attn_applied), 1)\n",
    "        output = self.attn_combine(output)\n",
    "        \n",
    "        # output: batch * hidden_size\n",
    "        gru_input = F.relu(output).unsqueeze(1)\n",
    "        # hidden_input: batch * hidden_size\n",
    "        hidden_input = hidden_input.unsqueeze(0)\n",
    "        # gru_input: batch * 1 * hidden_size\n",
    "        # hidden_input: 1 * batch * hidden_size\n",
    "        output, hidden = self.gru(gru_input, hidden_input)\n",
    "        output = self.out(output)\n",
    "        #output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers,  self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAttention(inp, output, out_max, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size):\n",
    "    total_avg_loss = 0\n",
    "    loss = 0\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_len = inp.shape[1]\n",
    "    encoder_outputs = torch.zeros(input_len, batch_size, 1, HIDDEN_SIZE, device=device)\n",
    "    encoder_hiddens = torch.zeros(input_len, 1, batch_size, HIDDEN_SIZE, device=device)\n",
    "    # Encode\n",
    "    for ec_idx in range(input_len):\n",
    "        # input batch_size * 1\n",
    "        encoder_output, encoder_hidden = encoder(inp[:, ec_idx].unsqueeze(1), encoder_hidden)\n",
    "        encoder_outputs[ec_idx] = encoder_output\n",
    "        encoder_hiddens[ec_idx] = encoder_hidden\n",
    "\n",
    "    # Decode\n",
    "    decoder_input = torch.tensor([SOS_ID] * batch_size, device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Always use Teacher Forcing\n",
    "    for dc_idx in range(out_max):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.unsqueeze(1), decoder_hidden, encoder_hiddens.squeeze(1))\n",
    "        decoder_output = decoder_output.squeeze(1).to(device) # get rid of the seq dimention\n",
    "        loss += criterion(decoder_output, output[:, dc_idx])\n",
    "        decoder_input = output[:, dc_idx]\n",
    "\n",
    "        ## Print Value\n",
    "#         sample_sentence.append(torch.argmax(decoder_output[0]).item())\n",
    "\n",
    "    loss.backward()\n",
    "    total_avg_loss += loss.item() / out_max\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    ## Print Value\n",
    "#     print(\"Predict: \", ids2sentence(sample_sentence, en.id2word))\n",
    "#     print(\"Actual: \", ids2sentence(output[0].cpu().numpy(), en.id2word))\n",
    "        \n",
    "    return total_avg_loss\n",
    "#     return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleuEvalAttention(encoder, decoder, data_loader, batch_size):\n",
    "    with torch.no_grad():\n",
    "        true_outputs = []\n",
    "        decoder_outputs = []\n",
    "        for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(data_loader):\n",
    "            if i * batch_size >= 10000 or len(inp[0]) != batch_size:\n",
    "                continue\n",
    "            inp = inp.transpose(0,1).to(device)\n",
    "            output = output.transpose(0,1).to(device)\n",
    "            true_outputs.append([[str(tok.item()) for tok in out if tok != 0] for out in output])\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            input_len = inp.shape[1]\n",
    "            encoder_outputs = torch.zeros(input_len, batch_size, 1, HIDDEN_SIZE, device=device)\n",
    "            encoder_hiddens = torch.zeros(input_len, 1, batch_size, HIDDEN_SIZE, device=device)\n",
    "\n",
    "            # Encode\n",
    "            for ec_idx in range(input_len):\n",
    "                # input batch_size * 1\n",
    "                encoder_output, encoder_hidden = encoder(inp[:, ec_idx].unsqueeze(1), encoder_hidden)\n",
    "                encoder_outputs[ec_idx] = encoder_output\n",
    "                encoder_hiddens[ec_idx] = encoder_hidden\n",
    "\n",
    "            # Decode\n",
    "            decoder_input = torch.tensor([SOS_ID] * batch_size, device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            # Greedy\n",
    "            for dc_idx in range(out_max):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.unsqueeze(1), decoder_hidden, encoder_hiddens.squeeze(1))\n",
    "                decoder_output = decoder_output.squeeze(1).to(device) # get rid of the seq dimention\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = torch.LongTensor([topi[i][0] for i in range(inp.size(0))]).to(device)\n",
    "                ## Print Value\n",
    "                decoder_outputs.append(list(decoder_input.cpu().numpy()))\n",
    "            ## Print Value\n",
    "        predict = []\n",
    "        for seq in np.array(decoder_outputs).T.astype(str):\n",
    "            seq_toks = []\n",
    "            for tok in seq:\n",
    "                seq_toks.append(tok)\n",
    "                if tok == '3':\n",
    "                    break\n",
    "            predict.append(seq_toks)\n",
    "#         print('Sample True: ', ' '.join([en.id2word[int(i)] for i in true_outputs[0][0]]))\n",
    "#         print('Sample Predicted: ', ' '.join([en.id2word[int(i)] for i in predict[0]]))\n",
    "#         for seq in predict:\n",
    "#             print('Sample Predicted: ', ' '.join([en.id2word[int(i)] for i in seq]))\n",
    "        bleu_score = corpus_bleu(predict, true_outputs, 4)\n",
    "        return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAttention(train_loader, dev_loader, encoder, decoder, encoder_opt, decoder_opt, criterion, batch_size, epochs, print_every):\n",
    "    start = time.time()\n",
    "    print('Initializing Model Training + Eval...')\n",
    "    losses = []\n",
    "    train_scores = []\n",
    "    dev_scores = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "            if (len(inp[0]) != batch_size):\n",
    "                continue\n",
    "            inp.transpose_(0,1)\n",
    "            output.transpose_(0,1)\n",
    "            inp = inp.to(device)\n",
    "            output = output.to(device)\n",
    "            loss += trainAttention(inp, output, out_max, encoder, decoder, encoder_opt, decoder_opt, criterion, batch_size)\n",
    "            if i % print_every == 0 and i > 0:\n",
    "                losses.append(loss/i)\n",
    "                print(\"Time Elapsed: {} | Loss: {:.4}\".format(asMinutes(time.time() - start),\n",
    "                                                                                loss/i))\n",
    "        train_score = bleuEvalAttention(encoder, decoder, train_loader, batch_size)\n",
    "#         dev_score = bleuEvalATtention(encoder, decoder, dev_loader, batch_size)\n",
    "        train_scores.append(train_score)\n",
    "#         dev_scores.append(dev_score)\n",
    "        print(\"Epoch: {} | Time Elapsed: {} | Loss: {:.4} | Train BLEU: {:.4} | Dev BLEU: \".format(epoch + 1, \n",
    "                                                                                                        asMinutes(time.time() - start),\n",
    "                                                                                                        loss/len(train_loader), \n",
    "                                                                                                        train_score))\n",
    "#                                                                                                         dev_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_size_vi = len(id2word_vi_dic.keys())\n",
    "# dic_size_en = len(id2word_en_dic.keys())\n",
    "HIDDEN_SIZE = 300\n",
    "LEARNING_RATE = 0.01\n",
    "MAX_LENGTH = 100\n",
    "## Add ignore index\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "encoder = EncoderRNN(input_size = vi.n_words, hidden_size = HIDDEN_SIZE, num_layers = 1, batch_size = BATCH_SIZE, raw_emb=vi.emb, learn_ids=vi.learn_ids).to(device)\n",
    "# decoder = DecoderRNN(hidden_size = HIDDEN_SIZE, output_size = en.n_words, num_layers = 1, batch_size = BATCH_SIZE, raw_emb=en.emb, learn_ids=en.learn_ids).to(device)\n",
    "decoder = AttentionDecoderRNN(hidden_size = HIDDEN_SIZE, output_size = en.n_words, num_layers = 1, max_length = MAX_LENGTH, batch_size = BATCH_SIZE, raw_emb = en.emb, learn_ids = en.learn_ids, dropout_p=0.1).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model Training + Eval...\n",
      "Time Elapsed: 10m 16s | Loss: 6.116\n",
      "Time Elapsed: 22m 33s | Loss: 6.678\n",
      "Time Elapsed: 34m 54s | Loss: 6.84\n",
      "Time Elapsed: 47m 13s | Loss: 6.906\n",
      "Time Elapsed: 59m 27s | Loss: 6.948\n",
      "Time Elapsed: 71m 40s | Loss: 6.986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e013219eb3bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfitAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-663b0e8b6878>\u001b[0m in \u001b[0;36mfitAttention\u001b[0;34m(train_loader, dev_loader, encoder, decoder, encoder_opt, decoder_opt, criterion, batch_size, epochs, print_every)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrainAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-99d6f76ddc1c>\u001b[0m in \u001b[0;36mtrainAttention\u001b[0;34m(inp, output, out_max, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#         sample_sentence.append(torch.argmax(decoder_output[0]).item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtotal_avg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mout_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fitAttention(train_loader, dev_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, BATCH_SIZE, 100, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model Training + Eval...\n",
      "Time Elapsed: 4m 13s | Loss: 5.741\n",
      "Time Elapsed: 8m 16s | Loss: 5.55\n",
      "Time Elapsed: 12m 10s | Loss: 5.456\n",
      "Time Elapsed: 16m 24s | Loss: 5.368\n",
      "Time Elapsed: 20m 26s | Loss: 5.299\n",
      "Time Elapsed: 24m 18s | Loss: 5.24\n",
      "Time Elapsed: 28m 30s | Loss: 5.197\n",
      "Time Elapsed: 32m 36s | Loss: 5.156\n",
      "Time Elapsed: 36m 25s | Loss: 5.124\n",
      "Time Elapsed: 40m 37s | Loss: 5.094\n",
      "Time Elapsed: 44m 45s | Loss: 5.067\n",
      "Epoch: 1 | Time Elapsed: 46m 32s | Loss: 5.054 | Train BLEU: 1.563 | Dev BLEU: 1.069\n",
      "Time Elapsed: 50m 37s | Loss: 4.608\n",
      "Time Elapsed: 54m 48s | Loss: 4.597\n",
      "Time Elapsed: 58m 31s | Loss: 4.575\n",
      "Time Elapsed: 62m 43s | Loss: 4.55\n",
      "Time Elapsed: 66m 54s | Loss: 4.54\n",
      "Time Elapsed: 70m 39s | Loss: 4.529\n",
      "Time Elapsed: 74m 51s | Loss: 4.518\n",
      "Time Elapsed: 79m 3s | Loss: 4.51\n",
      "Time Elapsed: 82m 48s | Loss: 4.501\n",
      "Time Elapsed: 86m 58s | Loss: 4.493\n",
      "Time Elapsed: 91m 10s | Loss: 4.486\n",
      "Epoch: 2 | Time Elapsed: 93m 10s | Loss: 4.481 | Train BLEU: 2.315 | Dev BLEU: 2.702\n",
      "Time Elapsed: 97m 1s | Loss: 4.145\n",
      "Time Elapsed: 101m 14s | Loss: 4.15\n",
      "Time Elapsed: 105m 21s | Loss: 4.16\n",
      "Time Elapsed: 109m 8s | Loss: 4.163\n",
      "Time Elapsed: 113m 21s | Loss: 4.168\n",
      "Time Elapsed: 117m 30s | Loss: 4.18\n",
      "Time Elapsed: 121m 14s | Loss: 4.189\n",
      "Time Elapsed: 125m 27s | Loss: 4.194\n",
      "Time Elapsed: 129m 40s | Loss: 4.198\n",
      "Time Elapsed: 133m 25s | Loss: 4.2\n",
      "Time Elapsed: 137m 38s | Loss: 4.204\n",
      "Epoch: 3 | Time Elapsed: 139m 49s | Loss: 4.203 | Train BLEU: 2.007 | Dev BLEU: 6.053\n",
      "Time Elapsed: 143m 35s | Loss: 3.935\n",
      "Time Elapsed: 147m 49s | Loss: 3.957\n",
      "Time Elapsed: 152m 0s | Loss: 3.968\n",
      "Time Elapsed: 155m 44s | Loss: 3.975\n",
      "Time Elapsed: 159m 57s | Loss: 3.984\n",
      "Time Elapsed: 164m 8s | Loss: 3.991\n",
      "Time Elapsed: 167m 55s | Loss: 4.001\n",
      "Time Elapsed: 172m 7s | Loss: 4.008\n",
      "Time Elapsed: 176m 20s | Loss: 4.015\n",
      "Time Elapsed: 180m 10s | Loss: 4.022\n",
      "Time Elapsed: 184m 18s | Loss: 4.028\n",
      "Epoch: 4 | Time Elapsed: 186m 23s | Loss: 4.029 | Train BLEU: 1.628 | Dev BLEU: 2.186\n",
      "Time Elapsed: 190m 30s | Loss: 3.786\n",
      "Time Elapsed: 194m 21s | Loss: 3.806\n",
      "Time Elapsed: 198m 35s | Loss: 3.824\n",
      "Time Elapsed: 202m 44s | Loss: 3.839\n",
      "Time Elapsed: 206m 31s | Loss: 3.855\n",
      "Time Elapsed: 210m 43s | Loss: 3.868\n",
      "Time Elapsed: 214m 53s | Loss: 3.879\n",
      "Time Elapsed: 218m 39s | Loss: 3.889\n",
      "Time Elapsed: 222m 52s | Loss: 3.896\n",
      "Time Elapsed: 226m 55s | Loss: 3.905\n",
      "Time Elapsed: 230m 49s | Loss: 3.912\n",
      "Epoch: 5 | Time Elapsed: 232m 51s | Loss: 3.913 | Train BLEU: 4.11 | Dev BLEU: 3.036\n",
      "Time Elapsed: 237m 4s | Loss: 3.715\n",
      "Time Elapsed: 240m 48s | Loss: 3.734\n",
      "Time Elapsed: 244m 59s | Loss: 3.747\n",
      "Time Elapsed: 249m 11s | Loss: 3.761\n",
      "Time Elapsed: 252m 57s | Loss: 3.777\n",
      "Time Elapsed: 257m 6s | Loss: 3.788\n",
      "Time Elapsed: 261m 20s | Loss: 3.801\n",
      "Time Elapsed: 265m 13s | Loss: 3.812\n",
      "Time Elapsed: 269m 16s | Loss: 3.825\n",
      "Time Elapsed: 273m 29s | Loss: 3.836\n",
      "Time Elapsed: 277m 23s | Loss: 3.846\n",
      "Epoch: 6 | Time Elapsed: 279m 20s | Loss: 3.847 | Train BLEU: 4.179 | Dev BLEU: 3.274\n",
      "Time Elapsed: 283m 32s | Loss: 3.66\n",
      "Time Elapsed: 287m 43s | Loss: 3.681\n",
      "Time Elapsed: 291m 29s | Loss: 3.695\n",
      "Time Elapsed: 295m 41s | Loss: 3.715\n",
      "Time Elapsed: 299m 54s | Loss: 3.729\n",
      "Time Elapsed: 303m 36s | Loss: 3.74\n",
      "Time Elapsed: 307m 49s | Loss: 3.753\n",
      "Time Elapsed: 311m 56s | Loss: 3.762\n",
      "Time Elapsed: 315m 41s | Loss: 3.772\n",
      "Time Elapsed: 319m 51s | Loss: 3.782\n",
      "Time Elapsed: 323m 58s | Loss: 3.791\n",
      "Epoch: 7 | Time Elapsed: 325m 47s | Loss: 3.794 | Train BLEU: 2.771 | Dev BLEU: 5.678\n",
      "Time Elapsed: 329m 50s | Loss: 3.587\n",
      "Time Elapsed: 334m 0s | Loss: 3.618\n",
      "Time Elapsed: 337m 56s | Loss: 3.637\n",
      "Time Elapsed: 341m 53s | Loss: 3.665\n",
      "Time Elapsed: 346m 2s | Loss: 3.687\n",
      "Time Elapsed: 349m 59s | Loss: 3.711\n",
      "Time Elapsed: 353m 54s | Loss: 3.725\n",
      "Time Elapsed: 358m 5s | Loss: 3.742\n",
      "Time Elapsed: 362m 4s | Loss: 3.753\n",
      "Time Elapsed: 365m 57s | Loss: 3.764\n",
      "Time Elapsed: 370m 8s | Loss: 3.773\n",
      "Epoch: 8 | Time Elapsed: 372m 13s | Loss: 3.775 | Train BLEU: 2.516 | Dev BLEU: 5.869\n",
      "Time Elapsed: 375m 59s | Loss: 3.588\n",
      "Time Elapsed: 380m 10s | Loss: 3.608\n",
      "Time Elapsed: 384m 21s | Loss: 3.627\n",
      "Time Elapsed: 388m 4s | Loss: 3.645\n",
      "Time Elapsed: 392m 13s | Loss: 3.66\n",
      "Time Elapsed: 396m 23s | Loss: 3.678\n",
      "Time Elapsed: 400m 4s | Loss: 3.692\n",
      "Time Elapsed: 404m 13s | Loss: 3.704\n",
      "Time Elapsed: 408m 24s | Loss: 3.716\n",
      "Time Elapsed: 412m 6s | Loss: 3.729\n",
      "Time Elapsed: 416m 16s | Loss: 3.74\n",
      "Epoch: 9 | Time Elapsed: 418m 15s | Loss: 3.742 | Train BLEU: 2.775 | Dev BLEU: 5.751\n",
      "Time Elapsed: 422m 18s | Loss: 3.598\n",
      "Time Elapsed: 426m 7s | Loss: 3.616\n",
      "Time Elapsed: 430m 16s | Loss: 3.63\n",
      "Time Elapsed: 434m 22s | Loss: 3.644\n",
      "Time Elapsed: 438m 10s | Loss: 3.659\n",
      "Time Elapsed: 442m 20s | Loss: 3.669\n",
      "Time Elapsed: 446m 27s | Loss: 3.678\n",
      "Time Elapsed: 450m 12s | Loss: 3.687\n",
      "Time Elapsed: 454m 20s | Loss: 3.697\n",
      "Time Elapsed: 458m 31s | Loss: 3.707\n",
      "Time Elapsed: 462m 15s | Loss: 3.716\n",
      "Epoch: 10 | Time Elapsed: 464m 16s | Loss: 3.718 | Train BLEU: 5.838 | Dev BLEU: 3.0\n"
     ]
    }
   ],
   "source": [
    "# vi - en non-hybrid embeddings\n",
    "fit(train_loader, dev_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, BATCH_SIZE, 10, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(encoder, open(\"./hybrid-vi-encoder.p\", \"wb\"))\n",
    "# pkl.dump(decoder, open(\"./hybrid-vi-decoder.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overfit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-b0423e06f8c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbleuEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-90-89c587f1deef>\u001b[0m in \u001b[0;36mbleuEval\u001b[0;34m(encoder, decoder, data_loader, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrue_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-89c587f1deef>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrue_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-89c587f1deef>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrue_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bleuEval(encoder, decoder, train_loader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./vi-encoder.p', 'rb') as pickle_file:\n",
    "    baseline_enc = pkl.load(pickle_file)\n",
    "with open('./vi-decoder.p', 'rb') as pickle_file:\n",
    "    baseline_dec = pkl.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample True:  <sos> in 4 minutes , atmospheric chemist <unk> pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> : the frequent gradients , the frequent revolution -- the themes of the world . <eos>\n",
      "Sample Predicted:  <sos> we have to introduce the <unk> <unk> and the <unk> . <eos>\n",
      "Sample Predicted:  <sos> so we went to the <unk> , and we were buddhist <unk> . <eos>\n",
      "Sample Predicted:  <sos> i went to the other problems , and i m able to solve the other problems . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> , the <unk> , the <unk> , the <unk> , the <unk> . <eos>\n",
      "Sample Predicted:  <sos> we re able to do the biggest challenge , we re able to do the biggest challenge . <eos>\n",
      "Sample Predicted:  <sos> i m sure that , i m sure that i m going to see the <unk> . <eos>\n",
      "Sample Predicted:  <sos> but the <unk> , but the <unk> , but the <unk> , but the <unk> . <eos>\n",
      "Sample Predicted:  <sos> i want to show you to do the biggest areas of the world . <eos>\n",
      "Sample Predicted:  <sos> so , and the other thing , and they don t know , and they don t know what will happen . <eos>\n",
      "Sample Predicted:  <sos> we also found the answer , we can t have to do the same thing . <eos>\n",
      "Sample Predicted:  <sos> we re going to be able to do the world . <eos>\n",
      "Sample Predicted:  <sos> so , and the problem , and the problem of the problem of the world . <eos>\n",
      "Sample Predicted:  <sos> so , as the in-situ , the other thing , the other thing , the other thing . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> : <unk> <unk> . <eos>\n",
      "Sample Predicted:  <sos> and the mainframe temple vessels are the world . <eos>\n",
      "Sample Predicted:  <sos> we re able to do the world . <eos>\n",
      "Sample Predicted:  <sos> it grows 14 feet , it grows 14 feet . <eos>\n",
      "Sample Predicted:  <sos> i m going to be able to do the <unk> . <eos>\n",
      "Sample Predicted:  <sos> we re able to do the world . <eos>\n",
      "Sample Predicted:  <sos> it s a lot of the soprano place . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> , the <unk> , the <unk> dative . <eos>\n",
      "Sample Predicted:  <sos> so , the first thing is the first thing , the first thing is the world . <eos>\n",
      "Sample Predicted:  <sos> so it s a righty . <eos>\n",
      "Sample Predicted:  <sos> so , the first thing , the first thing , the first thing is the world . <eos>\n",
      "Sample Predicted:  <sos> so the mainframe <unk> <unk> . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> : <unk> <unk> . <eos>\n",
      "Sample Predicted:  <sos> we ve got to induce of the world . <eos>\n",
      "Sample Predicted:  <sos> we narrowed it , and we can t have to induce . <eos>\n",
      "Sample Predicted:  <sos> we can t have assessed . <eos>\n",
      "Sample Predicted:  <sos> the problem of the problem . <eos>\n",
      "Sample Predicted:  <sos> so the problem of the problem . <eos>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4825853903799304"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleuEval(baseline_enc, baseline_dec, overfit_loader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.gru.batch_fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m(143)\u001b[0;36mcheck_hidden_size\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    141 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0mcheck_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Expected hidden size {}, got {}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    142 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 143 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    144 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    145 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'encoder' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-31-f7ca4d53521a>\u001b[0m(15)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     13 \u001b[0;31m        \u001b[0;31m# input batch_size * 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mec_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 15 \u001b[0;31m        \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mec_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m        \u001b[0mencoder_hiddens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mec_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  enoder_outputs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'enoder_outputs' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  encoder_outputs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]]],\n",
      "\n",
      "\n",
      "        [[[-0.0219, -0.0105, -0.2507,  ...,  0.1992, -0.4429, -0.4246]],\n",
      "\n",
      "         [[-0.0108,  0.4253,  0.3838,  ..., -0.2613,  0.1234, -0.1874]],\n",
      "\n",
      "         [[-0.2046, -0.4536,  0.5676,  ...,  0.0421,  0.3484, -0.3940]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0036, -0.4598,  0.2498,  ..., -0.1157,  0.1377,  0.2163]],\n",
      "\n",
      "         [[-0.0036, -0.4598,  0.2498,  ..., -0.1157,  0.1377,  0.2163]],\n",
      "\n",
      "         [[-0.0856,  0.0131,  0.2655,  ...,  0.2046, -0.2525, -0.0867]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4918, -0.1363,  0.1370,  ...,  0.0146, -0.2607, -0.5579]],\n",
      "\n",
      "         [[-0.1242,  0.3349,  0.5718,  ..., -0.3327,  0.2256, -0.0852]],\n",
      "\n",
      "         [[-0.3152, -0.5568,  0.6375,  ...,  0.0403,  0.5346, -0.4440]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0275,  0.3268, -0.1821,  ..., -0.0095,  0.1823,  0.4863]],\n",
      "\n",
      "         [[ 0.0017, -0.0015,  0.0989,  ..., -0.2592,  0.0363,  0.3519]],\n",
      "\n",
      "         [[ 0.1362,  0.0032,  0.2977,  ..., -0.0740, -0.0210, -0.0719]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1357,  0.1549,  0.2249,  ...,  0.0175,  0.4587,  0.1154]],\n",
      "\n",
      "         [[-0.1083, -0.2688,  0.4672,  ...,  0.2151, -0.3381,  0.1786]],\n",
      "\n",
      "         [[ 0.2141, -0.3156,  0.1468,  ...,  0.4485, -0.3331, -0.3143]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0820, -0.9278,  0.6974,  ...,  0.4278,  0.5836, -0.7664]],\n",
      "\n",
      "         [[-0.0804, -0.9272,  0.6960,  ...,  0.4278,  0.5823, -0.7665]],\n",
      "\n",
      "         [[-0.0783, -0.9280,  0.6984,  ...,  0.4253,  0.5912, -0.7671]]],\n",
      "\n",
      "\n",
      "        [[[-0.3314,  0.3519,  0.2042,  ..., -0.6216,  0.3659,  0.0270]],\n",
      "\n",
      "         [[-0.5117, -0.0432, -0.2642,  ...,  0.3886, -0.2846, -0.2133]],\n",
      "\n",
      "         [[ 0.2323, -0.4987, -0.0415,  ..., -0.0950, -0.3297, -0.1025]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0801, -0.9281,  0.6986,  ...,  0.4258,  0.5901, -0.7666]],\n",
      "\n",
      "         [[-0.0789, -0.9276,  0.6976,  ...,  0.4258,  0.5892, -0.7667]],\n",
      "\n",
      "         [[-0.0773, -0.9281,  0.6992,  ...,  0.4241,  0.5948, -0.7671]]],\n",
      "\n",
      "\n",
      "        [[[-0.1126, -0.0602,  0.0442,  ..., -0.0315,  0.2195, -0.5180]],\n",
      "\n",
      "         [[-0.1126,  0.0772, -0.5050,  ..., -0.0286, -0.3123,  0.3051]],\n",
      "\n",
      "         [[ 0.0513, -0.1630, -0.1074,  ...,  0.0284,  0.2293, -0.2188]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0788, -0.9282,  0.6993,  ...,  0.4244,  0.5944, -0.7668]],\n",
      "\n",
      "         [[-0.0778, -0.9278,  0.6987,  ...,  0.4244,  0.5938, -0.7668]],\n",
      "\n",
      "         [[-0.0766, -0.9281,  0.6997,  ...,  0.4233,  0.5973, -0.7671]]]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  len(encoder_outputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  encoder_outputs[23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** IndexError: index 23 is out of bounds for dimension 0 with size 23\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict:  ['<sos>', '<sos>', '<sos>', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  8.810749053955078\n",
      "Predict:  ['<sos>', '<sos>', 'the', 'the', 'the', 'two', 'the', 'the', 'the', 'freaky']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  4.840537643432617\n",
      "Predict:  ['<sos>', '<sos>', 's', 'just', 'these', 'two', 'the', 'the', 'the', 'freaky']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  1.9151222229003906\n",
      "Predict:  ['<sos>', '<sos>', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.9088132858276368\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.5086045742034913\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.3032816410064697\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.18692054748535156\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.12194809913635254\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.08495545387268066\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.06054344177246094\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.043548297882080075\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.03167123794555664\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.023690128326416017\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.017924118041992187\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f6d188e37f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverfit_vi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverfit_en_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a1868babeca0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train_data, target_data, encoder, decoder, encoder_opt, decoder_opt, criterion, batch_size, epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3b1f9816ee80>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_input, train_target, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtotal_avg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit(overfit_vi_train, overfit_en_train, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, BATCH_SIZE, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  8.852307891845703\n",
      "Predict:  ['<sos>', '<sos>', 'the', 'just', 'these', 'the', 'notes', 'the', 'the', 'the']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  4.9451759338378904\n",
      "Predict:  ['<sos>', '<sos>', 'coming', 'just', 'these', 'notes', 'notes', 'the', 'the', 'the']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  1.9719776153564452\n",
      "Predict:  ['<sos>', '<sos>', 's', 'just', 'these', 'two', 'notes', 'the', 'the', 'freaky']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.9234449386596679\n",
      "Predict:  ['<sos>', '<sos>', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'freaky']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.5161348819732666\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.3111863613128662\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.19171581268310547\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.1251605987548828\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.08669190406799317\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.06085610389709473\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.043444252014160155\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0317835807800293\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.02393932342529297\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.018381881713867187\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.014472675323486329\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.011426544189453125\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.009125995635986327\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0074138641357421875\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.006087779998779297\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.005056858062744141\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.004248332977294922\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0036081314086914063\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0030983924865722657\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0026922225952148438\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0023674964904785156\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.002105426788330078\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.001891326904296875\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0017138481140136718\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0015657424926757812\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0014398574829101562\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.00133209228515625\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0012396812438964845\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0011590957641601563\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.001088714599609375\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0010272979736328125\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0009730339050292968\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0009249687194824219\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0008824348449707031\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.000844573974609375\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0008105278015136719\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0007798194885253907\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.00075225830078125\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0007273674011230469\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0007046699523925781\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0006839752197265625\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0006653785705566407\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0006479263305664062\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0006319999694824219\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.000617218017578125\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.000603485107421875\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005910873413085937\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005793571472167969\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005681991577148437\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005581855773925781\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.000548553466796875\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005395889282226562\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005313873291015625\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005230903625488281\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005156517028808594\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-78ec3519bd0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverfit_vi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverfit_en_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-256264019c3a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_input, train_target, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/NLP/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for i in range(1000):\n",
    "#     train(overfit_vi_train, overfit_en_train, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(train_input, train_target, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size):\n",
    "#     # Batch\n",
    "#     total_avg_loss = 0\n",
    "#     for i in range(len(train_input) // batch_size):\n",
    "#         loss = 0\n",
    "#         encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "#         batch = get_batch(i, batch_size, train_input, train_target)\n",
    "#         # size batch_size * seq_length\n",
    "#         batch_input = torch.tensor(batch[0], device=device)\n",
    "#         batch_target = torch.tensor(batch[1], device=device)\n",
    "#         input_length = batch_input.shape[1] ## should be seq length\n",
    "#         target_length = batch_target.shape[1]\n",
    "#         print(input_length, target_length)\n",
    "\n",
    "#         encoder_optimizer.zero_grad()\n",
    "#         decoder_optimizer.zero_grad()\n",
    "        \n",
    "#         encoder_outputs = torch.zeros(input_length, batch_size, 1, 256, device=device)\n",
    "#         encoder_hiddens = torch.zeros(input_length, 1, batch_size, 256, device=device)\n",
    "        \n",
    "#         # Encode\n",
    "#         for ec_idx in range(input_length):\n",
    "#             # input batch_size * 1\n",
    "#             encoder_output, encoder_hidden = encoder(batch_input[:, ec_idx].unsqueeze(1), encoder_hidden)\n",
    "#             encoder_outputs[ec_idx] = encoder_output\n",
    "#             encoder_hiddens[ec_idx] = encoder_hidden\n",
    "        \n",
    "#         # Decode\n",
    "#         decoder_input = torch.tensor([2] * batch_size, device=device) # SOS token 2\n",
    "#         decoder_hidden = encoder_hidden\n",
    "        \n",
    "#         ## Print Value\n",
    "#         sample_sentence = []\n",
    "        \n",
    "#         # Always use Teacher Forcing\n",
    "#         for dc_idx in range(target_length):\n",
    "#             decoder_output, decoder_hidden = decoder(decoder_input.unsqueeze(1), decoder_hidden)\n",
    "#             decoder_output = decoder_output.squeeze(1).to(device) # get rid of the seq dimention\n",
    "#             loss += criterion(decoder_output, batch_target[:, dc_idx])\n",
    "#             decoder_input = batch_target[:, dc_idx]\n",
    "            \n",
    "#             ## Print Value\n",
    "#             sample_sentence.append(torch.argmax(decoder_output[0]).item())\n",
    "            \n",
    "#         loss.backward()\n",
    "#         total_avg_loss += loss.item() / target_length\n",
    "        \n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "        \n",
    "# #         print('Training Loss: ', loss.item() / target_length)\n",
    "        \n",
    "#         ## Print Value\n",
    "#         print(\"Predict: \", ids2sentence(sample_sentence, id2word_en_dic))\n",
    "#         print(\"Actual: \", ids2sentence(batch_target[0].cpu().numpy(), id2word_en_dic))\n",
    "        \n",
    "#     return total_avg_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
