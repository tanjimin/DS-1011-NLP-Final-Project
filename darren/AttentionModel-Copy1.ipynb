{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, tensorToList\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dictionaries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pkl.load(f)\n",
    "        return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sentence given numbers\n",
    "def ids2sentence(sentence, dictionary):\n",
    "    return ' '.join([dictionary[i] for i in sentence])\n",
    "#ids2sentence(en_train_num[0], id2word_en_dic)\n",
    "\n",
    "def add_symbol(id2word_dic, word2id_dic):\n",
    "    symbols = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    for i, symbol in enumerate(symbols):\n",
    "        id2word_dic[i] = symbol\n",
    "        word2id_dic[symbol] = i\n",
    "    return id2word_dic, word2id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2word_vi_dic = load_zipped_pickle(\"../embeddings/id2word_vi_dic.p\")\n",
    "# word2id_vi_dic = load_zipped_pickle(\"../embeddings/word2id_vi_dic.p\")\n",
    "\n",
    "# id2word_en_dic = load_zipped_pickle(\"../embeddings/id2word_en_dic.p\")\n",
    "# word2id_en_dic = load_zipped_pickle(\"../embeddings/word2id_en_dic.p\")\n",
    "\n",
    "# id2word_vi_dic, word2id_vi_dic = add_symbol(id2word_vi_dic, word2id_vi_dic)\n",
    "# id2word_en_dic, word2id_en_dic = add_symbol(id2word_en_dic, word2id_en_dic)\n",
    "\n",
    "# vi_train = load_zipped_pickle(\"../data/vi-en-tokens/train_vi_tok.p\")\n",
    "# en_train = load_zipped_pickle(\"../data/vi-en-tokens/train_en_tok.p\") # Already Processed for symbols\n",
    "\n",
    "# vi_train_num = load_zipped_pickle(\"../data/vi-en-tokens/train_vi_tok_num.p\")\n",
    "# en_train_num = load_zipped_pickle(\"../data/vi-en-tokens/train_en_tok_num.p\") # Already Processed for symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi, en = loadLangPairs(\"vi\")\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(len(vi.train_num)) if (2 < len(vi.train[i]) < vi.max_length) & (2 < len(en.train[i]) < en.max_length)])\n",
    "overfit_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(32)])\n",
    "overfit_loader = torch.utils.data.DataLoader(dataset=overfit_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "dev_dataset = langDataset([(vi.dev_num[i], en.dev_num[i]) for i in range(len(vi.dev_num)) if (2 < len(vi.dev[i]) < vi.max_length) & (2 < len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by input data length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sort_by_length(data_input, target_data):\n",
    "#     input_size = [len(data) for data in data_input]\n",
    "#     size_index = np.argsort(input_size)\n",
    "#     return list(np.array(data_input)[size_index]), list(np.array(target_data)[size_index])\n",
    "\n",
    "# vi_train_num, en_train_num = sort_by_length(vi_train_num, en_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Data given batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(data, length):\n",
    "    for i, line in enumerate(data):\n",
    "        if len(line) < length:\n",
    "            for i in range(len(line), length):\n",
    "                line.append(0)\n",
    "        else:\n",
    "            data[i] = line[0:length]\n",
    "    return data\n",
    "\n",
    "# Return the batch data and target\n",
    "def get_batch(i, batch_size, train_data, train_target):\n",
    "    if i * batch_size > len(train_data):\n",
    "        raise Exception('Incorrect batch index')\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "    batch_data = list(np.array(train_data)[start_idx:end_idx])\n",
    "    batch_target = list(np.array(train_target)[start_idx:end_idx])\n",
    "    batch_data = pad(batch_data, len(batch_data[batch_size - 1]))\n",
    "    max_target = max([len(data) for data in batch_data])\n",
    "    batch_target = pad(batch_target, max_target)\n",
    "    return batch_data, batch_target\n",
    "\n",
    "# get_batch(5, 64, vi_train_num, en_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_size, raw_emb, learn_ids):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        # input_size: input dictionary size\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(self.hidden_size, \n",
    "                          hidden_size, \n",
    "                          num_layers= num_layers, \n",
    "                          batch_first = True) # BATCH FIRST\n",
    "\n",
    "    def forward(self, encoder_input, hidden_input):\n",
    "        # encoder_input: batch * 1 (for 1 word each time)\n",
    "        embedded_input = self.embedding(encoder_input)\n",
    "        # embedded_input: batch * 1 * emb_dim\n",
    "        # hidden_input: batch * 1(layer) * hidden_size\n",
    "        output, hidden = self.gru(embedded_input, hidden_input)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, max_length, batch_size, raw_emb, learn_ids, dropout_p=0.1):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        # Max length for a sentence\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, \n",
    "                          self.hidden_size,\n",
    "                          num_layers= num_layers, \n",
    "                          batch_first = True) # BATCH_FRIST)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, decoder_input, hidden_input, encoder_hiddens):\n",
    "        # hidden_input: 1 * batch * hidden_size\n",
    "        hidden_input = hidden_input.squeeze(0)\n",
    "        # decoder_input: batch * 1\n",
    "        embedded_input = self.embedding(decoder_input)\n",
    "        # embedded_input: batch * 1 * embed_size\n",
    "        embedded_input = self.dropout(embedded_input).squeeze(1)\n",
    "        \n",
    "        # embedded_input: batch * embed_size\n",
    "        # hidden_input: batch * hidden_size \n",
    "        # (Use input and newest hidden to decide which encoder hidden is important)\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded_input, hidden_input), 1)), dim=1).unsqueeze(1)\n",
    "        # encoder_output: max_length * batch * encoder_hidden_size\n",
    "        encoder_hiddens_t = encoder_hiddens.transpose(0, 1)\n",
    "        # attn_weights: batch * 1 * max_length(theoretical)\n",
    "        cropped_attn_weights = attn_weights[:, :, :encoder_hiddens_t.shape[1]]\n",
    "        # cropped_attn_weights: batch * 1 * max_length(actual)\n",
    "        # encoder_hiddens_t: batch * max_length(actual) * encoder_hidden_size\n",
    "        ## \n",
    "        attn_applied = torch.bmm(cropped_attn_weights, encoder_hiddens_t).squeeze(1)\n",
    "        \n",
    "        # embedded_input: batch * embed_size\n",
    "        # attn_applied: batch * encoder_hidden_size\n",
    "        output = torch.cat((embedded_input, attn_applied), 1)\n",
    "        output = self.attn_combine(output)\n",
    "        \n",
    "        # output: batch * hidden_size\n",
    "        gru_input = F.relu(output).unsqueeze(1)\n",
    "        # hidden_input: batch * hidden_size\n",
    "        hidden_input = hidden_input.unsqueeze(0)\n",
    "        # gru_input: batch * 1 * hidden_size\n",
    "        # hidden_input: 1 * batch * hidden_size\n",
    "        output, hidden = self.gru(gru_input, hidden_input)\n",
    "        output = self.out(output)\n",
    "        #output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers,  self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAttention(inp, output, out_max, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size):\n",
    "    total_avg_loss = 0\n",
    "    loss = 0\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_len = inp.shape[1]\n",
    "    encoder_outputs = torch.zeros(input_len, batch_size, 1, HIDDEN_SIZE, device=device)\n",
    "    encoder_hiddens = torch.zeros(input_len, 1, batch_size, HIDDEN_SIZE, device=device)\n",
    "    # Encode\n",
    "    for ec_idx in range(input_len):\n",
    "        # input batch_size * 1\n",
    "        encoder_output, encoder_hidden = encoder(inp[:, ec_idx].unsqueeze(1), encoder_hidden)\n",
    "        encoder_outputs[ec_idx] = encoder_output\n",
    "        encoder_hiddens[ec_idx] = encoder_hidden\n",
    "\n",
    "    # Decode\n",
    "    decoder_input = torch.tensor([SOS_ID] * batch_size, device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Always use Teacher Forcing\n",
    "    for dc_idx in range(out_max):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.unsqueeze(1), decoder_hidden, encoder_hiddens.squeeze(1))\n",
    "        decoder_output = decoder_output.squeeze(1).to(device) # get rid of the seq dimention\n",
    "        loss += criterion(decoder_output, output[:, dc_idx])\n",
    "        decoder_input = output[:, dc_idx]\n",
    "\n",
    "        ## Print Value\n",
    "#         sample_sentence.append(torch.argmax(decoder_output[0]).item())\n",
    "\n",
    "    loss.backward()\n",
    "    total_avg_loss += loss.item() / out_max\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    ## Print Value\n",
    "#     print(\"Predict: \", ids2sentence(sample_sentence, en.id2word))\n",
    "#     print(\"Actual: \", ids2sentence(output[0].cpu().numpy(), en.id2word))\n",
    "        \n",
    "    return total_avg_loss\n",
    "#     return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleuEvalAttention(encoder, decoder, data_loader, batch_size):\n",
    "    with torch.no_grad():\n",
    "        true_outputs = []\n",
    "        decoder_outputs = []\n",
    "        for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(data_loader):\n",
    "            if i * batch_size >= 10000 or len(inp[0]) != batch_size:\n",
    "                continue\n",
    "            inp = inp.transpose(0,1).to(device)\n",
    "            output = output.transpose(0,1).to(device)\n",
    "            true_outputs.append([[str(tok.item()) for tok in out if tok != 0] for out in output])\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            input_len = inp.shape[1]\n",
    "            encoder_outputs = torch.zeros(input_len, batch_size, 1, HIDDEN_SIZE, device=device)\n",
    "            encoder_hiddens = torch.zeros(input_len, 1, batch_size, HIDDEN_SIZE, device=device)\n",
    "\n",
    "            # Encode\n",
    "            for ec_idx in range(input_len):\n",
    "                # input batch_size * 1\n",
    "                encoder_output, encoder_hidden = encoder(inp[:, ec_idx].unsqueeze(1), encoder_hidden)\n",
    "                encoder_outputs[ec_idx] = encoder_output\n",
    "                encoder_hiddens[ec_idx] = encoder_hidden\n",
    "\n",
    "            # Decode\n",
    "            decoder_input = torch.tensor([SOS_ID] * batch_size, device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            # Greedy\n",
    "            for dc_idx in range(out_max):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.unsqueeze(1), decoder_hidden, encoder_hiddens.squeeze(1))\n",
    "                decoder_output = decoder_output.squeeze(1).to(device) # get rid of the seq dimention\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = torch.LongTensor([topi[i][0] for i in range(inp.size(0))]).to(device)\n",
    "                ## Print Value\n",
    "                decoder_outputs.append(list(decoder_input.cpu().numpy()))\n",
    "            ## Print Value\n",
    "        predict = []\n",
    "        for seq in np.array(decoder_outputs).T.astype(str):\n",
    "            seq_toks = []\n",
    "            for tok in seq:\n",
    "                seq_toks.append(tok)\n",
    "                if tok == '3':\n",
    "                    break\n",
    "            predict.append(seq_toks)\n",
    "        print(np.shape(predict))\n",
    "        print(np.shape(decoder_outputs))\n",
    "        print(np.shape(true_outputs))\n",
    "#         print('Sample True: ', ' '.join([en.id2word[int(i)] for i in true_outputs[0][0]]))\n",
    "#         print('Sample Predicted: ', ' '.join([en.id2word[int(i)] for i in predict[0]]))\n",
    "#         for seq in predict:\n",
    "#             print('Sample Predicted: ', ' '.join([en.id2word[int(i)] for i in seq]))\n",
    "        bleu_score = corpus_bleu(predict, true_outputs, 4)\n",
    "        return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAttention(train_loader, dev_loader, encoder, decoder, encoder_opt, decoder_opt, criterion, batch_size, epochs, print_every):\n",
    "    start = time.time()\n",
    "    print('Initializing Model Training + Eval...')\n",
    "    losses = []\n",
    "    train_scores = []\n",
    "    dev_scores = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "            if (len(inp[0]) != batch_size):\n",
    "                continue\n",
    "            inp.transpose_(0,1)\n",
    "            output.transpose_(0,1)\n",
    "            inp = inp.to(device)\n",
    "            output = output.to(device)\n",
    "            loss += trainAttention(inp, output, out_max, encoder, decoder, encoder_opt, decoder_opt, criterion, batch_size)\n",
    "            if i % print_every == 0 and i > 0:\n",
    "                losses.append(loss/i)\n",
    "                print(\"Time Elapsed: {} | Loss: {:.4}\".format(asMinutes(time.time() - start),\n",
    "                                                                                loss/i))\n",
    "                pkl.dump(encoder, open(\"./vi-g-attn-encoder-sgd0.01.p\", \"wb\"))\n",
    "                pkl.dump(decoder, open(\"./vi-g-attn-decoder-sgd0.01.p\", \"wb\"))\n",
    "        train_score = bleuEvalAttention(encoder, decoder, train_loader, batch_size)\n",
    "#         dev_score = bleuEvalATtention(encoder, decoder, dev_loader, batch_size)\n",
    "        train_scores.append(train_score)\n",
    "#         dev_scores.append(dev_score)\n",
    "        print(\"Epoch: {} | Time Elapsed: {} | Loss: {:.4} | Train BLEU: {:.4}\".format(epoch + 1, \n",
    "                                                                                                        asMinutes(time.time() - start),\n",
    "                                                                                                        loss/len(train_loader), \n",
    "                                                                                                        train_score))\n",
    "#                                                                                                         dev_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_size_vi = len(id2word_vi_dic.keys())\n",
    "# dic_size_en = len(id2word_en_dic.keys())\n",
    "HIDDEN_SIZE = 300\n",
    "LEARNING_RATE = 0.01\n",
    "MAX_LENGTH = 100\n",
    "## Add ignore index\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "encoder = EncoderRNN(input_size = vi.n_words, hidden_size = HIDDEN_SIZE, num_layers = 1, batch_size = BATCH_SIZE, raw_emb=vi.emb, learn_ids=vi.learn_ids).to(device)\n",
    "# decoder = DecoderRNN(hidden_size = HIDDEN_SIZE, output_size = en.n_words, num_layers = 1, batch_size = BATCH_SIZE, raw_emb=en.emb, learn_ids=en.learn_ids).to(device)\n",
    "decoder = AttentionDecoderRNN(hidden_size = HIDDEN_SIZE, output_size = en.n_words, num_layers = 1, max_length = MAX_LENGTH, batch_size = BATCH_SIZE, raw_emb = en.emb, learn_ids = en.learn_ids, dropout_p=0.1).to(device)\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = pkl.load(open('vi-g-attn-encoder-sgd0.01.p', 'rb'))\n",
    "decoder = pkl.load(open('vi-g-attn-decoder-sgd0.01.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = langDataset([(vi.test_num[i], en.train_num[i]) for i in range(len(vi.test_num)) if (2 < len(vi.test[i]) < vi.max_length) & (2 < len(en.test[i]) < en.max_length)])\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "bleuEvalAttention(encoder, decoder, train_loader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model Training + Eval...\n",
      "Time Elapsed: 6m 29s | Loss: 6.931\n",
      "Time Elapsed: 15m 17s | Loss: 6.272\n",
      "Time Elapsed: 24m 2s | Loss: 5.961\n",
      "Time Elapsed: 32m 49s | Loss: 5.763\n",
      "Time Elapsed: 41m 39s | Loss: 5.62\n",
      "Time Elapsed: 50m 31s | Loss: 5.507\n",
      "Time Elapsed: 59m 14s | Loss: 5.42\n",
      "Time Elapsed: 68m 5s | Loss: 5.345\n",
      "Time Elapsed: 76m 49s | Loss: 5.279\n",
      "Time Elapsed: 85m 31s | Loss: 5.227\n",
      "Time Elapsed: 89m 40s | Loss: 5.181\n",
      "Epoch: 1 | Time Elapsed: 103m 54s | Loss: 5.161 | Train BLEU: 0.1724\n",
      "Time Elapsed: 112m 39s | Loss: 4.663\n",
      "Time Elapsed: 121m 27s | Loss: 4.642\n",
      "Time Elapsed: 130m 12s | Loss: 4.628\n",
      "Time Elapsed: 139m 0s | Loss: 4.611\n",
      "Time Elapsed: 147m 49s | Loss: 4.599\n",
      "Time Elapsed: 156m 32s | Loss: 4.586\n",
      "Time Elapsed: 165m 15s | Loss: 4.573\n",
      "Time Elapsed: 172m 35s | Loss: 4.561\n",
      "Time Elapsed: 176m 38s | Loss: 4.551\n",
      "Time Elapsed: 181m 49s | Loss: 4.539\n",
      "Time Elapsed: 190m 33s | Loss: 4.527\n",
      "Epoch: 2 | Time Elapsed: 210m 49s | Loss: 4.521 | Train BLEU: 1.213\n",
      "Time Elapsed: 219m 39s | Loss: 4.38\n",
      "Time Elapsed: 228m 25s | Loss: 4.369\n",
      "Time Elapsed: 237m 7s | Loss: 4.36\n",
      "Time Elapsed: 245m 47s | Loss: 4.351\n",
      "Time Elapsed: 254m 26s | Loss: 4.341\n",
      "Time Elapsed: 259m 23s | Loss: 4.338\n",
      "Time Elapsed: 263m 23s | Loss: 4.333\n",
      "Time Elapsed: 270m 40s | Loss: 4.328\n",
      "Time Elapsed: 279m 19s | Loss: 4.322\n",
      "Time Elapsed: 288m 0s | Loss: 4.316\n",
      "Time Elapsed: 296m 41s | Loss: 4.308\n",
      "Epoch: 3 | Time Elapsed: 316m 20s | Loss: 4.304 | Train BLEU: 1.125\n",
      "Time Elapsed: 325m 4s | Loss: 4.22\n",
      "Time Elapsed: 333m 50s | Loss: 4.199\n",
      "Time Elapsed: 341m 50s | Loss: 4.197\n",
      "Time Elapsed: 345m 53s | Loss: 4.194\n",
      "Time Elapsed: 350m 19s | Loss: 4.189\n",
      "Time Elapsed: 359m 1s | Loss: 4.181\n",
      "Time Elapsed: 367m 48s | Loss: 4.177\n",
      "Time Elapsed: 376m 43s | Loss: 4.171\n",
      "Time Elapsed: 385m 30s | Loss: 4.165\n",
      "Time Elapsed: 394m 19s | Loss: 4.159\n",
      "Time Elapsed: 403m 10s | Loss: 4.153\n",
      "Epoch: 4 | Time Elapsed: 423m 36s | Loss: 4.15 | Train BLEU: 1.589\n",
      "Time Elapsed: 429m 14s | Loss: 4.068\n",
      "Time Elapsed: 433m 15s | Loss: 4.057\n",
      "Time Elapsed: 439m 29s | Loss: 4.049\n",
      "Time Elapsed: 448m 19s | Loss: 4.049\n",
      "Time Elapsed: 457m 9s | Loss: 4.042\n",
      "Time Elapsed: 465m 58s | Loss: 4.042\n",
      "Time Elapsed: 474m 44s | Loss: 4.036\n",
      "Time Elapsed: 483m 34s | Loss: 4.031\n",
      "Time Elapsed: 492m 24s | Loss: 4.026\n",
      "Time Elapsed: 501m 11s | Loss: 4.022\n",
      "Time Elapsed: 509m 51s | Loss: 4.018\n",
      "Epoch: 5 | Time Elapsed: 520m 14s | Loss: 4.013 | Train BLEU: 3.363\n",
      "Time Elapsed: 525m 56s | Loss: 3.94\n",
      "Time Elapsed: 534m 35s | Loss: 3.933\n",
      "Time Elapsed: 543m 12s | Loss: 3.926\n",
      "Time Elapsed: 551m 53s | Loss: 3.918\n",
      "Time Elapsed: 560m 29s | Loss: 3.914\n",
      "Time Elapsed: 569m 10s | Loss: 3.907\n",
      "Time Elapsed: 577m 52s | Loss: 3.903\n",
      "Time Elapsed: 586m 35s | Loss: 3.898\n",
      "Time Elapsed: 595m 13s | Loss: 3.895\n",
      "Time Elapsed: 603m 12s | Loss: 3.888\n",
      "Time Elapsed: 608m 44s | Loss: 3.882\n",
      "Epoch: 6 | Time Elapsed: 620m 6s | Loss: 3.878 | Train BLEU: 4.745\n",
      "Time Elapsed: 628m 48s | Loss: 3.799\n",
      "Time Elapsed: 637m 25s | Loss: 3.784\n",
      "Time Elapsed: 646m 3s | Loss: 3.779\n",
      "Time Elapsed: 654m 44s | Loss: 3.77\n",
      "Time Elapsed: 663m 24s | Loss: 3.763\n",
      "Time Elapsed: 672m 7s | Loss: 3.761\n",
      "Time Elapsed: 680m 47s | Loss: 3.757\n",
      "Time Elapsed: 689m 30s | Loss: 3.753\n",
      "Time Elapsed: 694m 49s | Loss: 3.748\n",
      "Time Elapsed: 698m 51s | Loss: 3.747\n",
      "Time Elapsed: 705m 1s | Loss: 3.742\n",
      "Epoch: 7 | Time Elapsed: 725m 25s | Loss: 3.739 | Train BLEU: 5.528\n",
      "Time Elapsed: 734m 5s | Loss: 3.654\n",
      "Time Elapsed: 742m 46s | Loss: 3.646\n",
      "Time Elapsed: 751m 29s | Loss: 3.644\n",
      "Time Elapsed: 760m 10s | Loss: 3.633\n",
      "Time Elapsed: 768m 55s | Loss: 3.629\n",
      "Time Elapsed: 776m 36s | Loss: 3.626\n",
      "Time Elapsed: 781m 21s | Loss: 3.624\n",
      "Time Elapsed: 785m 25s | Loss: 3.619\n",
      "Time Elapsed: 792m 51s | Loss: 3.616\n",
      "Time Elapsed: 801m 40s | Loss: 3.615\n",
      "Time Elapsed: 810m 25s | Loss: 3.613\n",
      "Epoch: 8 | Time Elapsed: 830m 44s | Loss: 3.61 | Train BLEU: 7.61\n",
      "Time Elapsed: 839m 33s | Loss: 3.517\n",
      "Time Elapsed: 848m 23s | Loss: 3.52\n",
      "Time Elapsed: 857m 9s | Loss: 3.523\n",
      "Time Elapsed: 864m 6s | Loss: 3.518\n",
      "Time Elapsed: 868m 14s | Loss: 3.52\n",
      "Time Elapsed: 873m 10s | Loss: 3.519\n",
      "Time Elapsed: 881m 8s | Loss: 3.515\n",
      "Time Elapsed: 889m 54s | Loss: 3.514\n",
      "Time Elapsed: 898m 41s | Loss: 3.514\n",
      "Time Elapsed: 907m 26s | Loss: 3.511\n",
      "Time Elapsed: 916m 10s | Loss: 3.508\n",
      "Epoch: 9 | Time Elapsed: 936m 40s | Loss: 3.505 | Train BLEU: 4.058\n",
      "Time Elapsed: 945m 18s | Loss: 3.432\n",
      "Time Elapsed: 951m 1s | Loss: 3.426\n",
      "Time Elapsed: 955m 2s | Loss: 3.416\n",
      "Time Elapsed: 960m 42s | Loss: 3.418\n",
      "Time Elapsed: 969m 23s | Loss: 3.419\n",
      "Time Elapsed: 978m 12s | Loss: 3.42\n",
      "Time Elapsed: 986m 58s | Loss: 3.419\n",
      "Time Elapsed: 995m 43s | Loss: 3.418\n",
      "Time Elapsed: 1004m 25s | Loss: 3.416\n",
      "Time Elapsed: 1013m 7s | Loss: 3.416\n",
      "Time Elapsed: 1021m 49s | Loss: 3.414\n",
      "Epoch: 10 | Time Elapsed: 1036m 58s | Loss: 3.412 | Train BLEU: 8.396\n",
      "Time Elapsed: 1041m 11s | Loss: 3.327\n",
      "Time Elapsed: 1047m 1s | Loss: 3.322\n",
      "Time Elapsed: 1054m 9s | Loss: 3.327\n",
      "Time Elapsed: 1062m 46s | Loss: 3.328\n",
      "Time Elapsed: 1071m 27s | Loss: 3.332\n",
      "Time Elapsed: 1080m 5s | Loss: 3.329\n",
      "Time Elapsed: 1088m 49s | Loss: 3.325\n",
      "Time Elapsed: 1097m 39s | Loss: 3.322\n",
      "Time Elapsed: 1106m 25s | Loss: 3.322\n",
      "Time Elapsed: 1115m 11s | Loss: 3.321\n",
      "Time Elapsed: 1123m 6s | Loss: 3.32\n",
      "Epoch: 11 | Time Elapsed: 1131m 55s | Loss: 3.317 | Train BLEU: 11.53\n",
      "Time Elapsed: 1138m 2s | Loss: 3.258\n",
      "Time Elapsed: 1145m 54s | Loss: 3.241\n",
      "Time Elapsed: 1154m 36s | Loss: 3.249\n",
      "Time Elapsed: 1163m 21s | Loss: 3.246\n",
      "Time Elapsed: 1172m 6s | Loss: 3.244\n",
      "Time Elapsed: 1180m 52s | Loss: 3.243\n",
      "Time Elapsed: 1189m 39s | Loss: 3.243\n",
      "Time Elapsed: 1198m 22s | Loss: 3.242\n"
     ]
    }
   ],
   "source": [
    "fitAttention(train_loader, dev_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, BATCH_SIZE, 100, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model Training + Eval...\n",
      "Time Elapsed: 4m 13s | Loss: 5.741\n",
      "Time Elapsed: 8m 16s | Loss: 5.55\n",
      "Time Elapsed: 12m 10s | Loss: 5.456\n",
      "Time Elapsed: 16m 24s | Loss: 5.368\n",
      "Time Elapsed: 20m 26s | Loss: 5.299\n",
      "Time Elapsed: 24m 18s | Loss: 5.24\n",
      "Time Elapsed: 28m 30s | Loss: 5.197\n",
      "Time Elapsed: 32m 36s | Loss: 5.156\n",
      "Time Elapsed: 36m 25s | Loss: 5.124\n",
      "Time Elapsed: 40m 37s | Loss: 5.094\n",
      "Time Elapsed: 44m 45s | Loss: 5.067\n",
      "Epoch: 1 | Time Elapsed: 46m 32s | Loss: 5.054 | Train BLEU: 1.563 | Dev BLEU: 1.069\n",
      "Time Elapsed: 50m 37s | Loss: 4.608\n",
      "Time Elapsed: 54m 48s | Loss: 4.597\n",
      "Time Elapsed: 58m 31s | Loss: 4.575\n",
      "Time Elapsed: 62m 43s | Loss: 4.55\n",
      "Time Elapsed: 66m 54s | Loss: 4.54\n",
      "Time Elapsed: 70m 39s | Loss: 4.529\n",
      "Time Elapsed: 74m 51s | Loss: 4.518\n",
      "Time Elapsed: 79m 3s | Loss: 4.51\n",
      "Time Elapsed: 82m 48s | Loss: 4.501\n",
      "Time Elapsed: 86m 58s | Loss: 4.493\n",
      "Time Elapsed: 91m 10s | Loss: 4.486\n",
      "Epoch: 2 | Time Elapsed: 93m 10s | Loss: 4.481 | Train BLEU: 2.315 | Dev BLEU: 2.702\n",
      "Time Elapsed: 97m 1s | Loss: 4.145\n",
      "Time Elapsed: 101m 14s | Loss: 4.15\n",
      "Time Elapsed: 105m 21s | Loss: 4.16\n",
      "Time Elapsed: 109m 8s | Loss: 4.163\n",
      "Time Elapsed: 113m 21s | Loss: 4.168\n",
      "Time Elapsed: 117m 30s | Loss: 4.18\n",
      "Time Elapsed: 121m 14s | Loss: 4.189\n",
      "Time Elapsed: 125m 27s | Loss: 4.194\n",
      "Time Elapsed: 129m 40s | Loss: 4.198\n",
      "Time Elapsed: 133m 25s | Loss: 4.2\n",
      "Time Elapsed: 137m 38s | Loss: 4.204\n",
      "Epoch: 3 | Time Elapsed: 139m 49s | Loss: 4.203 | Train BLEU: 2.007 | Dev BLEU: 6.053\n",
      "Time Elapsed: 143m 35s | Loss: 3.935\n",
      "Time Elapsed: 147m 49s | Loss: 3.957\n",
      "Time Elapsed: 152m 0s | Loss: 3.968\n",
      "Time Elapsed: 155m 44s | Loss: 3.975\n",
      "Time Elapsed: 159m 57s | Loss: 3.984\n",
      "Time Elapsed: 164m 8s | Loss: 3.991\n",
      "Time Elapsed: 167m 55s | Loss: 4.001\n",
      "Time Elapsed: 172m 7s | Loss: 4.008\n",
      "Time Elapsed: 176m 20s | Loss: 4.015\n",
      "Time Elapsed: 180m 10s | Loss: 4.022\n",
      "Time Elapsed: 184m 18s | Loss: 4.028\n",
      "Epoch: 4 | Time Elapsed: 186m 23s | Loss: 4.029 | Train BLEU: 1.628 | Dev BLEU: 2.186\n",
      "Time Elapsed: 190m 30s | Loss: 3.786\n",
      "Time Elapsed: 194m 21s | Loss: 3.806\n",
      "Time Elapsed: 198m 35s | Loss: 3.824\n",
      "Time Elapsed: 202m 44s | Loss: 3.839\n",
      "Time Elapsed: 206m 31s | Loss: 3.855\n",
      "Time Elapsed: 210m 43s | Loss: 3.868\n",
      "Time Elapsed: 214m 53s | Loss: 3.879\n",
      "Time Elapsed: 218m 39s | Loss: 3.889\n",
      "Time Elapsed: 222m 52s | Loss: 3.896\n",
      "Time Elapsed: 226m 55s | Loss: 3.905\n",
      "Time Elapsed: 230m 49s | Loss: 3.912\n",
      "Epoch: 5 | Time Elapsed: 232m 51s | Loss: 3.913 | Train BLEU: 4.11 | Dev BLEU: 3.036\n",
      "Time Elapsed: 237m 4s | Loss: 3.715\n",
      "Time Elapsed: 240m 48s | Loss: 3.734\n",
      "Time Elapsed: 244m 59s | Loss: 3.747\n",
      "Time Elapsed: 249m 11s | Loss: 3.761\n",
      "Time Elapsed: 252m 57s | Loss: 3.777\n",
      "Time Elapsed: 257m 6s | Loss: 3.788\n",
      "Time Elapsed: 261m 20s | Loss: 3.801\n",
      "Time Elapsed: 265m 13s | Loss: 3.812\n",
      "Time Elapsed: 269m 16s | Loss: 3.825\n",
      "Time Elapsed: 273m 29s | Loss: 3.836\n",
      "Time Elapsed: 277m 23s | Loss: 3.846\n",
      "Epoch: 6 | Time Elapsed: 279m 20s | Loss: 3.847 | Train BLEU: 4.179 | Dev BLEU: 3.274\n",
      "Time Elapsed: 283m 32s | Loss: 3.66\n",
      "Time Elapsed: 287m 43s | Loss: 3.681\n",
      "Time Elapsed: 291m 29s | Loss: 3.695\n",
      "Time Elapsed: 295m 41s | Loss: 3.715\n",
      "Time Elapsed: 299m 54s | Loss: 3.729\n",
      "Time Elapsed: 303m 36s | Loss: 3.74\n",
      "Time Elapsed: 307m 49s | Loss: 3.753\n",
      "Time Elapsed: 311m 56s | Loss: 3.762\n",
      "Time Elapsed: 315m 41s | Loss: 3.772\n",
      "Time Elapsed: 319m 51s | Loss: 3.782\n",
      "Time Elapsed: 323m 58s | Loss: 3.791\n",
      "Epoch: 7 | Time Elapsed: 325m 47s | Loss: 3.794 | Train BLEU: 2.771 | Dev BLEU: 5.678\n",
      "Time Elapsed: 329m 50s | Loss: 3.587\n",
      "Time Elapsed: 334m 0s | Loss: 3.618\n",
      "Time Elapsed: 337m 56s | Loss: 3.637\n",
      "Time Elapsed: 341m 53s | Loss: 3.665\n",
      "Time Elapsed: 346m 2s | Loss: 3.687\n",
      "Time Elapsed: 349m 59s | Loss: 3.711\n",
      "Time Elapsed: 353m 54s | Loss: 3.725\n",
      "Time Elapsed: 358m 5s | Loss: 3.742\n",
      "Time Elapsed: 362m 4s | Loss: 3.753\n",
      "Time Elapsed: 365m 57s | Loss: 3.764\n",
      "Time Elapsed: 370m 8s | Loss: 3.773\n",
      "Epoch: 8 | Time Elapsed: 372m 13s | Loss: 3.775 | Train BLEU: 2.516 | Dev BLEU: 5.869\n",
      "Time Elapsed: 375m 59s | Loss: 3.588\n",
      "Time Elapsed: 380m 10s | Loss: 3.608\n",
      "Time Elapsed: 384m 21s | Loss: 3.627\n",
      "Time Elapsed: 388m 4s | Loss: 3.645\n",
      "Time Elapsed: 392m 13s | Loss: 3.66\n",
      "Time Elapsed: 396m 23s | Loss: 3.678\n",
      "Time Elapsed: 400m 4s | Loss: 3.692\n",
      "Time Elapsed: 404m 13s | Loss: 3.704\n",
      "Time Elapsed: 408m 24s | Loss: 3.716\n",
      "Time Elapsed: 412m 6s | Loss: 3.729\n",
      "Time Elapsed: 416m 16s | Loss: 3.74\n",
      "Epoch: 9 | Time Elapsed: 418m 15s | Loss: 3.742 | Train BLEU: 2.775 | Dev BLEU: 5.751\n",
      "Time Elapsed: 422m 18s | Loss: 3.598\n",
      "Time Elapsed: 426m 7s | Loss: 3.616\n",
      "Time Elapsed: 430m 16s | Loss: 3.63\n",
      "Time Elapsed: 434m 22s | Loss: 3.644\n",
      "Time Elapsed: 438m 10s | Loss: 3.659\n",
      "Time Elapsed: 442m 20s | Loss: 3.669\n",
      "Time Elapsed: 446m 27s | Loss: 3.678\n",
      "Time Elapsed: 450m 12s | Loss: 3.687\n",
      "Time Elapsed: 454m 20s | Loss: 3.697\n",
      "Time Elapsed: 458m 31s | Loss: 3.707\n",
      "Time Elapsed: 462m 15s | Loss: 3.716\n",
      "Epoch: 10 | Time Elapsed: 464m 16s | Loss: 3.718 | Train BLEU: 5.838 | Dev BLEU: 3.0\n"
     ]
    }
   ],
   "source": [
    "# vi - en non-hybrid embeddings\n",
    "fit(train_loader, dev_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, BATCH_SIZE, 10, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(encoder, open(\"./hybrid-vi-encoder.p\", \"wb\"))\n",
    "pkl.dump(\"hi\", open(\"./test.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overfit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-b0423e06f8c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbleuEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-90-89c587f1deef>\u001b[0m in \u001b[0;36mbleuEval\u001b[0;34m(encoder, decoder, data_loader, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrue_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-89c587f1deef>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrue_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-89c587f1deef>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrue_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bleuEval(encoder, decoder, train_loader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./vi-encoder.p', 'rb') as pickle_file:\n",
    "    baseline_enc = pkl.load(pickle_file)\n",
    "with open('./vi-decoder.p', 'rb') as pickle_file:\n",
    "    baseline_dec = pkl.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample True:  <sos> in 4 minutes , atmospheric chemist <unk> pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> : the frequent gradients , the frequent revolution -- the themes of the world . <eos>\n",
      "Sample Predicted:  <sos> we have to introduce the <unk> <unk> and the <unk> . <eos>\n",
      "Sample Predicted:  <sos> so we went to the <unk> , and we were buddhist <unk> . <eos>\n",
      "Sample Predicted:  <sos> i went to the other problems , and i m able to solve the other problems . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> , the <unk> , the <unk> , the <unk> , the <unk> . <eos>\n",
      "Sample Predicted:  <sos> we re able to do the biggest challenge , we re able to do the biggest challenge . <eos>\n",
      "Sample Predicted:  <sos> i m sure that , i m sure that i m going to see the <unk> . <eos>\n",
      "Sample Predicted:  <sos> but the <unk> , but the <unk> , but the <unk> , but the <unk> . <eos>\n",
      "Sample Predicted:  <sos> i want to show you to do the biggest areas of the world . <eos>\n",
      "Sample Predicted:  <sos> so , and the other thing , and they don t know , and they don t know what will happen . <eos>\n",
      "Sample Predicted:  <sos> we also found the answer , we can t have to do the same thing . <eos>\n",
      "Sample Predicted:  <sos> we re going to be able to do the world . <eos>\n",
      "Sample Predicted:  <sos> so , and the problem , and the problem of the problem of the world . <eos>\n",
      "Sample Predicted:  <sos> so , as the in-situ , the other thing , the other thing , the other thing . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> : <unk> <unk> . <eos>\n",
      "Sample Predicted:  <sos> and the mainframe temple vessels are the world . <eos>\n",
      "Sample Predicted:  <sos> we re able to do the world . <eos>\n",
      "Sample Predicted:  <sos> it grows 14 feet , it grows 14 feet . <eos>\n",
      "Sample Predicted:  <sos> i m going to be able to do the <unk> . <eos>\n",
      "Sample Predicted:  <sos> we re able to do the world . <eos>\n",
      "Sample Predicted:  <sos> it s a lot of the soprano place . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> , the <unk> , the <unk> dative . <eos>\n",
      "Sample Predicted:  <sos> so , the first thing is the first thing , the first thing is the world . <eos>\n",
      "Sample Predicted:  <sos> so it s a righty . <eos>\n",
      "Sample Predicted:  <sos> so , the first thing , the first thing , the first thing is the world . <eos>\n",
      "Sample Predicted:  <sos> so the mainframe <unk> <unk> . <eos>\n",
      "Sample Predicted:  <sos> <unk> <unk> : <unk> <unk> . <eos>\n",
      "Sample Predicted:  <sos> we ve got to induce of the world . <eos>\n",
      "Sample Predicted:  <sos> we narrowed it , and we can t have to induce . <eos>\n",
      "Sample Predicted:  <sos> we can t have assessed . <eos>\n",
      "Sample Predicted:  <sos> the problem of the problem . <eos>\n",
      "Sample Predicted:  <sos> so the problem of the problem . <eos>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4825853903799304"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleuEval(baseline_enc, baseline_dec, overfit_loader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.gru.batch_fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m(143)\u001b[0;36mcheck_hidden_size\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    141 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0mcheck_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Expected hidden size {}, got {}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    142 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 143 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    144 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    145 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'encoder' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-31-f7ca4d53521a>\u001b[0m(15)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     13 \u001b[0;31m        \u001b[0;31m# input batch_size * 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mec_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 15 \u001b[0;31m        \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mec_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m        \u001b[0mencoder_hiddens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mec_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  enoder_outputs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'enoder_outputs' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  encoder_outputs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]],\n",
      "\n",
      "         [[ 0.0170,  0.1717,  0.3893,  ...,  0.0381, -0.1810, -0.2559]]],\n",
      "\n",
      "\n",
      "        [[[-0.0219, -0.0105, -0.2507,  ...,  0.1992, -0.4429, -0.4246]],\n",
      "\n",
      "         [[-0.0108,  0.4253,  0.3838,  ..., -0.2613,  0.1234, -0.1874]],\n",
      "\n",
      "         [[-0.2046, -0.4536,  0.5676,  ...,  0.0421,  0.3484, -0.3940]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0036, -0.4598,  0.2498,  ..., -0.1157,  0.1377,  0.2163]],\n",
      "\n",
      "         [[-0.0036, -0.4598,  0.2498,  ..., -0.1157,  0.1377,  0.2163]],\n",
      "\n",
      "         [[-0.0856,  0.0131,  0.2655,  ...,  0.2046, -0.2525, -0.0867]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4918, -0.1363,  0.1370,  ...,  0.0146, -0.2607, -0.5579]],\n",
      "\n",
      "         [[-0.1242,  0.3349,  0.5718,  ..., -0.3327,  0.2256, -0.0852]],\n",
      "\n",
      "         [[-0.3152, -0.5568,  0.6375,  ...,  0.0403,  0.5346, -0.4440]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0275,  0.3268, -0.1821,  ..., -0.0095,  0.1823,  0.4863]],\n",
      "\n",
      "         [[ 0.0017, -0.0015,  0.0989,  ..., -0.2592,  0.0363,  0.3519]],\n",
      "\n",
      "         [[ 0.1362,  0.0032,  0.2977,  ..., -0.0740, -0.0210, -0.0719]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1357,  0.1549,  0.2249,  ...,  0.0175,  0.4587,  0.1154]],\n",
      "\n",
      "         [[-0.1083, -0.2688,  0.4672,  ...,  0.2151, -0.3381,  0.1786]],\n",
      "\n",
      "         [[ 0.2141, -0.3156,  0.1468,  ...,  0.4485, -0.3331, -0.3143]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0820, -0.9278,  0.6974,  ...,  0.4278,  0.5836, -0.7664]],\n",
      "\n",
      "         [[-0.0804, -0.9272,  0.6960,  ...,  0.4278,  0.5823, -0.7665]],\n",
      "\n",
      "         [[-0.0783, -0.9280,  0.6984,  ...,  0.4253,  0.5912, -0.7671]]],\n",
      "\n",
      "\n",
      "        [[[-0.3314,  0.3519,  0.2042,  ..., -0.6216,  0.3659,  0.0270]],\n",
      "\n",
      "         [[-0.5117, -0.0432, -0.2642,  ...,  0.3886, -0.2846, -0.2133]],\n",
      "\n",
      "         [[ 0.2323, -0.4987, -0.0415,  ..., -0.0950, -0.3297, -0.1025]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0801, -0.9281,  0.6986,  ...,  0.4258,  0.5901, -0.7666]],\n",
      "\n",
      "         [[-0.0789, -0.9276,  0.6976,  ...,  0.4258,  0.5892, -0.7667]],\n",
      "\n",
      "         [[-0.0773, -0.9281,  0.6992,  ...,  0.4241,  0.5948, -0.7671]]],\n",
      "\n",
      "\n",
      "        [[[-0.1126, -0.0602,  0.0442,  ..., -0.0315,  0.2195, -0.5180]],\n",
      "\n",
      "         [[-0.1126,  0.0772, -0.5050,  ..., -0.0286, -0.3123,  0.3051]],\n",
      "\n",
      "         [[ 0.0513, -0.1630, -0.1074,  ...,  0.0284,  0.2293, -0.2188]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0788, -0.9282,  0.6993,  ...,  0.4244,  0.5944, -0.7668]],\n",
      "\n",
      "         [[-0.0778, -0.9278,  0.6987,  ...,  0.4244,  0.5938, -0.7668]],\n",
      "\n",
      "         [[-0.0766, -0.9281,  0.6997,  ...,  0.4233,  0.5973, -0.7671]]]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  len(encoder_outputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  encoder_outputs[23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** IndexError: index 23 is out of bounds for dimension 0 with size 23\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict:  ['<sos>', '<sos>', '<sos>', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  8.810749053955078\n",
      "Predict:  ['<sos>', '<sos>', 'the', 'the', 'the', 'two', 'the', 'the', 'the', 'freaky']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  4.840537643432617\n",
      "Predict:  ['<sos>', '<sos>', 's', 'just', 'these', 'two', 'the', 'the', 'the', 'freaky']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  1.9151222229003906\n",
      "Predict:  ['<sos>', '<sos>', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.9088132858276368\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.5086045742034913\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.3032816410064697\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.18692054748535156\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.12194809913635254\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.08495545387268066\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.06054344177246094\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.043548297882080075\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.03167123794555664\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.023690128326416017\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Loss:  0.017924118041992187\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f6d188e37f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverfit_vi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverfit_en_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a1868babeca0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train_data, target_data, encoder, decoder, encoder_opt, decoder_opt, criterion, batch_size, epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3b1f9816ee80>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_input, train_target, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtotal_avg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit(overfit_vi_train, overfit_en_train, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, BATCH_SIZE, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  8.852307891845703\n",
      "Predict:  ['<sos>', '<sos>', 'the', 'just', 'these', 'the', 'notes', 'the', 'the', 'the']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  4.9451759338378904\n",
      "Predict:  ['<sos>', '<sos>', 'coming', 'just', 'these', 'notes', 'notes', 'the', 'the', 'the']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  1.9719776153564452\n",
      "Predict:  ['<sos>', '<sos>', 's', 'just', 'these', 'two', 'notes', 'the', 'the', 'freaky']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.9234449386596679\n",
      "Predict:  ['<sos>', '<sos>', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'freaky']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.5161348819732666\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.3111863613128662\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.19171581268310547\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.1251605987548828\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.08669190406799317\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.06085610389709473\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.043444252014160155\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0317835807800293\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.02393932342529297\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.018381881713867187\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.014472675323486329\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.011426544189453125\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.009125995635986327\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0074138641357421875\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.006087779998779297\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.005056858062744141\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.004248332977294922\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0036081314086914063\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0030983924865722657\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0026922225952148438\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0023674964904785156\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.002105426788330078\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.001891326904296875\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0017138481140136718\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0015657424926757812\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0014398574829101562\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.00133209228515625\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0012396812438964845\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0011590957641601563\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.001088714599609375\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0010272979736328125\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0009730339050292968\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0009249687194824219\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0008824348449707031\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.000844573974609375\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0008105278015136719\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0007798194885253907\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.00075225830078125\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0007273674011230469\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0007046699523925781\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0006839752197265625\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0006653785705566407\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0006479263305664062\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0006319999694824219\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.000617218017578125\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.000603485107421875\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005910873413085937\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005793571472167969\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005681991577148437\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005581855773925781\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.000548553466796875\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005395889282226562\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005313873291015625\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005230903625488281\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n",
      "Training Loss:  0.0005156517028808594\n",
      "Predict:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Actual:  ['<sos>', 'it', 's', 'just', 'these', 'two', 'notes', 'in', 'the', 'middle']\n",
      "Training Complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-78ec3519bd0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverfit_vi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverfit_en_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-256264019c3a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_input, train_target, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/NLP/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for i in range(1000):\n",
    "#     train(overfit_vi_train, overfit_en_train, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(train_input, train_target, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size):\n",
    "#     # Batch\n",
    "#     total_avg_loss = 0\n",
    "#     for i in range(len(train_input) // batch_size):\n",
    "#         loss = 0\n",
    "#         encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "#         batch = get_batch(i, batch_size, train_input, train_target)\n",
    "#         # size batch_size * seq_length\n",
    "#         batch_input = torch.tensor(batch[0], device=device)\n",
    "#         batch_target = torch.tensor(batch[1], device=device)\n",
    "#         input_length = batch_input.shape[1] ## should be seq length\n",
    "#         target_length = batch_target.shape[1]\n",
    "#         print(input_length, target_length)\n",
    "\n",
    "#         encoder_optimizer.zero_grad()\n",
    "#         decoder_optimizer.zero_grad()\n",
    "        \n",
    "#         encoder_outputs = torch.zeros(input_length, batch_size, 1, 256, device=device)\n",
    "#         encoder_hiddens = torch.zeros(input_length, 1, batch_size, 256, device=device)\n",
    "        \n",
    "#         # Encode\n",
    "#         for ec_idx in range(input_length):\n",
    "#             # input batch_size * 1\n",
    "#             encoder_output, encoder_hidden = encoder(batch_input[:, ec_idx].unsqueeze(1), encoder_hidden)\n",
    "#             encoder_outputs[ec_idx] = encoder_output\n",
    "#             encoder_hiddens[ec_idx] = encoder_hidden\n",
    "        \n",
    "#         # Decode\n",
    "#         decoder_input = torch.tensor([2] * batch_size, device=device) # SOS token 2\n",
    "#         decoder_hidden = encoder_hidden\n",
    "        \n",
    "#         ## Print Value\n",
    "#         sample_sentence = []\n",
    "        \n",
    "#         # Always use Teacher Forcing\n",
    "#         for dc_idx in range(target_length):\n",
    "#             decoder_output, decoder_hidden = decoder(decoder_input.unsqueeze(1), decoder_hidden)\n",
    "#             decoder_output = decoder_output.squeeze(1).to(device) # get rid of the seq dimention\n",
    "#             loss += criterion(decoder_output, batch_target[:, dc_idx])\n",
    "#             decoder_input = batch_target[:, dc_idx]\n",
    "            \n",
    "#             ## Print Value\n",
    "#             sample_sentence.append(torch.argmax(decoder_output[0]).item())\n",
    "            \n",
    "#         loss.backward()\n",
    "#         total_avg_loss += loss.item() / target_length\n",
    "        \n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "        \n",
    "# #         print('Training Loss: ', loss.item() / target_length)\n",
    "        \n",
    "#         ## Print Value\n",
    "#         print(\"Predict: \", ids2sentence(sample_sentence, id2word_en_dic))\n",
    "#         print(\"Actual: \", ids2sentence(batch_target[0].cpu().numpy(), id2word_en_dic))\n",
    "        \n",
    "#     return total_avg_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
