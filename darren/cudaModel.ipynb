{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, tensorToList\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir, em_dir = directories()\n",
    "\n",
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3\n",
    "NUM_SPECIAL = len(SPECIAL_SYMBOLS_ID)\n",
    "\n",
    "vi, en = loadLangPairs(\"vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(len(vi.train_num)) if (len(vi.train[i]) < vi.max_length) & (len(en.train[i]) < en.max_length)])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "dev_dataset = langDataset([(vi.dev_num[i], en.dev_num[i]) for i in range(len(vi.dev_num)) if (len(vi.dev[i]) < vi.max_length) & (len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.num_layers = params['num_layers']\n",
    "        \n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, inp, inp_lens, hidden=None):\n",
    "        embedded = self.embedding(inp)\n",
    "        packed = pack_padded_sequence(embedded, inp_lens)\n",
    "        \n",
    "        output, self.hidden = self.gru(packed, hidden)\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        return output, self.hidden\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.output_size = params['output_size']\n",
    "\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.num_layers, batch_first=True, bidirectional=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_output=None):\n",
    "        embedded = self.embedding(inp)\n",
    "        output = F.relu(embedded)\n",
    "        \n",
    "        output, self.hidden = self.gru(output, hidden)\n",
    "        orig = output\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        output = torch.exp(self.softmax(self.out(output))).squeeze(0)\n",
    "        return output, hidden, None\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1).to(device)))\n",
    "    loss = crossEntropy.masked_select(mask.to(device)).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, encoder, decoder, encoder_optim, decoder_optim):\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.encoder_optim = encoder_optim\n",
    "        self.decoder_optim = decoder_optim\n",
    "        \n",
    "    def fit(self, train_data, dev_data, teacher_forcing_ratio, n_epoch, print_every, n_grams):\n",
    "        start = time.time()\n",
    "        \n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.n_epoch = n_epoch\n",
    "        \n",
    "        \n",
    "        print(\"Initializing...\")\n",
    "        start_epoch = 1\n",
    "        print_loss_total = 0 \n",
    "        plot_loss_total = 0\n",
    "        plot_losses = []\n",
    "        plot_train_scores = []\n",
    "        plot_dev_scores = []\n",
    "        \n",
    "        for epoch in range(start_epoch, n_epoch):\n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "                loss = self.trainEpoch(inp, inp_lens, output, out_mask, out_max)\n",
    "                \n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "\n",
    "                if i % print_every == 0:\n",
    "                    train_score = self.bleuScore(train_loader, n_grams)\n",
    "                    dev_score = self.bleuScore(dev_loader, n_grams)\n",
    "                    plot_train_scores.append(train_score)\n",
    "                    plot_dev_scores.append(dev_score)\n",
    "                    plot_loss_avg = plot_loss_total / print_every\n",
    "                    plot_losses.append(plot_loss_avg)\n",
    "                    plot_loss_total = 0       \n",
    "\n",
    "                    print_loss_avg = print_loss_total / print_every\n",
    "                    print_loss_total = 0\n",
    "                    print(\"{} ({} {}) Iter: {}/{} | Loss:{:.4} | TrainScore:{} | DevScore:{}\".format(timeSince(start, epoch/n_epoch), \n",
    "                                                                                            epoch, \n",
    "                                                                                            epoch/n_epoch*100, \n",
    "                                                                                            i,\n",
    "                                                                                            len(train_loader),\n",
    "                                                                                            print_loss_avg, \n",
    "                                                                                            train_score,\n",
    "                                                                                            dev_score))\n",
    "\n",
    "#                 if i % plot_every == 0:\n",
    "#                     train_score = self.bleuScore(train_loader, n_grams)\n",
    "#                     dev_score = self.bleuScore(dev_loader, n_grams)\n",
    "#                     plot_train_scores.append(train_score)\n",
    "#                     plot_dev_scores.append(dev_score)\n",
    "#                     plot_loss_avg = plot_loss_total / plot_every\n",
    "#                     plot_losses.append(plot_loss_avg)\n",
    "#                     plot_loss_total = 0       \n",
    "                \n",
    "                \n",
    "        self.plot_losses = plot_losses\n",
    "        self.plot_train_scores = plot_train_scores\n",
    "        self.plot_dev_scores = plot_dev_scores\n",
    "        return \"Training Complete!\"            \n",
    "            \n",
    "    def trainEpoch(self, inp, inp_lens, output, out_mask, out_max):\n",
    "        self.encoder_optim.zero_grad()\n",
    "        self.decoder_optim.zero_grad()\n",
    "\n",
    "        loss, print_losses, n_totals = 0, [], 0\n",
    "        \n",
    "        encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "        \n",
    "        decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]]).to(device)\n",
    "        decoder_hidden = encoder_hidden[:,-1:,:].contiguous()\n",
    "\n",
    "        if random.random() < self.teacher_forcing_ratio:\n",
    "            for t in range(out_max):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                decoder_input = output[t].view(1, -1)\n",
    "                \n",
    "                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])\n",
    "                loss += mask_loss\n",
    "                print_losses.append(mask_loss.item() * nTotal)\n",
    "                n_totals += nTotal\n",
    "        else:\n",
    "            for t in range(out_max):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]]).to(device)\n",
    "                \n",
    "                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])\n",
    "                loss += mask_loss\n",
    "                print_losses.append(mask_loss.item() * nTotal)\n",
    "                n_totals += nTotal\n",
    "                \n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optim.step()\n",
    "        self.decoder_optim.step()\n",
    "\n",
    "        return sum(print_losses) / n_totals\n",
    "    \n",
    "    def bleuScore(self, data_loader, n_grams):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            true_outputs = []\n",
    "            decoder_outputs = []\n",
    "            \n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(data_loader):\n",
    "                if i * BATCH_SIZE > len(dev_loader) * BATCH_SIZE:\n",
    "                    break\n",
    "                true_outputs += tensorToList(output)\n",
    "                \n",
    "                encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "                \n",
    "                decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]])\n",
    "                decoder_hidden = encoder_hidden[:,-1:,:].contiguous()\n",
    "                \n",
    "                decoder_batch_outputs = []\n",
    "                decoder_batch_outputs += decoder_input.tolist()\n",
    "                \n",
    "                for t in range(out_max):\n",
    "                    decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                    _, topi = decoder_output.topk(1)\n",
    "                    decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]])\n",
    "                    decoder_batch_outputs += decoder_input.tolist()\n",
    "                    \n",
    "                decoder_outputs += tensorToList(torch.tensor(decoder_batch_outputs))\n",
    "        return corpus_bleu(decoder_outputs, true_outputs, n_grams)\n",
    "\n",
    "    def showLoss(self):\n",
    "        plt.figure()\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        fig_plt = sns.lineplot(x=np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), y=self.plot_losses)\n",
    "        fig_plt.set_title(\"Loss Over Time\")\n",
    "        fig_plt.set_ylabel(\"Loss\")\n",
    "        fig_plt.set_xlabel(\"Epochs\")\n",
    "        return fig_plt.get_figure()\n",
    "    \n",
    "    def showScore(self):\n",
    "        df = pd.concat([pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_train_scores, 'Score':'Train'}), \n",
    "                        pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_dev_scores, 'Score':'Dev'})], axis=0)\n",
    "    \n",
    "        plt.figure()\n",
    "        pp = sns.lineplot(data=df, x = 'X', y = 'Y', hue='Score', style=\"Score\", legend= \"brief\")\n",
    "        fig_plt.set_title(\"Score Over Time\")\n",
    "        fig_plt.set_ylabel(\"Score\")\n",
    "        fig_plt.set_xlabel(\"Epoch\")\n",
    "        return fig_plt.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "encoder_params = {'hidden_size':256, 'num_layers':1}\n",
    "decoder_params = {'hidden_size':encoder_params['hidden_size'], 'num_layers':1, 'output_size':en.n_words}\n",
    "\n",
    "encoder = EncoderRNN(encoder_params, vi.emb, vi.learn_ids).to(device)\n",
    "encoder_optim = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "decoder = DecoderRNN(decoder_params, en.emb, en.learn_ids).to(device)\n",
    "decoder_optim = optim.Adam(decoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "0m 15s (- 2m 15s) (1 10.0) Iter: 0/1713 | Loss:0.01039 | TrainScore:9.301821611466174 | DevScore:10.144865992263254\n",
      "6m 18s (- 56m 42s) (1 10.0) Iter: 400/1713 | Loss:3.481 | TrainScore:3.9491317896699902 | DevScore:3.4373696280375103\n",
      "12m 21s (- 111m 10s) (1 10.0) Iter: 800/1713 | Loss:3.195 | TrainScore:2.9839270337976247 | DevScore:2.741219284529995\n",
      "18m 25s (- 165m 49s) (1 10.0) Iter: 1200/1713 | Loss:3.132 | TrainScore:1.402114597097759 | DevScore:1.476456197084048\n",
      "24m 30s (- 220m 37s) (1 10.0) Iter: 1600/1713 | Loss:3.099 | TrainScore:1.0247680801474728 | DevScore:0.937463584188316\n",
      "26m 21s (- 105m 27s) (2 20.0) Iter: 0/1713 | Loss:0.8663 | TrainScore:0.7355134314590411 | DevScore:0.7061211883251046\n",
      "32m 25s (- 129m 42s) (2 20.0) Iter: 400/1713 | Loss:3.031 | TrainScore:0.6045048829654783 | DevScore:0.8465479049353456\n",
      "38m 29s (- 153m 59s) (2 20.0) Iter: 800/1713 | Loss:3.027 | TrainScore:0.430588790903775 | DevScore:0.4009448010837915\n",
      "44m 34s (- 178m 18s) (2 20.0) Iter: 1200/1713 | Loss:3.021 | TrainScore:0.6896377234874003 | DevScore:0.6633675811526206\n",
      "50m 37s (- 202m 30s) (2 20.0) Iter: 1600/1713 | Loss:3.014 | TrainScore:0.6862591268417261 | DevScore:0.8590360894058751\n",
      "52m 29s (- 122m 27s) (3 30.0) Iter: 0/1713 | Loss:0.847 | TrainScore:1.0706205075388604 | DevScore:1.1980895360860881\n",
      "58m 34s (- 136m 40s) (3 30.0) Iter: 400/1713 | Loss:2.955 | TrainScore:0.8933152743360675 | DevScore:1.1104067516575693\n",
      "64m 39s (- 150m 51s) (3 30.0) Iter: 800/1713 | Loss:2.955 | TrainScore:1.3035661526750635 | DevScore:1.0761902035916213\n",
      "70m 43s (- 165m 0s) (3 30.0) Iter: 1200/1713 | Loss:2.964 | TrainScore:1.2228818789218265 | DevScore:1.1820538176967748\n",
      "76m 46s (- 179m 8s) (3 30.0) Iter: 1600/1713 | Loss:2.961 | TrainScore:1.261292150689597 | DevScore:1.2544234564870729\n",
      "78m 37s (- 117m 56s) (4 40.0) Iter: 0/1713 | Loss:0.8355 | TrainScore:1.2307149448351342 | DevScore:1.3087255399807294\n",
      "84m 41s (- 127m 2s) (4 40.0) Iter: 400/1713 | Loss:2.913 | TrainScore:0.952116669668053 | DevScore:1.309404800620031\n",
      "90m 47s (- 136m 10s) (4 40.0) Iter: 800/1713 | Loss:2.919 | TrainScore:0.782194995136758 | DevScore:1.200373605591051\n",
      "96m 50s (- 145m 16s) (4 40.0) Iter: 1200/1713 | Loss:2.922 | TrainScore:0.7536919047422265 | DevScore:0.47590848528524443\n",
      "102m 54s (- 154m 21s) (4 40.0) Iter: 1600/1713 | Loss:2.923 | TrainScore:0.564422205074967 | DevScore:0.6417374918349669\n",
      "104m 46s (- 104m 46s) (5 50.0) Iter: 0/1713 | Loss:0.8224 | TrainScore:0.5488775273417978 | DevScore:0.6769991895673787\n",
      "110m 49s (- 110m 49s) (5 50.0) Iter: 400/1713 | Loss:2.883 | TrainScore:0.7491739962095747 | DevScore:0.7060800130693187\n",
      "116m 55s (- 116m 55s) (5 50.0) Iter: 800/1713 | Loss:2.891 | TrainScore:0.6559042609003284 | DevScore:0.5194618643468599\n",
      "123m 1s (- 123m 1s) (5 50.0) Iter: 1200/1713 | Loss:2.895 | TrainScore:0.6017341406440795 | DevScore:0.7151994259827857\n",
      "129m 6s (- 129m 6s) (5 50.0) Iter: 1600/1713 | Loss:2.897 | TrainScore:0.6453657657492349 | DevScore:0.9587829282704871\n",
      "130m 58s (- 87m 18s) (6 60.0) Iter: 0/1713 | Loss:0.817 | TrainScore:0.9120966569518829 | DevScore:0.7723914379666267\n",
      "137m 2s (- 91m 21s) (6 60.0) Iter: 400/1713 | Loss:2.857 | TrainScore:0.5477729937141224 | DevScore:0.24552804155875\n",
      "143m 6s (- 95m 24s) (6 60.0) Iter: 800/1713 | Loss:2.869 | TrainScore:0.4263836579800939 | DevScore:0.2982386116800426\n",
      "149m 12s (- 99m 28s) (6 60.0) Iter: 1200/1713 | Loss:2.875 | TrainScore:0.44272982159833835 | DevScore:0.4719741438304421\n",
      "155m 16s (- 103m 31s) (6 60.0) Iter: 1600/1713 | Loss:2.879 | TrainScore:0.5599121256215924 | DevScore:0.597526154481686\n",
      "157m 8s (- 67m 20s) (7 70.0) Iter: 0/1713 | Loss:0.8109 | TrainScore:0.4503171070691073 | DevScore:0.7008448346717561\n",
      "163m 12s (- 69m 56s) (7 70.0) Iter: 400/1713 | Loss:2.843 | TrainScore:0.038878831444602406 | DevScore:0.0725633923411494\n",
      "169m 17s (- 72m 33s) (7 70.0) Iter: 800/1713 | Loss:2.851 | TrainScore:0.1555754166528492 | DevScore:0.22153373602868345\n",
      "175m 22s (- 75m 9s) (7 70.0) Iter: 1200/1713 | Loss:2.854 | TrainScore:0.12314243082986899 | DevScore:0.034549864461042676\n",
      "181m 28s (- 77m 46s) (7 70.0) Iter: 1600/1713 | Loss:2.862 | TrainScore:0.12158828673485515 | DevScore:0.4666607163320021\n",
      "183m 20s (- 45m 50s) (8 80.0) Iter: 0/1713 | Loss:0.8076 | TrainScore:0.41129133006437996 | DevScore:0.01740545583071127\n",
      "189m 24s (- 47m 21s) (8 80.0) Iter: 400/1713 | Loss:2.825 | TrainScore:0.01616844876759632 | DevScore:0.04092499108013422\n",
      "195m 28s (- 48m 52s) (8 80.0) Iter: 800/1713 | Loss:2.839 | TrainScore:0.17714896601933844 | DevScore:0.12149484033330571\n",
      "201m 33s (- 50m 23s) (8 80.0) Iter: 1200/1713 | Loss:2.844 | TrainScore:0.08349865818495165 | DevScore:0.21828949379249202\n",
      "207m 39s (- 51m 54s) (8 80.0) Iter: 1600/1713 | Loss:2.848 | TrainScore:0.009740611323428175 | DevScore:0.010283989409644511\n",
      "209m 31s (- 23m 16s) (9 90.0) Iter: 0/1713 | Loss:0.8022 | TrainScore:0.029196789516247938 | DevScore:0.012451155371047752\n",
      "215m 36s (- 23m 57s) (9 90.0) Iter: 400/1713 | Loss:2.814 | TrainScore:0.03378809628132957 | DevScore:0.010272967401433394\n",
      "221m 40s (- 24m 37s) (9 90.0) Iter: 800/1713 | Loss:2.825 | TrainScore:0.00957724358302013 | DevScore:0.006131045823193079\n",
      "227m 45s (- 25m 18s) (9 90.0) Iter: 1200/1713 | Loss:2.836 | TrainScore:0.1814493121761148 | DevScore:0.013607689328058767\n",
      "233m 49s (- 25m 58s) (9 90.0) Iter: 1600/1713 | Loss:2.837 | TrainScore:0.005887063337469081 | DevScore:0.014679065563555572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training Complete!'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(encoder, decoder, encoder_optim, decoder_optim)\n",
    "model.fit(train_loader, dev_loader, teacher_forcing_ratio=1.0, n_epoch=10, print_every=400, n_grams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1532582123400/work/aten/src/THC/generic/THCTensorCopy.cpp:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ae386fe486dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DS-1011-NLP-Final-Project/isaac/langUtils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids_tensor)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mfixed_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_tensor\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mNUM_SPECIAL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_tensor\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mNUM_SPECIAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mlearned_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_tensor\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mNUM_SPECIAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1532582123400/work/aten/src/THC/generic/THCTensorCopy.cpp:20"
     ]
    }
   ],
   "source": [
    "encoder.embedding(inp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1532582123400/work/aten/src/THC/generic/THCTensorCopy.cpp:70",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    400\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# characters to replace unicode characters with.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0msuffix\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m', dtype='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mcopy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1532582123400/work/aten/src/THC/generic/THCTensorCopy.cpp:70"
     ]
    }
   ],
   "source": [
    "encoder.embedding(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
