{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import pickle as pkl\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_tag = '<sos>'\n",
    "eos_tag = '<eos>'\n",
    "\n",
    "#Windows hack\n",
    "if \"\\\\\" in os.getcwd():\n",
    "    data_dir = '\\\\'.join(os.getcwd().split(\"\\\\\")[:-1]) + '\\\\data\\\\'\n",
    "    emb_dir = '\\\\'.join(os.getcwd().split(\"\\\\\")[:-1]) + '\\\\embeddings\\\\'\n",
    "else:\n",
    "    data_dir = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/data/'\n",
    "    emb_dir = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname, count):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for i, line in enumerate(fin):\n",
    "        if i == count:\n",
    "            break\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data\n",
    "\n",
    "def save_zipped_pickle(obj, filename, protocol=-1):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pkl.dump(obj, f, protocol)\n",
    "\n",
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pkl.load(f)\n",
    "        return loaded_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Vietnamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"\\\\\" in os.getcwd():\n",
    "    vi_emb_path = '\\\\'.join(os.getcwd().split(\"\\\\\")[:-2]) + \"\\\\cc.vi.300.vec\"\n",
    "else:\n",
    "    vi_emb_path = '/'.join(os.getcwd().split(\"/\")[:-2]) + \"/cc.vi.300.vec\"\n",
    "    \n",
    "vi_emb = load_vectors(vi_emb_path, 100000)\n",
    "vi_emb_mat = np.array([v for k,v in vi_emb.items()])\n",
    "\n",
    "vi_emb_mat = np.concatenate(\n",
    "        [np.zeros(vi_emb_mat.shape[1]).reshape(1,-1),  \n",
    "        np.random.uniform(vi_emb_mat.mean()-vi_emb_mat.std(),vi_emb_mat.mean()+vi_emb_mat.std(), vi_emb_mat.shape[1]).reshape(1,-1),\n",
    "        np.random.uniform(vi_emb_mat.mean()-vi_emb_mat.std(),vi_emb_mat.mean()+vi_emb_mat.std(), vi_emb_mat.shape[1]).reshape(1,-1),\n",
    "        np.random.uniform(vi_emb_mat.mean()-vi_emb_mat.std(),vi_emb_mat.mean()+vi_emb_mat.std(), vi_emb_mat.shape[1]).reshape(1,-1),\n",
    "        vi_emb_mat])\n",
    "\n",
    "#save_zipped_pickle(vi_emb, emb_dir + \"vi_embeddings_100K.p\")\n",
    "save_zipped_pickle(vi_emb_mat, emb_dir + \"vi_embeddings_matrix_100K.p\")\n",
    "\n",
    "vi_id2word = {i+4:k for i, k in enumerate(vi_emb.keys())}\n",
    "vi_id2word[0] = \"<pad>\"\n",
    "vi_id2word[1] = \"<unk>\"\n",
    "vi_id2word[2] = \"<sos>\"\n",
    "vi_id2word[3] = \"<eos>\"\n",
    "\n",
    "vi_word2id = {v:k for k,v in vi_id2word.items()}\n",
    "\n",
    "save_zipped_pickle(vi_id2word, emb_dir + \"id2word_vi_dic.p\")\n",
    "\n",
    "save_zipped_pickle(vi_word2id, emb_dir + \"word2id_vi_dic.p\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(emb_dir + \"vi_embeddings_50K.p\", 'rb') as f:\n",
    "    vi_emb = pkl.load(f)\n",
    "\n",
    "with open(emb_dir + 'id2word_vi_dic.p', 'rb') as f:\n",
    "    vi_id2word = pkl.load(f)\n",
    "with open(emb_dir + 'word2id_vi_dic.p', 'rb') as f:\n",
    "    vi_word2id = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeVietnamese(vi_path):\n",
    "    with open(vi_path, encoding='utf-8') as vi:\n",
    "        lines = [sos_tag + \" \" + x.strip().lower().replace(\"_\", \" \").replace(\"\\\"\", \"\") + \" \" + eos_tag for x in vi]\n",
    "        \n",
    "    tokens = [[t for t in x.split(\" \") if t != \"\"] for x in lines]\n",
    "    \n",
    "    tokens_num = [[\"x\" for k in range(len(sentence))] for sentence in tokens]\n",
    "\n",
    "    for i, sentence in enumerate(tokens):\n",
    "        for j, token in enumerate(sentence):\n",
    "            if token in vi_word2id:\n",
    "                tokens_num[i][j] = vi_word2id[token]\n",
    "            else:\n",
    "                tokens_num[i][j] = vi_word2id['<unk>']\n",
    "\n",
    "    return tokens, tokens_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"train\", \"test\", \"dev\"]:\n",
    "    tokens, tokens_num = tokenizeVietnamese(\"{}iwslt-vi-en/{}.tok.vi\".format(data_dir, i))\n",
    "    \n",
    "    save_zipped_pickle(tokens, \"{}vi-en-tokens/{}_vi_tok.p\".format(data_dir, i))\n",
    "    save_zipped_pickle(tokens_num, \"{}vi-en-tokens/{}_vi_tok_num.p\".format(data_dir, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"\\\\\" in os.getcwd():\n",
    "    en_emb_path = '\\\\'.join(os.getcwd().split(\"\\\\\")[:-2]) + \"\\\\wiki-news-300d-1M.vec\"\n",
    "else:\n",
    "    en_emb_path = '/'.join(os.getcwd().split(\"/\")[:-2]) + \"/wiki-news-300d-1M.vec\"\n",
    "    \n",
    "en_emb = load_vectors(en_emb_path, 100000)\n",
    "en_emb_mat = np.array([v for k,v in en_emb.items()])\n",
    "\n",
    "en_emb_mat = np.concatenate(\n",
    "        [np.zeros(en_emb_mat.shape[1]).reshape(1,-1),  \n",
    "        np.random.uniform(en_emb_mat.mean()-en_emb_mat.std(),en_emb_mat.mean()+en_emb_mat.std(), en_emb_mat.shape[1]).reshape(1,-1),\n",
    "        np.random.uniform(en_emb_mat.mean()-en_emb_mat.std(),en_emb_mat.mean()+en_emb_mat.std(), en_emb_mat.shape[1]).reshape(1,-1),\n",
    "        np.random.uniform(en_emb_mat.mean()-en_emb_mat.std(),en_emb_mat.mean()+en_emb_mat.std(), en_emb_mat.shape[1]).reshape(1,-1),\n",
    "        en_emb_mat])\n",
    "\n",
    "#save_zipped_pickle(en_emb, emb_dir + \"en_embeddings_100K.p\")\n",
    "save_zipped_pickle(en_emb_mat, emb_dir + \"en_embeddings_matrix_100K.p\")\n",
    "\n",
    "en_id2word = {i+4:k for i, k in enumerate(en_emb.keys())}\n",
    "en_id2word[0] = \"<pad>\"\n",
    "en_id2word[1] = \"<unk>\"\n",
    "en_id2word[2] = \"<sos>\"\n",
    "en_id2word[3] = \"<eos>\"\n",
    "\n",
    "en_word2id = {v:k for k,v in en_id2word.items()}\n",
    " \n",
    "save_zipped_pickle(en_id2word, emb_dir + \"id2word_en_dic.p\")\n",
    "save_zipped_pickle(en_word2id, emb_dir + \"word2id_en_dic.p\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(emb_dir + \"en_embeddings_100K.p\", 'rb') as f:\n",
    "    en_emb = pkl.load(f)\n",
    "\n",
    "with open(emb_dir + 'id2word_en_dic.p', 'rb') as f:\n",
    "    en_id2word = pkl.load(f)\n",
    "with open(emb_dir + 'word2id_en_dic.p', 'rb') as f:\n",
    "    en_word2id = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def englishClean(x):\n",
    "    x = ''.join(c for c in unicodedata.normalize('NFD', x) if unicodedata.category(c) != 'Mn')\n",
    "    x = re.sub(r\"&apos;\", r\"\", x)\n",
    "    x = re.sub(r\"&quot;\", r\"\", x)\n",
    "    x = re.sub(r\" &quot;\", r\"\", x)\n",
    "    x = re.sub(r\" &quot; \", r\"\", x)\n",
    "    x = re.sub(r\"([.!?])\", r\" \\1 \", x)\n",
    "    return x \n",
    "\n",
    "\n",
    "def tokenizeEnglish(en_path):\n",
    "    with open(en_path, encoding='utf-8') as en:\n",
    "        lines = [sos_tag + \" \" + englishClean(x.lower().strip()) + \" \" + eos_tag for x in en]\n",
    "        \n",
    "    tokens = [[t for t in x.split(\" \") if t != \"\"] for x in lines]\n",
    "    \n",
    "    tokens_num = [[\"x\" for k in range(len(sentence))] for sentence in tokens]\n",
    "\n",
    "    for i, sentence in enumerate(tokens):\n",
    "        for j, token in enumerate(sentence):\n",
    "            if token in en_word2id:\n",
    "                tokens_num[i][j] = en_word2id[token]\n",
    "            else:\n",
    "                tokens_num[i][j] = en_word2id['<unk>']\n",
    "\n",
    "    return tokens, tokens_num\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"train\", \"test\", \"dev\"]:\n",
    "    tokens, tokens_num = tokenizeEnglish(\"{}iwslt-vi-en/{}.tok.en\".format(data_dir, i))\n",
    "    \n",
    "    save_zipped_pickle(tokens, \"{}vi-en-tokens/{}_en_tok.p\".format(data_dir, i))\n",
    "    save_zipped_pickle(tokens_num, \"{}vi-en-tokens/{}_en_tok_num.p\".format(data_dir, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"train\", \"test\", \"dev\"]:\n",
    "    tokens, tokens_num = tokenizeEnglish(\"{}iwslt-zh-en/{}.tok.en\".format(data_dir, i))\n",
    "    \n",
    "    save_zipped_pickle(tokens, \"{}zh-en-tokens/{}_en_tok.p\".format(data_dir, i))\n",
    "    save_zipped_pickle(tokens_num, \"{}zh-en-tokens/{}_en_tok_num.p\".format(data_dir, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, tokens_num = tokenizeEnglish(\"{}iwslt-vi-en/{}.tok.en\".format(data_dir, 'train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"\\\\\" in os.getcwd():\n",
    "    zh_emb_path = '\\\\'.join(os.getcwd().split(\"\\\\\")[:-2]) + \"\\\\cc.zh.300.vec\"\n",
    "else:\n",
    "    zh_emb_path = '/'.join(os.getcwd().split(\"/\")[:-2]) + \"/cc.zh.300.vec\"\n",
    "    \n",
    "zh_emb = load_vectors(zh_emb_path, 100000)\n",
    "zh_emb_mat = np.array([v for k,v in zh_emb.items()])\n",
    "\n",
    "zh_emb_mat = np.concatenate(\n",
    "        [np.zeros(zh_emb_mat.shape[1]).reshape(1,-1),  \n",
    "        np.random.uniform(zh_emb_mat.mean()-zh_emb_mat.std(),zh_emb_mat.mean()+zh_emb_mat.std(), zh_emb_mat.shape[1]).reshape(1,-1),\n",
    "        np.random.uniform(zh_emb_mat.mean()-zh_emb_mat.std(),zh_emb_mat.mean()+zh_emb_mat.std(), zh_emb_mat.shape[1]).reshape(1,-1),\n",
    "        np.random.uniform(zh_emb_mat.mean()-zh_emb_mat.std(),zh_emb_mat.mean()+zh_emb_mat.std(), zh_emb_mat.shape[1]).reshape(1,-1),\n",
    "        zh_emb_mat])\n",
    "\n",
    "#save_zipped_pickle(zh_emb, emb_dir + \"zh_embeddings_100K.p\")\n",
    "save_zipped_pickle(zh_emb_mat, emb_dir + \"zh_embeddings_matrix_100K.p\")\n",
    "\n",
    "zh_id2word = {i+4:k for i, k in enumerate(zh_emb.keys())}\n",
    "zh_id2word[0] = \"<pad>\"\n",
    "zh_id2word[1] = \"<unk>\"\n",
    "vi_id2word[2] = \"<sos>\"\n",
    "vi_id2word[3] = \"<eos>\"\n",
    "\n",
    "zh_word2id = {v:k for k,v in zh_id2word.items()}\n",
    "    \n",
    "save_zipped_pickle(zh_id2word, emb_dir + \"id2word_zh_dic.p\")\n",
    "save_zipped_pickle(zh_word2id, emb_dir + \"word2id_zh_dic.p\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(emb_dir + \"zh_embeddings_100K.p\", 'rb') as f:\n",
    "    zh_emb = pkl.load(f)\n",
    "\n",
    "with open(emb_dir + 'id2word_vi_dic.p', 'rb') as f:\n",
    "    zh_id2word = pkl.load(f)\n",
    "with open(emb_dir + 'word2id_vi_dic.p', 'rb') as f:\n",
    "    zh_word2id = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeChinese(zh_path):\n",
    "    with open(zh_path, encoding='utf-8') as zh:\n",
    "        lines = [sos_tag + \" \"  + x.strip().lower().replace(\"_\", \" \").replace(\"\\\"\", \"\") + \" \" + eos_tag for x in zh]\n",
    "        \n",
    "    tokens = [[t for t in x.split(\" \") if t != \"\"] for x in lines]\n",
    "    \n",
    "    tokens_num = [[\"x\" for k in range(len(sentence))] for sentence in tokens]\n",
    "\n",
    "    for i, sentence in enumerate(tokens):\n",
    "        for j, token in enumerate(sentence):\n",
    "            if token in zh_word2id:\n",
    "                tokens_num[i][j] = zh_word2id[token]\n",
    "            else:\n",
    "                tokens_num[i][j] = zh_word2id['<unk>']\n",
    "\n",
    "    return tokens, tokens_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"train\", \"test\", \"dev\"]:\n",
    "    tokens, tokens_num = tokenizeChinese(\"{}iwslt-zh-en/{}.tok.zh\".format(data_dir, i))\n",
    "    \n",
    "    save_zipped_pickle(tokens, \"{}zh-en-tokens/{}_zh_tok.p\".format(data_dir, i))\n",
    "    save_zipped_pickle(tokens_num, \"{}zh-en-tokens/{}_zh_tok_num.p\".format(data_dir, i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
