{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/data/'\n",
    "emb_dir = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/embeddings/'\n",
    "\n",
    "SPECIAL_SYMBOLS_ID = \\\n",
    "    PAD_ID, OOV_ID, EOS_ID = \\\n",
    "    0, 1, 5\n",
    "SPECIAL_SYMBOLS_STR = \\\n",
    "    PAD_STR, OOV_STR, EOS_STR = \\\n",
    "    \"<pad>\", \"<unk>\", \"</s>\"\n",
    "NUM_SPECIAL = 3\n",
    "\n",
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pkl.load(f)\n",
    "        return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings\n",
    "vi_emb = load_zipped_pickle(emb_dir + \"vi_embeddings_100K.p\")\n",
    "vi_emb_np = np.array([v for k,v in vi_emb.items()])\n",
    "vi_id2word = load_zipped_pickle(emb_dir + \"id2word_vi_dic.p\")\n",
    "vi_word2id = load_zipped_pickle(emb_dir + \"word2id_vi_dic.p\")\n",
    "\n",
    "#Vietnamese Tokens\n",
    "vi_train_tokens = load_zipped_pickle(data_dir + \"vi-en-tokens/train_vi_tok.p\")\n",
    "vi_train_tokens_num = load_zipped_pickle(data_dir + \"vi-en-tokens/train_vi_tok_num.p\")\n",
    "\n",
    "vi_dev_tokens = load_zipped_pickle(data_dir + \"vi-en-tokens/dev_vi_tok.p\")\n",
    "vi_dev_tokens_num = load_zipped_pickle(data_dir + \"vi-en-tokens/dev_vi_tok_num.p\")\n",
    "\n",
    "vi_test_tokens = load_zipped_pickle(data_dir + \"vi-en-tokens/test_vi_tok.p\")\n",
    "vi_test_tokens_num = load_zipped_pickle(data_dir + \"vi-en-tokens/test_vi_tok_num.p\")\n",
    "\n",
    "#English tokens\n",
    "en_train_tokens = load_zipped_pickle(data_dir + \"vi-en-tokens/train_en_tok.p\")\n",
    "en_train_tokens_num = load_zipped_pickle(data_dir + \"vi-en-tokens/train_en_tok_num.p\")\n",
    "\n",
    "en_dev_tokens = load_zipped_pickle(data_dir + \"vi-en-tokens/dev_en_tok.p\")\n",
    "en_dev_tokens_num = load_zipped_pickle(data_dir + \"vi-en-tokens/dev_en_tok_num.p\")\n",
    "\n",
    "en_test_tokens = load_zipped_pickle(data_dir + \"vi-en-tokens/test_en_tok.p\")\n",
    "en_test_tokens_num = load_zipped_pickle(data_dir + \"vi-en-tokens/test_en_tok_num.p\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEmbeddings(nn.Module):\n",
    "    def __init__(self, fixed_embeddings, learned_embeddings):\n",
    "        super(HybridEmbeddings, self).__init__()\n",
    "        self.fixed_embeddings = fixed_embeddings\n",
    "        self.num_fixed = self.fixed_embeddings.num_embeddings - 1\n",
    "\n",
    "        self.learned_embeddings = learned_embeddings\n",
    "        self.num_learned = self.learned_embeddings.num_embeddings - 1\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.fixed_embeddings.embedding_dim\n",
    "\n",
    "    def forward(self, ids_tensor):\n",
    "        fixed_ids = transform_ids(\n",
    "            ids_tensor,\n",
    "            start=NUM_SPECIAL,\n",
    "            end=NUM_SPECIAL + self.num_fixed,\n",
    "        )\n",
    "        learned_ids = transform_ids(\n",
    "            ids_tensor,\n",
    "            start=NUM_SPECIAL + self.num_fixed,\n",
    "            end=NUM_SPECIAL + self.num_fixed + self.num_learned,\n",
    "        )\n",
    "        embeddings = (\n",
    "            self.fixed_embeddings(fixed_ids)\n",
    "            + self.learned_embeddings(learned_ids)\n",
    "        )\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_emb_fixed = vi_emb_np[[i for i in range(2, vi_emb_np.shape[0]) if i != 5],:]\n",
    "\n",
    "fixed_embeddings = nn.Embedding(\n",
    "        vi_emb_fixed.shape[0],\n",
    "        vi_emb_fixed.shape[1],\n",
    "        padding_idx=0,\n",
    "    )\n",
    "fixed_embeddings.weight.data.copy_(\n",
    "        torch.from_numpy(vi_emb_fixed))\n",
    "fixed_embeddings.weight.requires_grad = False\n",
    "learned_embeddings = nn.Embedding(\n",
    "        NUM_SPECIAL+1,\n",
    "        vi_emb_fixed.shape[1],\n",
    "        padding_idx=0,\n",
    "    )\n",
    "embeddings = HybridEmbeddings(\n",
    "        fixed_embeddings=fixed_embeddings,\n",
    "        learned_embeddings=learned_embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ids_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-6d5337645d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m transform_ids(\n\u001b[0;32m----> 2\u001b[0;31m             \u001b[0mids_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_SPECIAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_SPECIAL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_fixed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ids_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "transform_ids(\n",
    "            ids_tensor,\n",
    "            start=NUM_SPECIAL,\n",
    "            end=NUM_SPECIAL + embeddings.num_fixed,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ids(ids_tensor, start, end):\n",
    "        return (ids_tensor - start + 1) * (\n",
    "            (ids_tensor >= start) & (ids_tensor < end)\n",
    "        ).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_word2id[ '</s>']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
