{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, EncoderRNN, LocalAttnDecoder\n",
    "from trainUtils import train, fit, bleuEval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_dir, em_dir, fig_dir = directories()\n",
    "\n",
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3\n",
    "NUM_SPECIAL = len(SPECIAL_SYMBOLS_ID)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "grid = 10.0**np.arange(-4,1)\n",
    "\n",
    "\n",
    "for lang in [\"vi\", \"zh\"]:\n",
    "    \n",
    "    print(\"Starting Language: {}\".format(lang))\n",
    "    \n",
    "    inp_lang, out_lang = loadLangPairs(lang)\n",
    "\n",
    "    for j, i in enumerate(grid):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"*******************************************************\")\n",
    "        print(\"*******************************************************\")\n",
    "        print(\"*******************************************************\")\n",
    "        print(\"Running Learning Rate: {} | {:.3}% Complete\".format(i, j/len(grid)))\n",
    "        print(\"*******************************************************\")\n",
    "        print(\"*******************************************************\")\n",
    "        print(\"*******************************************************\")\n",
    "        print(\"\")\n",
    "        #LOAD LANGS\n",
    "        train_dataset = langDataset([(inp_lang.train_num[i], out_lang.train_num[i]) for i in range(len(inp_lang.train_num)) if (len(inp_lang.train[i]) < inp_lang.max_length) & \n",
    "                                                                                                                         (len(out_lang.train[i]) < out_lang.max_length)])\n",
    "        #overfit_dataset = langDataset([(inp_lang.train_num[i], out_lang.train_num[i]) for i in range(int(len(train_dataset) * .25))])\n",
    "        overfit_dataset = langDataset([(inp_lang.train_num[i], out_lang.train_num[i]) for i in range(BATCH_SIZE * 5)])\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=overfit_dataset,\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=langCollateFn,\n",
    "                                                   shuffle=False)\n",
    "        dev_dataset = langDataset([(inp_lang.dev_num[i], out_lang.dev_num[i]) for i in range(len(inp_lang.dev_num)) if (len(inp_lang.dev[i]) < inp_lang.max_length) & \n",
    "                                                                                                           (len(out_lang.dev[i]) < out_lang.max_length)])\n",
    "        dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=langCollateFn,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        #SET PARAMS\n",
    "        encoder_params = {'hidden_size':250, 'n_layers':1}\n",
    "        decoder_params = {'hidden_size':encoder_params['hidden_size'], 'n_layers':1, 'output_size':out_lang.n_words, 'dropout':0.1}\n",
    "\n",
    "        encoder = EncoderRNN(encoder_params, inp_lang.emb, inp_lang.learn_ids).to(device)\n",
    "        encoder_optim = optim.Adam(encoder.parameters(), lr=i)\n",
    "\n",
    "        decoder = LocalAttnDecoder(decoder_params, out_lang.emb, out_lang.learn_ids).to(device)\n",
    "        decoder_optim = optim.Adam(decoder.parameters(), lr=i)\n",
    "\n",
    "        #SET CRITERION\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID).to(device)\n",
    "\n",
    "        #FIT AND TRAIN\n",
    "        losses, train_scores, dev_scores = fit(train_loader, dev_loader, encoder, decoder, encoder_optim, decoder_optim, criterion, 5, 2, lang)\n",
    "\n",
    "        #PLOT LOSSES\n",
    "        plt.figure()\n",
    "\n",
    "        pp = sns.lineplot(x = np.arange(len(losses)), y = losses, legend='brief')\n",
    "        pp.set_title('Loss Over Time | Hidden Size: {}'.format(i))\n",
    "        pp.set_ylabel(\"Loss\")\n",
    "        pp.set_xlabel(\"Time\")\n",
    "        \n",
    "        if \"\\\\\" in os.getcwd():\n",
    "            pp.get_figure().savefig(fig_dir+\"local_att\\\\lr\\\\{}_lr_{}_loss.png\".format(lang, str(i)), bbox_inches='tight')\n",
    "        else:\n",
    "            pp.get_figure().savefig(fig_dir+\"local_att/lr/{}_lr_{}_loss.png\".format(lang, str(i)), bbox_inches='tight')\n",
    "\n",
    "        #PLOT SCORES\n",
    "        df = pd.concat([pd.DataFrame({'X':np.arange(len(train_scores)), 'Y':train_scores, 'Acc':'Train'}), \n",
    "                    pd.DataFrame({'X':np.arange(len(dev_scores)), 'Y':dev_scores, 'Acc':'Dev'})], axis=0)\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "        pp = sns.lineplot(data=df, x = 'X', y = 'Y', hue='Acc', style=\"Acc\", legend= \"brief\")\n",
    "        pp.set_title(\"Scores\")\n",
    "        pp.set_ylabel(\"Accuracy\")\n",
    "        pp.set_xlabel(\"Epoch\")\n",
    "        \n",
    "        if \"\\\\\" in os.getcwd():\n",
    "            pp.get_figure().savefig(fig_dir+\"local_att\\\\lr\\\\{}_lr_{}_scores.png\".format(lang, str(i)), bbox_inches='tight')\n",
    "        else:\n",
    "            pp.get_figure().savefig(fig_dir+\"local_att/lr/{}_lr_{}_scores.png\".format(lang, str(i)), bbox_inches='tight')\n",
    "\n",
    "        torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
