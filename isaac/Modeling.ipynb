{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, tensorToList\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir, em_dir = directories()\n",
    "\n",
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3\n",
    "NUM_SPECIAL = len(SPECIAL_SYMBOLS_ID)\n",
    "\n",
    "vi, en = loadLangPairs(\"vi\")\n",
    "\n",
    "class SortedList(list):\n",
    "    def insort(self, x):\n",
    "        bisect.insort(self, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(len(vi.train_num)) if (len(vi.train[i]) < vi.max_length) & (len(en.train[i]) < en.max_length)])\n",
    "overfit_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(64)])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=overfit_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=False)\n",
    "dev_dataset = langDataset([(vi.dev_num[i], en.dev_num[i]) for i in range(len(vi.dev_num)) if (len(vi.dev[i]) < vi.max_length) & (len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Encoder RNN\n",
    "        Input\n",
    "            params - Dictionary of paramter\n",
    "            raw_emb - (100,000, 300) raw embeddings\n",
    "            learn_ids - list of ids to do embedding learning\n",
    "            \n",
    "            inp - (Max Length, Batch Size), original inputs\n",
    "            inp_lens - (Batch Size), true length of inputs\n",
    "        Output\n",
    "            output - (Max Length, Batch Size, Hidden Size), GRU output\n",
    "            hidden - (2, Batch Size, Hidden Size), Final hidden state of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "    \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        \n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.n_layers, bidirectional=True)\n",
    "        \n",
    "    def forward(self, inp, inp_lens):\n",
    "        #Embed input\n",
    "        embedded = self.embedding(inp)\n",
    "        #Pack padded\n",
    "        packed = pack_padded_sequence(embedded, inp_lens).to(device)\n",
    "        \n",
    "        #GRU\n",
    "        output, self.hidden = self.gru(packed)\n",
    "        #Pad packed\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        #Concat bidirectional layers\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        return output, self.hidden\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Decoder RNN\n",
    "        Input\n",
    "            params - Dictionary of paramter\n",
    "            raw_emb - (100,000, 300) raw embeddings\n",
    "            learn_ids - list of ids to do embedding learning\n",
    "            \n",
    "            inp - (1, Batch Size), SOS Token each in batch size\n",
    "            hidden - (2, 1, Hidden Size), Prev hidden size\n",
    "        Output\n",
    "            output - (32, Vocab Size), Probabilities\n",
    "            hidden - (2, 1 Size, Hidden Size), Final hidden state of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        self.output_size = params['output_size']\n",
    "\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.n_layers, bidirectional=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_output=None):\n",
    "        #Embed\n",
    "        embedded = self.embedding(inp)\n",
    "        #Dropout\n",
    "        output = F.relu(embedded)\n",
    "        \n",
    "        #Gru\n",
    "        output, self.hidden = self.gru(output, hidden)\n",
    "        #Concat directions\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        #GRU output to probabilities\n",
    "        output = torch.exp(self.softmax(self.out(output))).squeeze(0)\n",
    "        return output, hidden\n",
    "\n",
    "def maskedLoss(inp, target, mask):\n",
    "    \"\"\"Masked Loss\"\"\"\n",
    "    #Total # real words\n",
    "    nTotal = mask.sum()\n",
    "    #Cross entropy\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1).to(device)))\n",
    "    #Select loss of real words\n",
    "    loss = crossEntropy.masked_select(mask.to(device)).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = params['hidden_size']\n",
    "           \n",
    "    def forward(self, hidden, encoder_output):\n",
    "        #h_t.T.dot(h_s)\n",
    "        attn_energies = torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "        return F.softmax(attn_energies.t(), dim=1).unsqueeze(1)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.output_size = params['output_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        self.dropout = params['dropout']\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.embed_dropout = nn.Dropout(self.dropout)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, self.hidden_size, self.n_layers, bidirectional=True)\n",
    "        self.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "        self.attn = Attn({'hidden_size':self.hidden_size})\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_output):\n",
    "        #Embedding and dropout\n",
    "        embedded = self.embedding(inp)\n",
    "        output = self.embed_dropout(embedded)\n",
    "        \n",
    "        #GRU\n",
    "        rnn_out, hidden = self.gru(output, hidden)\n",
    "        rnn_out = rnn_out[:, :, :self.hidden_size] + rnn_out[:, : ,self.hidden_size:]\n",
    "\n",
    "        #Attn weights * enc_output\n",
    "        attn_weights = self.attn(rnn_out, encoder_output)\n",
    "        context = attn_weights.bmm(encoder_output.transpose(0, 1))\n",
    "        \n",
    "        #Concat context to GRU output     \n",
    "        rnn_out = rnn_out.squeeze(0)\n",
    "\n",
    "        context = context.squeeze(1)\n",
    "        concat_inp = torch.cat((rnn_out, context), 1)\n",
    "        concat_out = torch.tanh(self.concat(concat_inp))\n",
    "        \n",
    "        #Prediction\n",
    "        output = self.out(concat_out)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, encoder, decoder, encoder_optim, decoder_optim):\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.encoder_optim = encoder_optim\n",
    "        self.decoder_optim = decoder_optim\n",
    "        \n",
    "    def fit(self, train_data, dev_data, n_epoch, print_every, n_grams):\n",
    "        start = time.time()\n",
    "\n",
    "        self.n_epoch = n_epoch\n",
    "        \n",
    "        \n",
    "        print(\"Initializing...\")\n",
    "        start_epoch = 1\n",
    "        print_loss_total, plot_loss_total = 0 , 0\n",
    "        plot_losses, plot_train_scores, plot_dev_scores = [], [], []\n",
    "        \n",
    "        for epoch in range(start_epoch, n_epoch):\n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "                loss = self.trainEpoch(inp, inp_lens, output, out_mask, out_max)\n",
    "                \n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "                    \n",
    "                if i % print_every == 0:\n",
    "                    train_score = self.bleuScore(train_loader, n_grams)\n",
    "                    dev_score = self.bleuScore(dev_loader, n_grams)\n",
    "                    plot_train_scores.append(train_score)\n",
    "                    plot_dev_scores.append(dev_score)\n",
    "                    plot_loss_avg = plot_loss_total / print_every\n",
    "                    plot_losses.append(plot_loss_avg)\n",
    "                    plot_loss_total = 0       \n",
    "\n",
    "                    print_loss_avg = print_loss_total / print_every\n",
    "                    print_loss_total = 0\n",
    "                    print(\"Epoch:{} | Time Elapsed:{} | Percent Complete:{:.1} | Loss:{:.4} | TrainScore:{:.4} | DevScore:{:.4}\".format(epoch,\n",
    "                                                                                                                                  timeSince(start, epoch/n_epoch), \n",
    "                                                                                                                                  epoch/n_epoch*100, \n",
    "                                                                                                                                  print_loss_avg, \n",
    "                                                                                                                                  train_score,\n",
    "                                                                                                                                  dev_score))                \n",
    "\n",
    "        self.plot_losses = plot_losses\n",
    "        self.plot_train_scores = plot_train_scores\n",
    "        self.plot_dev_scores = plot_dev_scores\n",
    "        return \"Training Complete!\"            \n",
    "            \n",
    "    def trainEpoch(self, inp, inp_lens, output, out_mask, out_max):\n",
    "        #Zero gradients\n",
    "        self.encoder_optim.zero_grad()\n",
    "        self.decoder_optim.zero_grad()\n",
    "\n",
    "        #Loss vars\n",
    "        loss, print_losses, n_totals = 0, [], 0\n",
    "        \n",
    "        #Encoder Forward\n",
    "        encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "        \n",
    "        #Init decoder_input\n",
    "        decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]]).to(device)\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]\n",
    "\n",
    "        #Teacher Forcing\n",
    "        for t in range(out_max):\n",
    "            #Decoder Forward\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            #True Output\n",
    "            decoder_input = output[t].view(1, -1)\n",
    "                \n",
    "            mask_loss, nTotal = maskedLoss(decoder_output, output[t], out_mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "                \n",
    "        loss.backward()\n",
    "        \n",
    "        self.encoder_optim.step()\n",
    "        self.decoder_optim.step()\n",
    "\n",
    "        return sum(print_losses) / n_totals\n",
    "    \n",
    "    def bleuScore(self, data_loader, n_grams):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            true_outputs = []\n",
    "            decoder_outputs = []\n",
    "\n",
    "            for i, (inp, inp_lens, out, out_mask, out_max) in enumerate(data_loader):\n",
    "                if i * BATCH_SIZE > 10000:\n",
    "                    break\n",
    "\n",
    "                #Save true masked outputs\n",
    "                true_outputs += [out[:,i].masked_select(out_mask[:,i]).tolist() for i in range(out.size(1))]\n",
    "                \n",
    "                #Encoder forward\n",
    "                encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "                \n",
    "                #Decoder inputs\n",
    "                decoder_input = torch.LongTensor([[SOS_ID] * inp.size(1)]).to(device)\n",
    "                decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]\n",
    "\n",
    "                #Decoder Results\n",
    "                batch_output = decoder_input.clone()\n",
    "                \n",
    "                for t in range(out_max):\n",
    "                    #Decoder forward\n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                    _, topi = decoder_output.topk(1)\n",
    "                    decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]]).to(device)\n",
    "                    \n",
    "                    #Save result\n",
    "                    batch_output = torch.cat((batch_output, decoder_input), dim=0)\n",
    "\n",
    "                    \n",
    "                #Mask results\n",
    "                batch_pos = []\n",
    "\n",
    "                for i in range(batch_output.size(1)):\n",
    "                    try:\n",
    "                        batch_pos.append((batch_output[:,i]==EOS_ID).nonzero().item())\n",
    "                    except:\n",
    "                        try:\n",
    "                            batch_pos.append((batch_output[:,i]==PAD_ID).nonzero().item())\n",
    "                        except:\n",
    "                            batch_pos.append(batch_output.size(0)+1)\n",
    "                \n",
    "                #To list\n",
    "                decoder_outputs += [batch_output[:,batch_pos[i]].tolist() for i in range(len(batch_pos))]\n",
    "\n",
    "        true_outputs = [[str(i) for i in seq] for seq in true_outputs] \n",
    "        decoder_outputs = [[str(i) for i in seq] for seq in decoder_outputs] \n",
    "        return corpus_bleu(decoder_outputs, true_outputs, n_grams)\n",
    "\n",
    "    def showLoss(self):\n",
    "        plt.figure()\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        fig_plt = sns.lineplot(x=np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), y=self.plot_losses)\n",
    "        fig_plt.set_title(\"Loss Over Time\")\n",
    "        fig_plt.set_ylabel(\"Loss\")\n",
    "        fig_plt.set_xlabel(\"Epochs\")\n",
    "        return fig_plt.get_figure()\n",
    "    \n",
    "    def showScore(self):\n",
    "        df = pd.concat([pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_train_scores, 'Score':'Train'}), \n",
    "                        pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_dev_scores, 'Score':'Dev'})], axis=0)\n",
    "    \n",
    "        plt.figure()\n",
    "        pp = sns.lineplot(data=df, x = 'X', y = 'Y', hue='Score', style=\"Score\", legend= \"brief\")\n",
    "        fig_plt.set_title(\"Score Over Time\")\n",
    "        fig_plt.set_ylabel(\"Score\")\n",
    "        fig_plt.set_xlabel(\"Epoch\")\n",
    "        return fig_plt.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1\n",
    "\n",
    "encoder_params = {'hidden_size':512, 'n_layers':1}\n",
    "decoder_params = {'hidden_size':encoder_params['hidden_size'], 'n_layers':1, 'output_size':en.n_words}\n",
    "attn_decoder_params = {'hidden_size':encoder_params['hidden_size'], 'n_layers':1, 'output_size':en.n_words, \"dropout\":0.1}\n",
    "\n",
    "encoder = EncoderRNN(encoder_params, vi.emb, vi.learn_ids).to(device)\n",
    "encoder_optim = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "decoder = DecoderRNN(decoder_params, en.emb, en.learn_ids).to(device)\n",
    "decoder_optim = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "attn_decoder = AttnDecoderRNN(attn_decoder_params, en.emb, en.learn_ids).to(device)\n",
    "attn_decoder_optim = optim.Adam(attn_decoder.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Epoch:1 | Time Elapsed:0m 10s (- 1m 33s) | Percent Complete:1e+01 | Loss:0.04598 | TrainScore:0.03035 | DevScore:0.00612\n",
      "Epoch:2 | Time Elapsed:0m 20s (- 1m 20s) | Percent Complete:2e+01 | Loss:0.04183 | TrainScore:0.03035 | DevScore:0.006092\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bdcff06bfbf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_optim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_optim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-6320a63db923>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, dev_data, n_epoch, print_every, n_grams)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mtrain_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbleuScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0mdev_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbleuScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0mplot_train_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mplot_dev_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6320a63db923>\u001b[0m in \u001b[0;36mbleuScore\u001b[0;34m(self, data_loader, n_grams)\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[1;31m#Save result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(encoder, attn_decoder, encoder_optim, decoder_optim)\n",
    "model.fit(train_loader, dev_loader,  n_epoch=10, print_every=250, n_grams=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp, inp_lens, out, out_mask, out_max in train_loader:\n",
    "    for s, seq in enumerate(inp.t()):\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2', '3'], ['1', '2', '3'], ['4', '5', '2']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your unsorted array\n",
    "\n",
    "a = [[d0, p0], [d1, p1], [d2,p2], [d3, p3], [d4, p4]]\n",
    "arr_sum = 0\n",
    "\n",
    "for i in range(len(a)):\n",
    "    #All elements of array up to i are sorted\n",
    "    #min_idx is the index of the minimum of the unsorted array\n",
    "    min_idx = i\n",
    "    \n",
    "    #Iterate through unsorted part of array\n",
    "    for j in range(i+1, len(a)):\n",
    "        #If j is less than min_idx min_idx = j\n",
    "        if a[min_idx][0] > a[j][0]:\n",
    "            min_idx = j\n",
    "            \n",
    "    #Move minimum of unsorted array into correct sorted position\n",
    "    a[i], a[min_idx] = a[min_idx], a[i]\n",
    "    \n",
    "    #Add the price of the new sorted house to the total\n",
    "    arr_sum += a[i][1]\n",
    "    \n",
    "    #Check if you have enough neighbors\n",
    "    if i == k:\n",
    "        #Stop loop\n",
    "        break\n",
    "    \n",
    "#Return average\n",
    "return arr_sum/k\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
