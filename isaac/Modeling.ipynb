{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, tensorToList\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir, em_dir = directories()\n",
    "\n",
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3\n",
    "NUM_SPECIAL = len(SPECIAL_SYMBOLS_ID)\n",
    "\n",
    "vi, en = loadLangPairs(\"vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(len(vi.train_num)) if (len(vi.train[i]) < vi.max_length) & (len(en.train[i]) < en.max_length)])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "dev_dataset = langDataset([(vi.dev_num[i], en.dev_num[i]) for i in range(len(vi.dev_num)) if (len(vi.dev[i]) < vi.max_length) & (len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Encoder RNN\n",
    "        Input\n",
    "            params - Dictionary of paramter\n",
    "            raw_emb - (100,000, 300) raw embeddings\n",
    "            learn_ids - list of ids to do embedding learning\n",
    "            \n",
    "            inp - (Max Length, Batch Size), original inputs\n",
    "            inp_lens - (Batch Size), true length of inputs\n",
    "        Output\n",
    "            output - (Max Length, Batch Size, Hidden Size), GRU output\n",
    "            hidden - (2, Batch Size, Hidden Size), Final hidden state of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "    \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        \n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.n_layers, bidirectional=True)\n",
    "        \n",
    "    def forward(self, inp, inp_lens):\n",
    "        #Embed input\n",
    "        embedded = self.embedding(inp)\n",
    "        #Pack padded\n",
    "        packed = pack_padded_sequence(embedded, inp_lens).to(device)\n",
    "        \n",
    "        #GRU\n",
    "        output, self.hidden = self.gru(packed)\n",
    "        #Pad packed\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        #Concat bidirectional layers\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        return output, self.hidden\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Decoder RNN\n",
    "        Input\n",
    "            params - Dictionary of paramter\n",
    "            raw_emb - (100,000, 300) raw embeddings\n",
    "            learn_ids - list of ids to do embedding learning\n",
    "            \n",
    "            inp - (1, Batch Size), SOS Token each in batch size\n",
    "            hidden - (2, 1, Hidden Size), Prev hidden size\n",
    "        Output\n",
    "            output - (32, Vocab Size), Probabilities\n",
    "            hidden - (2, 1 Size, Hidden Size), Final hidden state of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        self.output_size = params['output_size']\n",
    "\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.n_layers, bidirectional=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_output=None):\n",
    "        #Embed\n",
    "        embedded = self.embedding(inp)\n",
    "        #Dropout\n",
    "        output = F.relu(embedded)\n",
    "        \n",
    "        #Gru\n",
    "        output, self.hidden = self.gru(output, hidden)\n",
    "        #Concat directions\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        #GRU output to probabilities\n",
    "        output = torch.exp(self.softmax(self.out(output))).squeeze(0)\n",
    "        return output, hidden, None\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    \"\"\"Masked Loss\"\"\"\n",
    "    #Total # real words\n",
    "    nTotal = mask.sum()\n",
    "    #Cross entropy\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1).to(device)))\n",
    "    #Select loss of real words\n",
    "    loss = crossEntropy.masked_select(mask.to(device)).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, encoder, decoder, encoder_optim, decoder_optim):\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.encoder_optim = encoder_optim\n",
    "        self.decoder_optim = decoder_optim\n",
    "        \n",
    "    def fit(self, train_data, dev_data, teacher_forcing_ratio, n_epoch, print_every, n_grams):\n",
    "        start = time.time()\n",
    "        \n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.n_epoch = n_epoch\n",
    "        \n",
    "        \n",
    "        print(\"Initializing...\")\n",
    "        start_epoch = 1\n",
    "        print_loss_total = 0 \n",
    "        plot_loss_total = 0\n",
    "        plot_losses = []\n",
    "        plot_train_scores = []\n",
    "        plot_dev_scores = []\n",
    "        \n",
    "        for epoch in range(start_epoch, n_epoch):\n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "                loss = self.trainEpoch(inp, inp_lens, output, out_mask, out_max)\n",
    "                \n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "            \n",
    "                if i % print_every == 0:\n",
    "                    train_score = self.bleuScore(train_loader, n_grams)\n",
    "                    dev_score = self.bleuScore(dev_loader, n_grams)\n",
    "                    plot_train_scores.append(train_score)\n",
    "                    plot_dev_scores.append(dev_score)\n",
    "                    plot_loss_avg = plot_loss_total / print_every\n",
    "                    plot_losses.append(plot_loss_avg)\n",
    "                    plot_loss_total = 0       \n",
    "\n",
    "                    print_loss_avg = print_loss_total / print_every\n",
    "                    print_loss_total = 0\n",
    "                    print(\"{} ({} {}) Iter: {}/{} | Loss:{:.4} | TrainScore:{} | DevScore:{}\".format(timeSince(start, epoch/n_epoch), \n",
    "                                                                                                    epoch, \n",
    "                                                                                                    epoch/n_epoch*100, \n",
    "                                                                                                    i,\n",
    "                                                                                                    len(train_loader),\n",
    "                                                                                                    print_loss_avg, \n",
    "                                                                                                    train_score,\n",
    "                                                                                                    dev_score))                \n",
    "                if i % print_every == 0 and i > 0:\n",
    "                    break\n",
    "        self.plot_losses = plot_losses\n",
    "        self.plot_train_scores = plot_train_scores\n",
    "        self.plot_dev_scores = plot_dev_scores\n",
    "        return \"Training Complete!\"            \n",
    "            \n",
    "    def trainEpoch(self, inp, inp_lens, output, out_mask, out_max):\n",
    "        #Zero gradients\n",
    "        self.encoder_optim.zero_grad()\n",
    "        self.decoder_optim.zero_grad()\n",
    "\n",
    "        #Loss vars\n",
    "        loss, print_losses, n_totals = 0, [], 0\n",
    "        \n",
    "        #Encoder Forward\n",
    "        encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "        \n",
    "        #Init decoder_input\n",
    "        decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]]).to(device)\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]\n",
    "\n",
    "        #Teacher Forcing\n",
    "        if random.random() < self.teacher_forcing_ratio:\n",
    "            for t in range(out_max):\n",
    "                #Decoder Forward\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                #True Output\n",
    "                decoder_input = output[t].view(1, -1)\n",
    "                \n",
    "                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])\n",
    "                loss += mask_loss\n",
    "                print_losses.append(mask_loss.item() * nTotal)\n",
    "                n_totals += nTotal\n",
    "        else:\n",
    "            for t in range(out_max):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]]).to(device)\n",
    "                \n",
    "                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])\n",
    "                loss += mask_loss\n",
    "                print_losses.append(mask_loss.item() * nTotal)\n",
    "                n_totals += nTotal\n",
    "                \n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optim.step()\n",
    "        self.decoder_optim.step()\n",
    "\n",
    "        return sum(print_losses) / n_totals\n",
    "    \n",
    "    def bleuScore(self, data_loader, n_grams):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            true_outputs = []\n",
    "            decoder_outputs = []\n",
    "            \n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(data_loader):\n",
    "                if i * BATCH_SIZE > len(dev_loader) * BATCH_SIZE:\n",
    "                    break\n",
    "                true_outputs += tensorToList(output)\n",
    "                \n",
    "                encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "                \n",
    "                decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]])\n",
    "                decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]\n",
    "                \n",
    "                decoder_batch_outputs = []\n",
    "                decoder_batch_outputs += decoder_input.tolist()\n",
    "                \n",
    "                for t in range(out_max):\n",
    "                    decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                    _, topi = decoder_output.topk(1)\n",
    "                    decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]])\n",
    "                    decoder_batch_outputs += decoder_input.tolist()\n",
    "                    \n",
    "                decoder_outputs += tensorToList(torch.tensor(decoder_batch_outputs))\n",
    "        return corpus_bleu(decoder_outputs, true_outputs, n_grams)\n",
    "\n",
    "    def showLoss(self):\n",
    "        plt.figure()\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        fig_plt = sns.lineplot(x=np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), y=self.plot_losses)\n",
    "        fig_plt.set_title(\"Loss Over Time\")\n",
    "        fig_plt.set_ylabel(\"Loss\")\n",
    "        fig_plt.set_xlabel(\"Epochs\")\n",
    "        return fig_plt.get_figure()\n",
    "    \n",
    "    def showScore(self):\n",
    "        df = pd.concat([pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_train_scores, 'Score':'Train'}), \n",
    "                        pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_dev_scores, 'Score':'Dev'})], axis=0)\n",
    "    \n",
    "        plt.figure()\n",
    "        pp = sns.lineplot(data=df, x = 'X', y = 'Y', hue='Score', style=\"Score\", legend= \"brief\")\n",
    "        fig_plt.set_title(\"Score Over Time\")\n",
    "        fig_plt.set_ylabel(\"Score\")\n",
    "        fig_plt.set_xlabel(\"Epoch\")\n",
    "        return fig_plt.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "encoder_params = {'hidden_size':256, 'n_layers':1}\n",
    "decoder_params = {'hidden_size':encoder_params['hidden_size'], 'n_layers':1, 'output_size':en.n_words}\n",
    "\n",
    "encoder = EncoderRNN(encoder_params, vi.emb, vi.learn_ids).to(device)\n",
    "encoder_optim = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "decoder = DecoderRNN(decoder_params, en.emb, en.learn_ids).to(device)\n",
    "decoder_optim = optim.Adam(decoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "0m 14s (- 0m 43s) (1 25.0) Iter: 0/1713 | Loss:0.0104 | TrainScore:1.1092725623125301 | DevScore:0.9266998864732778\n"
     ]
    }
   ],
   "source": [
    "model = Model(encoder, decoder, encoder_optim, decoder_optim)\n",
    "model.fit(train_loader, dev_loader, teacher_forcing_ratio=1.0, n_epoch=4, print_every=400, n_grams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = params['method']\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        \n",
    "        #Define extra functions depending on method\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'general':\n",
    "            #h_t.T.dot(W_a.dot(h_s))\n",
    "            attn_energies = torch.sum(hidden * self.attn(encoder_output), dim=2)\n",
    "        elif self.method == 'concat':\n",
    "            #v_a * tanh(w_a[h_t,h_s])\n",
    "            energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "            attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "        elif self.method == 'dot':\n",
    "            #h_t.T.dot(h_s)\n",
    "            attn_energies = torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "        return F.softmax(attn_energies.t(), dim=1).unsqueeze(1)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = params['attn_model']\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.output_size = params['output_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        self.dropout = params['dropout']\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.embed_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.n_layers, dropout=(0 if n_layers == 1 else dropout), batch_first=True)\n",
    "        self.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "        self.attn = Attn(self.attn_model, self.hidden_size)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_output):\n",
    "        #Embedding and dropout\n",
    "        embedded = self.embedding(inp)\n",
    "        embedded = self.embed_dropout(embedded)\n",
    "        \n",
    "        #GRU\n",
    "        rnn_out, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        #Attn weights * enc_output\n",
    "        attn_weights = self.attn(rnn_output, encoder_output)\n",
    "        context = attn_weights.bmm(encoder_output.transpose(0, 1))\n",
    "        \n",
    "        #Concat context to GRU output\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_inp = torch.cat((rnn_out, context), 1)\n",
    "        concat_out = torch.tanh(self.concat(concat_inp))\n",
    "        \n",
    "        #Prediction\n",
    "        output = self.out(concat_out)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        return output, hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
