{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, tensorToList\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir, em_dir = directories()\n",
    "\n",
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3\n",
    "NUM_SPECIAL = len(SPECIAL_SYMBOLS_ID)\n",
    "\n",
    "vi, en = loadLangPairs(\"vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(len(vi.train_num)) if (len(vi.train[i]) < vi.max_length) & (len(en.train[i]) < en.max_length)])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "dev_dataset = langDataset([(vi.dev_num[i], en.dev_num[i]) for i in range(len(vi.dev_num)) if (len(vi.dev[i]) < vi.max_length) & (len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.num_layers = params['num_layers']\n",
    "        \n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, inp, inp_lens, hidden=None):\n",
    "        embedded = self.embedding(inp)\n",
    "        packed = pack_padded_sequence(embedded, inp_lens).to(device)\n",
    "        \n",
    "        output, self.hidden = self.gru(packed, hidden)\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        return output, self.hidden\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.output_size = params['output_size']\n",
    "\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.num_layers, batch_first=True, bidirectional=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_output=None):\n",
    "        embedded = self.embedding(inp)\n",
    "        output = F.relu(embedded)\n",
    "        \n",
    "        output, self.hidden = self.gru(output, hidden)\n",
    "        orig = output\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        output = torch.exp(self.softmax(self.out(output))).squeeze(0)\n",
    "        return output, hidden, None\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1).to(device)))\n",
    "    loss = crossEntropy.masked_select(mask.to(device)).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, encoder, decoder, encoder_optim, decoder_optim):\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.encoder_optim = encoder_optim\n",
    "        self.decoder_optim = decoder_optim\n",
    "        \n",
    "    def fit(self, train_data, dev_data, teacher_forcing_ratio, n_epoch, print_every, plot_every, n_grams):\n",
    "        start = time.time()\n",
    "        \n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.n_epoch = n_epoch\n",
    "        \n",
    "        \n",
    "        print(\"Initializing...\")\n",
    "        start_epoch = 1\n",
    "        print_loss_total = 0 \n",
    "        plot_loss_total = 0\n",
    "        plot_losses = []\n",
    "        plot_train_scores = []\n",
    "        plot_dev_scores = []\n",
    "        \n",
    "        for epoch in range(start_epoch, n_epoch):\n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "                loss = self.trainEpoch(inp, inp_lens, output, out_mask, out_max)\n",
    "                \n",
    "                print_loss_total += loss\n",
    "                \n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                train_score = self.bleuScore(train_loader, n_grams)\n",
    "                dev_score = self.bleuScore(dev_score, n_grams)\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print(\"{} ({} {}) Loss:{:.4} | TrainScore:{} | DevScore:{}\".format(timeSince(start, epoch/n_epoch), \n",
    "                                                                                        epoch, \n",
    "                                                                                        epoch/n_epoch*100, \n",
    "                                                                                        print_loss_avg, \n",
    "                                                                                        train_score,\n",
    "                                                                                        dev_score))\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                train_score = self.bleuScore(train_loader, n_grams)\n",
    "                dev_score = self.bleuScore(dev_score, n_grams)\n",
    "                plot_train_scores.append(train_score)\n",
    "                plot_dev_scores.append(dev_score)\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0       \n",
    "                \n",
    "                \n",
    "        self.plot_losses = plot_losses\n",
    "        self.plot_train_scores = plot_train_scores\n",
    "        self.plot_dev_scores = plot_dev_scores\n",
    "        return \"Training Complete!\"            \n",
    "            \n",
    "    def trainEpoch(self, inp, inp_lens, output, out_mask, out_max):\n",
    "        self.encoder_optim.zero_grad()\n",
    "        self.decoder_optim.zero_grad()\n",
    "\n",
    "        loss, print_losses, n_totals = 0, [], 0\n",
    "        \n",
    "        encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "        \n",
    "        decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]]).to(device)\n",
    "        decoder_hidden = encoder_hidden[:,-1:,:].contiguous()\n",
    "\n",
    "        if random.random() < self.teacher_forcing_ratio:\n",
    "            for t in range(out_max):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                decoder_input = output[t].view(1, -1)\n",
    "                \n",
    "                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])\n",
    "                loss += mask_loss\n",
    "                print_losses.append(mask_loss.item() * nTotal)\n",
    "                n_totals += nTotal\n",
    "        else:\n",
    "            for t in range(out_max):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]]).to(device)\n",
    "                \n",
    "                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])\n",
    "                loss += mask_loss\n",
    "                print_losses.append(mask_loss.item() * nTotal)\n",
    "                n_totals += nTotal\n",
    "                \n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optim.step()\n",
    "        self.decoder_optim.step()\n",
    "\n",
    "        return sum(print_losses) / n_totals\n",
    "    \n",
    "    def bleuScore(self, data_loader, n_grams):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            true_outputs = []\n",
    "            output_to_bleu = []\n",
    "            \n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(data_loader):\n",
    "                \n",
    "                true_outputs += tensorToList(output)\n",
    "                \n",
    "                encoder_ouput, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "                \n",
    "                decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]]).to(device)\n",
    "                decoder_hidden = encoder_hidden[:,-1:,:].contiguous()\n",
    "                \n",
    "                decoder_batch_outputs = []\n",
    "                decoder_batch_outputs += decoder_input.tolist()\n",
    "                \n",
    "                for t in range(out_max):\n",
    "                    decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                    _, topi = decoder_output.topk(1)\n",
    "                    decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]]).to(device)\n",
    "                    decoder_batch_outputs += decoder_input.tolist()\n",
    "                    \n",
    "                decoder_outputs = tensorToList(torch.tensor(decoder_batch_outputs))\n",
    "                \n",
    "        return corpus_bleu(decoder_outputs, true_outputs, n_grams)\n",
    "\n",
    "    def showLoss(self):\n",
    "        plt.figure()\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        fig_plt = sns.lineplot(x=np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), y=self.plot_losses)\n",
    "        fig_plt.set_title(\"Loss Over Time\")\n",
    "        fig_plt.set_ylabel(\"Loss\")\n",
    "        fig_plt.set_xlabel(\"Epochs\")\n",
    "        return fig_plt.get_figure()\n",
    "    \n",
    "    def showScore(self):\n",
    "        df = pd.concat([pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_train_scores, 'Score':'Train'}), \n",
    "                        pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_dev_scores, 'Score':'Dev'})], axis=0)\n",
    "    \n",
    "        plt.figure()\n",
    "        pp = sns.lineplot(data=df, x = 'X', y = 'Y', hue='Score', style=\"Score\", legend= \"brief\")\n",
    "        fig_plt.set_title(\"Score Over Time\")\n",
    "        fig_plt.set_ylabel(\"Score\")\n",
    "        fig_plt.set_xlabel(\"Epoch\")\n",
    "        return fig_plt.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "encoder_params = {'hidden_size':256, 'num_layers':1}\n",
    "decoder_params = {'hidden_size':encoder_params['hidden_size'], 'num_layers':1, 'output_size':en.n_words}\n",
    "\n",
    "encoder = EncoderRNN(encoder_params, vi.emb, vi.learn_ids).to(device)\n",
    "encoder_optim = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "decoder = DecoderRNN(decoder_params, en.emb, en.learn_ids).to(device)\n",
    "decoder_optim = optim.Adam(decoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n"
     ]
    }
   ],
   "source": [
    "model = Model(encoder, decoder, encoder_optim, decoder_optim)\n",
    "model.fit(train_loader, dev_loader, teacher_forcing_ratio=0.5, n_epoch=10, print_every=1, plot_every=1, n_grams=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
