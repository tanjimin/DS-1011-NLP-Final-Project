{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, tensorToList\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir, em_dir = directories()\n",
    "\n",
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3\n",
    "NUM_SPECIAL = len(SPECIAL_SYMBOLS_ID)\n",
    "\n",
    "vi, en = loadLangPairs(\"vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(len(vi.train_num)) if (len(vi.train[i]) < vi.max_length) & (len(en.train[i]) < en.max_length)])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)\n",
    "dev_dataset = langDataset([(vi.dev_num[i], en.dev_num[i]) for i in range(len(vi.dev_num)) if (len(vi.dev[i]) < vi.max_length) & (len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Encoder RNN\n",
    "        Input\n",
    "            params - Dictionary of paramter\n",
    "            raw_emb - (100,000, 300) raw embeddings\n",
    "            learn_ids - list of ids to do embedding learning\n",
    "            \n",
    "            inp - (Max Length, Batch Size), original inputs\n",
    "            inp_lens - (Batch Size), true length of inputs\n",
    "        Output\n",
    "            output - (Max Length, Batch Size, Hidden Size), GRU output\n",
    "            hidden - (2, Batch Size, Hidden Size), Final hidden state of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "    \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        \n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.n_layers, bidirectional=True)\n",
    "        \n",
    "    def forward(self, inp, inp_lens):\n",
    "        #Embed input\n",
    "        embedded = self.embedding(inp)\n",
    "        #Pack padded\n",
    "        packed = pack_padded_sequence(embedded, inp_lens).to(device)\n",
    "        \n",
    "        #GRU\n",
    "        output, self.hidden = self.gru(packed)\n",
    "        #Pad packed\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        #Concat bidirectional layers\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        return output, self.hidden\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Decoder RNN\n",
    "        Input\n",
    "            params - Dictionary of paramter\n",
    "            raw_emb - (100,000, 300) raw embeddings\n",
    "            learn_ids - list of ids to do embedding learning\n",
    "            \n",
    "            inp - (1, Batch Size), SOS Token each in batch size\n",
    "            hidden - (2, 1, Hidden Size), Prev hidden size\n",
    "        Output\n",
    "            output - (32, Vocab Size), Probabilities\n",
    "            hidden - (2, 1 Size, Hidden Size), Final hidden state of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        self.output_size = params['output_size']\n",
    "\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.n_layers, bidirectional=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_output=None):\n",
    "        #Embed\n",
    "        embedded = self.embedding(inp)\n",
    "        #Dropout\n",
    "        output = F.relu(embedded)\n",
    "        \n",
    "        #Gru\n",
    "        output, self.hidden = self.gru(output, hidden)\n",
    "        #Concat directions\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        #GRU output to probabilities\n",
    "        output = torch.exp(self.softmax(self.out(output))).squeeze(0)\n",
    "        return output, hidden, None\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    \"\"\"Masked Loss\"\"\"\n",
    "    #Total # real words\n",
    "    nTotal = mask.sum()\n",
    "    #Cross entropy\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1).to(device)))\n",
    "    #Select loss of real words\n",
    "    loss = crossEntropy.masked_select(mask.to(device)).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, encoder, decoder, encoder_optim, decoder_optim):\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.encoder_optim = encoder_optim\n",
    "        self.decoder_optim = decoder_optim\n",
    "        \n",
    "    def fit(self, train_data, dev_data, teacher_forcing_ratio, n_epoch, print_every, n_grams):\n",
    "        start = time.time()\n",
    "        \n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.n_epoch = n_epoch\n",
    "        \n",
    "        \n",
    "        print(\"Initializing...\")\n",
    "        start_epoch = 1\n",
    "        print_loss_total, plot_loss_total = 0 , 0\n",
    "        plot_losses, plot_train_scores, plot_dev_scores = [], [], []\n",
    "        \n",
    "        for epoch in range(start_epoch, n_epoch):\n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "                loss = self.trainEpoch(inp, inp_lens, output, out_mask, out_max)\n",
    "                \n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "                    \n",
    "                if i % print_every == 0:\n",
    "                    train_score = self.bleuScore(train_loader, n_grams)\n",
    "                    dev_score = self.bleuScore(dev_loader, n_grams)\n",
    "                    plot_train_scores.append(train_score)\n",
    "                    plot_dev_scores.append(dev_score)\n",
    "                    plot_loss_avg = plot_loss_total / print_every\n",
    "                    plot_losses.append(plot_loss_avg)\n",
    "                    plot_loss_total = 0       \n",
    "\n",
    "                    print_loss_avg = print_loss_total / print_every\n",
    "                    print_loss_total = 0\n",
    "                    print(\"Epoch:{} | Time Elapsed:{} | Percent Complete:{:.1} | Loss:{:.4} | TrainScore:{:.4} | DevScore:{:.4}\".format(epoch,\n",
    "                                                                                                                                  timeSince(start, epoch/n_epoch), \n",
    "                                                                                                                                  epoch/n_epoch*100, \n",
    "                                                                                                                                  print_loss_avg, \n",
    "                                                                                                                                  train_score,\n",
    "                                                                                                                                  dev_score))                \n",
    "\n",
    "        self.plot_losses = plot_losses\n",
    "        self.plot_train_scores = plot_train_scores\n",
    "        self.plot_dev_scores = plot_dev_scores\n",
    "        return \"Training Complete!\"            \n",
    "            \n",
    "    def trainEpoch(self, inp, inp_lens, output, out_mask, out_max):\n",
    "        #Zero gradients\n",
    "        self.encoder_optim.zero_grad()\n",
    "        self.decoder_optim.zero_grad()\n",
    "\n",
    "        #Loss vars\n",
    "        loss, print_losses, n_totals = 0, [], 0\n",
    "        \n",
    "        #Encoder Forward\n",
    "        encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "        \n",
    "        #Init decoder_input\n",
    "        decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]]).to(device)\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]\n",
    "\n",
    "        #Teacher Forcing\n",
    "        if random.random() < self.teacher_forcing_ratio:\n",
    "            for t in range(out_max):\n",
    "                #Decoder Forward\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                #True Output\n",
    "                decoder_input = output[t].view(1, -1)\n",
    "                \n",
    "                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])\n",
    "                loss += mask_loss\n",
    "                print_losses.append(mask_loss.item() * nTotal)\n",
    "                n_totals += nTotal\n",
    "        else:\n",
    "            for t in range(out_max):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]]).to(device)\n",
    "                \n",
    "                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])\n",
    "                loss += mask_loss\n",
    "                print_losses.append(mask_loss.item() * nTotal)\n",
    "                n_totals += nTotal\n",
    "                \n",
    "        loss.backward()\n",
    "        \n",
    "        _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), 50)\n",
    "        _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), 50)\n",
    "        \n",
    "        self.encoder_optim.step()\n",
    "        self.decoder_optim.step()\n",
    "\n",
    "        return sum(print_losses) / n_totals\n",
    "    \n",
    "    def bleuScore(self, data_loader, n_grams):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            true_outputs = []\n",
    "            decoder_outputs = []\n",
    "            \n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(data_loader):\n",
    "                if i * BATCH_SIZE > 1000:\n",
    "                    break\n",
    "            \n",
    "\n",
    "                for s, seq in enumerate(inp.t()):\n",
    "                    true_outputs += [[str(w) for w in output[:,s].tolist()]]\n",
    "        \n",
    "                    encoder_output, encoder_hidden = self.encoder(seq.view(-1, 1), inp_lens[s].view(1))\n",
    "        \n",
    "                    decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_ID\n",
    "                    decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]\n",
    "\n",
    "                    decoder_seq_output = [decoder_input.item()]\n",
    "\n",
    "                    for t in range(out_max):\n",
    "                        decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                        _, decoder_input = torch.max(decoder_output, dim=1)\n",
    "                        \n",
    "                        if (decoder_input.item() == EOS_ID) | (decoder_input.item() == PAD_ID):\n",
    "                            decoder_seq_output.append(EOS_ID)\n",
    "                            break\n",
    "                        \n",
    "                        decoder_seq_output.append(decoder_input.item())\n",
    "                        decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "                    \n",
    "                    decoder_seq_output[-1] = EOS_ID\n",
    "                    decoder_outputs += [[str(w) for w in decoder_seq_output]]\n",
    "                    \n",
    "        return corpus_bleu(decoder_outputs, true_outputs, n_grams)\n",
    "\n",
    "    def showLoss(self):\n",
    "        plt.figure()\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        fig_plt = sns.lineplot(x=np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), y=self.plot_losses)\n",
    "        fig_plt.set_title(\"Loss Over Time\")\n",
    "        fig_plt.set_ylabel(\"Loss\")\n",
    "        fig_plt.set_xlabel(\"Epochs\")\n",
    "        return fig_plt.get_figure()\n",
    "    \n",
    "    def showScore(self):\n",
    "        df = pd.concat([pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_train_scores, 'Score':'Train'}), \n",
    "                        pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_dev_scores, 'Score':'Dev'})], axis=0)\n",
    "    \n",
    "        plt.figure()\n",
    "        pp = sns.lineplot(data=df, x = 'X', y = 'Y', hue='Score', style=\"Score\", legend= \"brief\")\n",
    "        fig_plt.set_title(\"Score Over Time\")\n",
    "        fig_plt.set_ylabel(\"Score\")\n",
    "        fig_plt.set_xlabel(\"Epoch\")\n",
    "        return fig_plt.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "encoder_params = {'hidden_size':512, 'n_layers':1}\n",
    "decoder_params = {'hidden_size':encoder_params['hidden_size'], 'n_layers':1, 'output_size':en.n_words}\n",
    "\n",
    "encoder = EncoderRNN(encoder_params, vi.emb, vi.learn_ids).to(device)\n",
    "encoder_optim = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "decoder = DecoderRNN(decoder_params, en.emb, en.learn_ids).to(device)\n",
    "decoder_optim = optim.Adam(decoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Epoch:1 | Time Elapsed:4m 9s (- 37m 25s) | Percent Complete:1e+01 | Loss:0.0104 | TrainScore:46.82 | DevScore:43.93\n",
      "Epoch:1 | Time Elapsed:11m 45s (- 105m 47s) | Percent Complete:1e+01 | Loss:3.322 | TrainScore:46.21 | DevScore:43.73\n",
      "Epoch:1 | Time Elapsed:19m 17s (- 173m 40s) | Percent Complete:1e+01 | Loss:2.938 | TrainScore:45.45 | DevScore:43.66\n",
      "Epoch:1 | Time Elapsed:26m 50s (- 241m 31s) | Percent Complete:1e+01 | Loss:2.794 | TrainScore:46.99 | DevScore:44.17\n",
      "Epoch:1 | Time Elapsed:34m 22s (- 309m 21s) | Percent Complete:1e+01 | Loss:2.711 | TrainScore:45.7 | DevScore:43.6\n",
      "Epoch:2 | Time Elapsed:39m 23s (- 157m 33s) | Percent Complete:2e+01 | Loss:0.7494 | TrainScore:43.99 | DevScore:43.98\n",
      "Epoch:2 | Time Elapsed:46m 55s (- 187m 42s) | Percent Complete:2e+01 | Loss:2.542 | TrainScore:45.14 | DevScore:44.04\n",
      "Epoch:2 | Time Elapsed:54m 29s (- 217m 58s) | Percent Complete:2e+01 | Loss:2.513 | TrainScore:46.11 | DevScore:44.32\n",
      "Epoch:2 | Time Elapsed:62m 1s (- 248m 6s) | Percent Complete:2e+01 | Loss:2.481 | TrainScore:45.9 | DevScore:44.09\n",
      "Epoch:2 | Time Elapsed:69m 33s (- 278m 12s) | Percent Complete:2e+01 | Loss:2.463 | TrainScore:44.76 | DevScore:43.75\n",
      "Epoch:3 | Time Elapsed:74m 36s (- 174m 5s) | Percent Complete:3e+01 | Loss:0.6883 | TrainScore:44.56 | DevScore:44.18\n",
      "Epoch:3 | Time Elapsed:82m 9s (- 191m 41s) | Percent Complete:3e+01 | Loss:2.336 | TrainScore:45.37 | DevScore:44.43\n",
      "Epoch:3 | Time Elapsed:89m 43s (- 209m 20s) | Percent Complete:3e+01 | Loss:2.333 | TrainScore:46.56 | DevScore:44.3\n",
      "Epoch:3 | Time Elapsed:97m 15s (- 226m 57s) | Percent Complete:3e+01 | Loss:2.329 | TrainScore:45.9 | DevScore:44.52\n",
      "Epoch:3 | Time Elapsed:104m 47s (- 244m 30s) | Percent Complete:3e+01 | Loss:2.322 | TrainScore:45.95 | DevScore:43.95\n",
      "Epoch:4 | Time Elapsed:109m 56s (- 164m 54s) | Percent Complete:4e+01 | Loss:0.6508 | TrainScore:45.48 | DevScore:44.58\n",
      "Epoch:4 | Time Elapsed:117m 30s (- 176m 15s) | Percent Complete:4e+01 | Loss:2.232 | TrainScore:46.47 | DevScore:44.06\n",
      "Epoch:4 | Time Elapsed:125m 1s (- 187m 31s) | Percent Complete:4e+01 | Loss:2.239 | TrainScore:44.23 | DevScore:44.06\n",
      "Epoch:4 | Time Elapsed:132m 34s (- 198m 51s) | Percent Complete:4e+01 | Loss:2.24 | TrainScore:45.81 | DevScore:43.8\n",
      "Epoch:4 | Time Elapsed:140m 8s (- 210m 13s) | Percent Complete:4e+01 | Loss:2.237 | TrainScore:46.39 | DevScore:44.09\n",
      "Epoch:5 | Time Elapsed:145m 11s (- 145m 11s) | Percent Complete:5e+01 | Loss:0.6281 | TrainScore:45.08 | DevScore:43.84\n",
      "Epoch:5 | Time Elapsed:152m 40s (- 152m 40s) | Percent Complete:5e+01 | Loss:2.162 | TrainScore:44.56 | DevScore:44.18\n",
      "Epoch:5 | Time Elapsed:160m 14s (- 160m 14s) | Percent Complete:5e+01 | Loss:2.171 | TrainScore:45.63 | DevScore:44.11\n",
      "Epoch:5 | Time Elapsed:167m 47s (- 167m 47s) | Percent Complete:5e+01 | Loss:2.179 | TrainScore:46.59 | DevScore:44.0\n",
      "Epoch:5 | Time Elapsed:175m 19s (- 175m 19s) | Percent Complete:5e+01 | Loss:2.18 | TrainScore:45.51 | DevScore:43.7\n",
      "Epoch:6 | Time Elapsed:180m 21s (- 120m 14s) | Percent Complete:6e+01 | Loss:0.6143 | TrainScore:44.87 | DevScore:44.09\n",
      "Epoch:6 | Time Elapsed:187m 53s (- 125m 15s) | Percent Complete:6e+01 | Loss:2.11 | TrainScore:47.51 | DevScore:44.25\n",
      "Epoch:6 | Time Elapsed:195m 28s (- 130m 19s) | Percent Complete:6e+01 | Loss:2.12 | TrainScore:46.49 | DevScore:44.25\n",
      "Epoch:6 | Time Elapsed:202m 59s (- 135m 19s) | Percent Complete:6e+01 | Loss:2.125 | TrainScore:46.12 | DevScore:43.88\n",
      "Epoch:6 | Time Elapsed:210m 29s (- 140m 19s) | Percent Complete:6e+01 | Loss:2.135 | TrainScore:44.82 | DevScore:43.58\n",
      "Epoch:7 | Time Elapsed:215m 31s (- 92m 22s) | Percent Complete:7e+01 | Loss:0.6001 | TrainScore:45.64 | DevScore:44.32\n",
      "Epoch:7 | Time Elapsed:223m 3s (- 95m 35s) | Percent Complete:7e+01 | Loss:2.065 | TrainScore:46.36 | DevScore:44.2\n",
      "Epoch:7 | Time Elapsed:230m 34s (- 98m 48s) | Percent Complete:7e+01 | Loss:2.075 | TrainScore:45.04 | DevScore:44.36\n",
      "Epoch:7 | Time Elapsed:238m 6s (- 102m 2s) | Percent Complete:7e+01 | Loss:2.084 | TrainScore:44.36 | DevScore:44.31\n",
      "Epoch:7 | Time Elapsed:245m 36s (- 105m 15s) | Percent Complete:7e+01 | Loss:2.091 | TrainScore:45.38 | DevScore:44.11\n",
      "Epoch:8 | Time Elapsed:250m 36s (- 62m 39s) | Percent Complete:8e+01 | Loss:0.5892 | TrainScore:45.32 | DevScore:43.75\n",
      "Epoch:8 | Time Elapsed:258m 9s (- 64m 32s) | Percent Complete:8e+01 | Loss:2.024 | TrainScore:46.53 | DevScore:44.62\n",
      "Epoch:8 | Time Elapsed:265m 42s (- 66m 25s) | Percent Complete:8e+01 | Loss:2.035 | TrainScore:45.16 | DevScore:44.44\n",
      "Epoch:8 | Time Elapsed:273m 14s (- 68m 18s) | Percent Complete:8e+01 | Loss:2.044 | TrainScore:45.38 | DevScore:44.22\n",
      "Epoch:8 | Time Elapsed:280m 45s (- 70m 11s) | Percent Complete:8e+01 | Loss:2.051 | TrainScore:46.16 | DevScore:44.22\n",
      "Epoch:9 | Time Elapsed:285m 47s (- 31m 45s) | Percent Complete:9e+01 | Loss:0.5803 | TrainScore:45.63 | DevScore:44.05\n",
      "Epoch:9 | Time Elapsed:293m 21s (- 32m 35s) | Percent Complete:9e+01 | Loss:1.983 | TrainScore:45.87 | DevScore:44.53\n",
      "Epoch:9 | Time Elapsed:300m 51s (- 33m 25s) | Percent Complete:9e+01 | Loss:2.004 | TrainScore:45.15 | DevScore:44.04\n",
      "Epoch:9 | Time Elapsed:308m 23s (- 34m 15s) | Percent Complete:9e+01 | Loss:2.011 | TrainScore:45.86 | DevScore:44.03\n",
      "Epoch:9 | Time Elapsed:315m 59s (- 35m 6s) | Percent Complete:9e+01 | Loss:2.019 | TrainScore:46.07 | DevScore:44.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Training Complete!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(encoder, decoder, encoder_optim, decoder_optim)\n",
    "model.fit(train_loader, dev_loader, teacher_forcing_ratio=1.0, n_epoch=10, print_every=400, n_grams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp, inp_lens, out, out_mask, out_max in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleuScore(data_loader):\n",
    "    with torch.no_grad():\n",
    "            \n",
    "        true_outputs = []\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(data_loader):\n",
    "            if i * BATCH_SIZE > 100:\n",
    "                break\n",
    "\n",
    "            for s, seq in enumerate(inp.t()):\n",
    "                true_outputs += [[str(w) for w in output[:,s].tolist()]]\n",
    "\n",
    "                encoder_output, encoder_hidden = encoder(seq.view(-1, 1), inp_lens[s].view(1))\n",
    "\n",
    "                decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_ID\n",
    "                decoder_hidden = encoder_hidden[:decoder.n_layers* (2 *decoder.gru.bidirectional)]\n",
    "\n",
    "                decoder_seq_output = [decoder_input.item()]\n",
    "\n",
    "                for t in range(out_max):\n",
    "                    decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                    _, decoder_input = torch.max(decoder_output, dim=1)\n",
    "                    decoder_seq_output.append(decoder_input.item())\n",
    "                    decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "                \n",
    "                decoder_seq_output[-1] = EOS_ID\n",
    "                decoder_outputs += [[str(w) for w in decoder_seq_output]]\n",
    "                    \n",
    "    return decoder_outputs, true_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, true_outputs = bleuScore(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = inp.t()[1]\n",
    "encoder_output, encoder_hidden = encoder(seq.view(-1, 1), inp_lens[1].view(1))\n",
    "        \n",
    "decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_ID\n",
    "decoder_hidden = encoder_hidden[:decoder.n_layers* (2 *decoder.gru.bidirectional)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " tensor([[[-0.0003,  0.0421,  0.0274,  ..., -0.0932, -0.0216, -0.0177]],\n",
       " \n",
       "         [[-0.0174,  0.0321, -0.0225,  ..., -0.0277,  0.0395,  0.0499]]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward>),\n",
       " None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(decoder_input, decoder_hidden, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
       "        [  61,   61,   30,  ...,    7,  226,   61],\n",
       "        [  23,   22, 1485,  ...,  674,   81,   99],\n",
       "        ...,\n",
       "        [2039,    0,    0,  ...,    0,    0,    0],\n",
       "        [   6,    0,    0,  ...,    0,    0,    0],\n",
       "        [   3,    0,    0,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " with torch.no_grad():\n",
    "            \n",
    "            true_outputs = []\n",
    "            decoder_outputs = []\n",
    "            \n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(data_loader):\n",
    "                if i * BATCH_SIZE > len(dev_loader) * BATCH_SIZE:\n",
    "                    break\n",
    "                true_outputs += tensorToList(output)\n",
    "                \n",
    "                encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "                \n",
    "                decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]])\n",
    "                decoder_hidden = \n",
    "                \n",
    "                decoder_batch_outputs = []\n",
    "                decoder_batch_outputs += decoder_input.tolist()\n",
    "                \n",
    "                for t in range(out_max):\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                decoder_outputs += tensorToList(torch.tensor(decoder_batch_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = params['method']\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        \n",
    "        #Define extra functions depending on method\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'general':\n",
    "            #h_t.T.dot(W_a.dot(h_s))\n",
    "            attn_energies = torch.sum(hidden * self.attn(encoder_output), dim=2)\n",
    "        elif self.method == 'concat':\n",
    "            #v_a * tanh(w_a[h_t,h_s])\n",
    "            energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "            attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "        elif self.method == 'dot':\n",
    "            #h_t.T.dot(h_s)\n",
    "            attn_energies = torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "        return F.softmax(attn_energies.t(), dim=1).unsqueeze(1)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = params['attn_model']\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.output_size = params['output_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        self.dropout = params['dropout']\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.embed_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.n_layers, dropout=(0 if n_layers == 1 else dropout), batch_first=True)\n",
    "        self.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "        self.attn = Attn(self.attn_model, self.hidden_size)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_output):\n",
    "        #Embedding and dropout\n",
    "        embedded = self.embedding(inp)\n",
    "        embedded = self.embed_dropout(embedded)\n",
    "        \n",
    "        #GRU\n",
    "        rnn_out, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        #Attn weights * enc_output\n",
    "        attn_weights = self.attn(rnn_output, encoder_output)\n",
    "        context = attn_weights.bmm(encoder_output.transpose(0, 1))\n",
    "        \n",
    "        #Concat context to GRU output\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_inp = torch.cat((rnn_out, context), 1)\n",
    "        concat_out = torch.tanh(self.concat(concat_inp))\n",
    "        \n",
    "        #Prediction\n",
    "        output = self.out(concat_out)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        return output, hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
