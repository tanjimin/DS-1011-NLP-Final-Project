class Model():
    def __init__(self, encoder, decoder, encoder_optim, decoder_optim):
        
        self.encoder = encoder
        self.decoder = decoder
        
        self.encoder_optim = encoder_optim
        self.decoder_optim = decoder_optim
        
    def fit(self, train_data, dev_data, teacher_forcing_ratio, n_epoch, print_every, n_grams):
        start = time.time()
        
        self.teacher_forcing_ratio = teacher_forcing_ratio
        self.n_epoch = n_epoch
        
        
        print("Initializing...")
        start_epoch = 1
        print_loss_total, plot_loss_total = 0 , 0
        plot_losses, plot_train_scores, plot_dev_scores = [], [], []
        
        for epoch in range(start_epoch, n_epoch):
            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):
                loss = self.trainEpoch(inp, inp_lens, output, out_mask, out_max)
                
                print_loss_total += loss
                plot_loss_total += loss
                    
                if i % print_every == 0:
                    train_score = self.bleuScore(train_loader, n_grams)
                    dev_score = self.bleuScore(dev_loader, n_grams)
                    plot_train_scores.append(train_score)
                    plot_dev_scores.append(dev_score)
                    plot_loss_avg = plot_loss_total / print_every
                    plot_losses.append(plot_loss_avg)
                    plot_loss_total = 0       

                    print_loss_avg = print_loss_total / print_every
                    print_loss_total = 0
                    print("Epoch:{} | Time Elapsed:{} | Percent Complete:{:.1} | Loss:{:.4} | TrainScore:{:.4} | DevScore:{:.4}".format(epoch,
                                                                                                                                  timeSince(start, epoch/n_epoch), 
                                                                                                                                  epoch/n_epoch*100, 
                                                                                                                                  print_loss_avg, 
                                                                                                                                  train_score,
                                                                                                                                  dev_score))                

        self.plot_losses = plot_losses
        self.plot_train_scores = plot_train_scores
        self.plot_dev_scores = plot_dev_scores
        return "Training Complete!"            
            
    def trainEpoch(self, inp, inp_lens, output, out_mask, out_max):
        #Zero gradients
        self.encoder_optim.zero_grad()
        self.decoder_optim.zero_grad()

        #Loss vars
        loss, print_losses, n_totals = 0, [], 0
        
        #Encoder Forward
        encoder_output, encoder_hidden = self.encoder(inp, inp_lens)
        
        #Init decoder_input
        decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]]).to(device)
        decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]

        #Teacher Forcing
        if random.random() < self.teacher_forcing_ratio:
            for t in range(out_max):
                #Decoder Forward
                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)
                #True Output
                decoder_input = output[t].view(1, -1)
                
                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])
                loss += mask_loss
                print_losses.append(mask_loss.item() * nTotal)
                n_totals += nTotal
        else:
            for t in range(out_max):
                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)
                _, topi = decoder_output.topk(1)
                decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]]).to(device)
                
                mask_loss, nTotal = maskNLLLoss(decoder_output, output[t], out_mask[t])
                loss += mask_loss
                print_losses.append(mask_loss.item() * nTotal)
                n_totals += nTotal
                
        loss.backward()
        
        _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), 50)
        _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), 50)
        
        self.encoder_optim.step()
        self.decoder_optim.step()

        return sum(print_losses) / n_totals
    
    def bleuScore(self, data_loader, n_grams):
        with torch.no_grad():
            
            true_outputs = []
            decoder_outputs = []

            for i, (inp, inp_lens, out, out_mask, out_max) in enumerate(data_loader):
                if i * BATCH_SIZE > 10000:
                    break

                #Save true masked outputs
                true_outputs += [out[:,i].masked_select(out_mask[:,i]).tolist() for i in range(out.size(1))]
                
                #Encoder forward
                encoder_output, encoder_hidden = self.encoder(inp, inp_lens)
                
                #Decoder inputs
                decoder_input = torch.LongTensor([[SOS_ID] * inp.size(1)]).to(device)
                decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]

                #Decoder Results
                batch_output = decoder_input.clone()
                
                for t in range(out_max):
                    #Decoder forward
                    decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)
                    _, topi = decoder_output.topk(1)
                    decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]]).to(device)
                    
                    #Save result
                    batch_output = torch.cat((batch_output, decoder_input), dim=0)

                    
                #Mask results
                batch_pos = []

                for i in range(batch_output.size(1)):
                    try:
                        batch_pos.append((batch_output[:,i]==EOS_ID).nonzero().item())
                    except:
                        try:
                            batch_pos.append((batch_output[:,i]==PAD_ID).nonzero().item())
                        except:
                            batch_pos.append(batch_output.size(0)+1)
                
                #To list
                decoder_outputs += [batch_output[:,batch_pos[i]].tolist() for i in range(len(batch_pos))]

        true_outputs = [[str(i) for i in seq] for seq in true_outputs] 
        decoder_outputs = [[str(i) for i in seq] for seq in decoder_outputs] 
        return corpus_bleu(decoder_outputs, true_outputs, n_grams)

    def showLoss(self):
        plt.figure()
        fig = plt.figure(figsize=(10,6))
        fig_plt = sns.lineplot(x=np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), y=self.plot_losses)
        fig_plt.set_title("Loss Over Time")
        fig_plt.set_ylabel("Loss")
        fig_plt.set_xlabel("Epochs")
        return fig_plt.get_figure()
    
    def showScore(self):
        df = pd.concat([pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_train_scores, 'Score':'Train'}), 
                        pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_dev_scores, 'Score':'Dev'})], axis=0)
    
        plt.figure()
        pp = sns.lineplot(data=df, x = 'X', y = 'Y', hue='Score', style="Score", legend= "brief")
        fig_plt.set_title("Score Over Time")
        fig_plt.set_ylabel("Score")
        fig_plt.set_xlabel("Epoch")
        return fig_plt.get_figure()