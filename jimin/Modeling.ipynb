{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dfe3e807285a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"darkgrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paper\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils import asMinutes, timeSince, load_zipped_pickle, corpus_bleu, directories\n",
    "from langUtils import loadLangPairs, langDataset, langCollateFn, initHybridEmbeddings, tensorToList\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir, em_dir = directories()\n",
    "\n",
    "SPECIAL_SYMBOLS_ID = PAD_ID, UNK_ID, SOS_ID, EOS_ID = 0, 1, 2, 3\n",
    "NUM_SPECIAL = len(SPECIAL_SYMBOLS_ID)\n",
    "\n",
    "vi, en = loadLangPairs(\"vi\")\n",
    "\n",
    "class SortedList(list):\n",
    "    def insort(self, x):\n",
    "        bisect.insort(self, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(len(vi.train_num)) if (len(vi.train[i]) < vi.max_length) & (len(en.train[i]) < en.max_length)])\n",
    "overfit_dataset = langDataset([(vi.train_num[i], en.train_num[i]) for i in range(64)])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=False)\n",
    "dev_dataset = langDataset([(vi.dev_num[i], en.dev_num[i]) for i in range(len(vi.dev_num)) if (len(vi.dev[i]) < vi.max_length) & (len(en.dev[i]) < en.max_length)])\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=langCollateFn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Encoder RNN\n",
    "        Input\n",
    "            params - Dictionary of paramter\n",
    "            raw_emb - (100,000, 300) raw embeddings\n",
    "            learn_ids - list of ids to do embedding learning\n",
    "            \n",
    "            inp - (Max Length, Batch Size), original inputs\n",
    "            inp_lens - (Batch Size), true length of inputs\n",
    "        Output\n",
    "            output - (Max Length, Batch Size, Hidden Size), GRU output\n",
    "            hidden - (2, Batch Size, Hidden Size), Final hidden state of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "    \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        \n",
    "        #self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.embedding = nn.Embedding(100000, 300).to(device)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.n_layers, bidirectional=True)\n",
    "        \n",
    "    def forward(self, inp, inp_lens):\n",
    "        #Embed input\n",
    "        inp= inp.to(device)\n",
    "        embedded = self.embedding(inp)\n",
    "        #Pack padded\n",
    "        packed = pack_padded_sequence(embedded, inp_lens).to(device)\n",
    "        \n",
    "        #GRU\n",
    "        output, self.hidden = self.gru(packed)\n",
    "        #Pad packed\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        #Concat bidirectional layers\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        return output, self.hidden\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Decoder RNN\n",
    "        Input\n",
    "            params - Dictionary of paramter\n",
    "            raw_emb - (100,000, 300) raw embeddings\n",
    "            learn_ids - list of ids to do embedding learning\n",
    "            \n",
    "            inp - (1, Batch Size), SOS Token each in batch size\n",
    "            hidden - (2, 1, Hidden Size), Prev hidden size\n",
    "        Output\n",
    "            output - (32, Vocab Size), Probabilities\n",
    "            hidden - (2, 1 Size, Hidden Size), Final hidden state of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, params, raw_emb, learn_ids):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.n_layers = params['n_layers']\n",
    "        self.output_size = params['output_size']\n",
    "\n",
    "        #self.embedding = initHybridEmbeddings(raw_emb, learn_ids)\n",
    "        self.embedding = nn.Embedding(100000, 300).to(device)\n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, params['hidden_size'], self.n_layers, bidirectional=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_output=None):\n",
    "        #Embed\n",
    "        inp = inp.to(device)\n",
    "        embedded = self.embedding(inp)\n",
    "        #Dropout\n",
    "        output = F.relu(embedded)\n",
    "        \n",
    "        #Gru\n",
    "        output, self.hidden = self.gru(output, hidden)\n",
    "        #Concat directions\n",
    "        output = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n",
    "        #GRU output to probabilities\n",
    "        output = torch.exp(self.softmax(self.out(output))).squeeze(0)\n",
    "        return output, hidden\n",
    "\n",
    "def maskedLoss(inp, target, mask, criterion):\n",
    "    \"\"\"Masked Loss\"\"\"\n",
    "    #Total # real words\n",
    "    nTotal = mask.sum()\n",
    "    loss = criterion(inp, target.to(device))\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, encoder, decoder, encoder_optim, decoder_optim):\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.encoder_optim = encoder_optim\n",
    "        self.decoder_optim = decoder_optim\n",
    "        \n",
    "    def fit(self, train_data, dev_data, n_epoch, print_every, n_grams, criterion):\n",
    "        start = time.time()\n",
    "\n",
    "        self.n_epoch = n_epoch\n",
    "        \n",
    "        \n",
    "        print(\"Initializing...\")\n",
    "        start_epoch = 1\n",
    "        print_loss_total, plot_loss_total = 0 , 0\n",
    "        plot_losses, plot_train_scores, plot_dev_scores = [], [], []\n",
    "        \n",
    "        for epoch in range(start_epoch, n_epoch):\n",
    "            for i, (inp, inp_lens, output, out_mask, out_max) in enumerate(train_loader):\n",
    "                loss = self.trainEpoch(inp, inp_lens, output, out_mask, out_max, criterion)\n",
    "                \n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "                    \n",
    "                if i % print_every == 0:\n",
    "                    train_score = self.bleuScore(train_loader, n_grams)\n",
    "                    dev_score = self.bleuScore(dev_loader, n_grams)\n",
    "                    plot_train_scores.append(train_score)\n",
    "                    plot_dev_scores.append(dev_score)\n",
    "                    plot_loss_avg = plot_loss_total / print_every\n",
    "                    plot_losses.append(plot_loss_avg)\n",
    "                    plot_loss_total = 0       \n",
    "\n",
    "                    print_loss_avg = print_loss_total / print_every\n",
    "                    print_loss_total = 0\n",
    "                    print(\"Epoch:{} | Time Elapsed:{} | Percent Complete:{:.1} | Loss:{:.4} | TrainScore:{:.4} | DevScore:{:.4}\".format(epoch,\n",
    "                                                                                                                                  timeSince(start, epoch/n_epoch), \n",
    "                                                                                                                                  epoch/n_epoch*100, \n",
    "                                                                                                                                  print_loss_avg, \n",
    "                                                                                                                                  train_score,\n",
    "                                                                                                                                  dev_score))                \n",
    "\n",
    "        self.plot_losses = plot_losses\n",
    "        self.plot_train_scores = plot_train_scores\n",
    "        self.plot_dev_scores = plot_dev_scores\n",
    "        return \"Training Complete!\"            \n",
    "            \n",
    "    def trainEpoch(self, inp, inp_lens, output, out_mask, out_max, criterion):\n",
    "        #Zero gradients\n",
    "        self.encoder_optim.zero_grad()\n",
    "        self.decoder_optim.zero_grad()\n",
    "\n",
    "        #Loss vars\n",
    "        loss, print_losses, n_totals = 0, [], 0\n",
    "        \n",
    "        #Encoder Forward\n",
    "        encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "        \n",
    "        #Init decoder_input\n",
    "        decoder_input = torch.LongTensor([[SOS_ID for _ in range(inp.size(1))]]).to(device)\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]\n",
    "\n",
    "        #Teacher Forcing\n",
    "        for t in range(out_max):\n",
    "            #Decoder Forward\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            #True Output\n",
    "            decoder_input = output[t].view(1, -1)\n",
    "                \n",
    "            mask_loss, nTotal = maskedLoss(decoder_output, output[t], out_mask[t], criterion)\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "        \n",
    "        #self.encoder.embd.weight.data[] = 0\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.encoder_optim.step()\n",
    "        self.decoder_optim.step()\n",
    "\n",
    "        return sum(print_losses) / n_totals\n",
    "    \n",
    "    def bleuScore(self, data_loader, n_grams):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            true_outputs = []\n",
    "            decoder_outputs = []\n",
    "\n",
    "            for i, (inp, inp_lens, out, out_mask, out_max) in enumerate(data_loader):\n",
    "                if i * BATCH_SIZE > 10000:\n",
    "                    break\n",
    "\n",
    "                #Save true masked outputs\n",
    "                true_outputs += [out[:,i].masked_select(out_mask[:,i]).tolist() for i in range(out.size(1))]\n",
    "                \n",
    "                #Encoder forward\n",
    "                encoder_output, encoder_hidden = self.encoder(inp, inp_lens)\n",
    "                \n",
    "                #Decoder inputs\n",
    "                decoder_input = torch.LongTensor([[SOS_ID] * inp.size(1)]).to(device)\n",
    "                decoder_hidden = encoder_hidden[:self.decoder.n_layers* (2 *self.decoder.gru.bidirectional)]\n",
    "\n",
    "                #Decoder Results\n",
    "                batch_output = decoder_input.clone()\n",
    "                \n",
    "                for t in range(out_max):\n",
    "                    #Decoder forward\n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                    _, topi = decoder_output.topk(1)\n",
    "                    decoder_input = torch.LongTensor([[topi[i][0] for i in range(inp.size(1))]]).to(device)\n",
    "                    \n",
    "                    #Save result\n",
    "                    batch_output = torch.cat((batch_output, decoder_input), dim=0)\n",
    "\n",
    "                    \n",
    "                #Mask results\n",
    "                batch_pos = []\n",
    "\n",
    "                for i in range(batch_output.size(1)):\n",
    "                    try:\n",
    "                        batch_pos.append((batch_output[:,i]==EOS_ID).nonzero().item())\n",
    "                    except:\n",
    "                        try:\n",
    "                            batch_pos.append((batch_output[:,i]==PAD_ID).nonzero().item())\n",
    "                        except:\n",
    "                            batch_pos.append(batch_output.size(0)+1)\n",
    "                \n",
    "                #To list\n",
    "                decoder_outputs += [batch_output[:,batch_pos[i]].tolist() for i in range(len(batch_pos))]\n",
    "\n",
    "        true_outputs = [[str(i) for i in seq] for seq in true_outputs] \n",
    "        decoder_outputs = [[str(i) for i in seq] for seq in decoder_outputs] \n",
    "        return corpus_bleu(decoder_outputs, true_outputs, n_grams)\n",
    "\n",
    "    def showLoss(self):\n",
    "        plt.figure()\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        fig_plt = sns.lineplot(x=np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), y=self.plot_losses)\n",
    "        fig_plt.set_title(\"Loss Over Time\")\n",
    "        fig_plt.set_ylabel(\"Loss\")\n",
    "        fig_plt.set_xlabel(\"Epochs\")\n",
    "        return fig_plt.get_figure()\n",
    "    \n",
    "    def showScore(self):\n",
    "        df = pd.concat([pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_train_scores, 'Score':'Train'}), \n",
    "                        pd.DataFrame({'X':np.arange(0, self.n_epoch, int(self.n_epoch/len(self.plot_losses))), 'Y':self.plot_dev_scores, 'Score':'Dev'})], axis=0)\n",
    "    \n",
    "        plt.figure()\n",
    "        pp = sns.lineplot(data=df, x = 'X', y = 'Y', hue='Score', style=\"Score\", legend= \"brief\")\n",
    "        fig_plt.set_title(\"Score Over Time\")\n",
    "        fig_plt.set_ylabel(\"Score\")\n",
    "        fig_plt.set_xlabel(\"Epoch\")\n",
    "        return fig_plt.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "\n",
    "encoder_params = {'hidden_size':512, 'n_layers':1}\n",
    "decoder_params = {'hidden_size':encoder_params['hidden_size'], 'n_layers':1, 'output_size':en.n_words}\n",
    "attn_decoder_params = {'hidden_size':encoder_params['hidden_size'], 'n_layers':1, 'output_size':en.n_words, \"dropout\":0.1}\n",
    "\n",
    "encoder = EncoderRNN(encoder_params, vi.emb, vi.learn_ids).to(device)\n",
    "encoder_optim = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "decoder = DecoderRNN(decoder_params, en.emb, en.learn_ids).to(device)\n",
    "decoder_optim = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Epoch:1 | Time Elapsed:0m 25s (- 21m 4s) | Percent Complete:2e+00 | Loss:0.04605 | TrainScore:0.01387 | DevScore:0.04221\n",
      "Epoch:1 | Time Elapsed:2m 2s (- 99m 40s) | Percent Complete:2e+00 | Loss:11.43 | TrainScore:0.05241 | DevScore:0.09607\n",
      "Epoch:1 | Time Elapsed:3m 34s (- 175m 32s) | Percent Complete:2e+00 | Loss:11.44 | TrainScore:0.0353 | DevScore:0.04926\n",
      "Epoch:1 | Time Elapsed:4m 59s (- 244m 58s) | Percent Complete:2e+00 | Loss:11.44 | TrainScore:0.03012 | DevScore:0.02759\n",
      "Epoch:1 | Time Elapsed:6m 25s (- 315m 7s) | Percent Complete:2e+00 | Loss:11.43 | TrainScore:0.0293 | DevScore:0.07736\n",
      "Epoch:1 | Time Elapsed:7m 52s (- 385m 32s) | Percent Complete:2e+00 | Loss:11.43 | TrainScore:0.03562 | DevScore:0.08656\n",
      "Epoch:1 | Time Elapsed:9m 19s (- 457m 6s) | Percent Complete:2e+00 | Loss:11.42 | TrainScore:0.02864 | DevScore:0.06946\n",
      "Epoch:2 | Time Elapsed:10m 37s (- 255m 8s) | Percent Complete:4e+00 | Loss:9.729 | TrainScore:0.03531 | DevScore:0.04977\n",
      "Epoch:2 | Time Elapsed:12m 4s (- 289m 59s) | Percent Complete:4e+00 | Loss:11.42 | TrainScore:0.05356 | DevScore:0.06165\n",
      "Epoch:2 | Time Elapsed:13m 32s (- 324m 59s) | Percent Complete:4e+00 | Loss:11.42 | TrainScore:0.04406 | DevScore:0.0667\n",
      "Epoch:2 | Time Elapsed:14m 59s (- 359m 37s) | Percent Complete:4e+00 | Loss:11.41 | TrainScore:0.05086 | DevScore:0.0785\n",
      "Epoch:2 | Time Elapsed:16m 27s (- 394m 51s) | Percent Complete:4e+00 | Loss:11.41 | TrainScore:0.04998 | DevScore:0.05529\n",
      "Epoch:2 | Time Elapsed:17m 54s (- 429m 40s) | Percent Complete:4e+00 | Loss:11.41 | TrainScore:0.0714 | DevScore:0.09617\n",
      "Epoch:2 | Time Elapsed:19m 20s (- 464m 1s) | Percent Complete:4e+00 | Loss:11.41 | TrainScore:0.06496 | DevScore:0.08395\n",
      "Epoch:3 | Time Elapsed:20m 37s (- 323m 14s) | Percent Complete:6e+00 | Loss:9.719 | TrainScore:0.06984 | DevScore:0.06113\n",
      "Epoch:3 | Time Elapsed:22m 5s (- 346m 11s) | Percent Complete:6e+00 | Loss:11.41 | TrainScore:0.05964 | DevScore:0.12\n",
      "Epoch:3 | Time Elapsed:23m 32s (- 368m 48s) | Percent Complete:6e+00 | Loss:11.41 | TrainScore:0.07951 | DevScore:0.03923\n",
      "Epoch:3 | Time Elapsed:24m 57s (- 390m 57s) | Percent Complete:6e+00 | Loss:11.41 | TrainScore:0.0529 | DevScore:0.1219\n",
      "Epoch:3 | Time Elapsed:26m 24s (- 413m 40s) | Percent Complete:6e+00 | Loss:11.41 | TrainScore:0.05776 | DevScore:0.07886\n",
      "Epoch:3 | Time Elapsed:27m 46s (- 435m 13s) | Percent Complete:6e+00 | Loss:11.41 | TrainScore:0.08272 | DevScore:0.1145\n",
      "Epoch:3 | Time Elapsed:29m 9s (- 456m 46s) | Percent Complete:6e+00 | Loss:11.41 | TrainScore:0.04733 | DevScore:0.09668\n",
      "Epoch:4 | Time Elapsed:30m 27s (- 350m 12s) | Percent Complete:8e+00 | Loss:9.714 | TrainScore:0.05539 | DevScore:0.1098\n",
      "Epoch:4 | Time Elapsed:31m 52s (- 366m 38s) | Percent Complete:8e+00 | Loss:11.4 | TrainScore:0.07279 | DevScore:0.08099\n",
      "Epoch:4 | Time Elapsed:33m 18s (- 383m 5s) | Percent Complete:8e+00 | Loss:11.4 | TrainScore:0.04677 | DevScore:0.1012\n",
      "Epoch:4 | Time Elapsed:34m 46s (- 399m 58s) | Percent Complete:8e+00 | Loss:11.4 | TrainScore:0.05787 | DevScore:0.1048\n",
      "Epoch:4 | Time Elapsed:36m 10s (- 416m 3s) | Percent Complete:8e+00 | Loss:11.4 | TrainScore:0.07399 | DevScore:0.06427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3170f85ed8a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a1ed4245bacf>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, dev_data, n_epoch, print_every, n_grams, criterion)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a1ed4245bacf>\u001b[0m in \u001b[0;36mtrainEpoch\u001b[0;34m(self, inp, inp_lens, output, out_mask, out_max, criterion)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#self.encoder.embd.weight.data[] = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "model = Model(encoder, decoder, encoder_optim, decoder_optim)\n",
    "model.fit(train_loader, dev_loader,  n_epoch=50, print_every=250, n_grams=4, criterion=criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "input_sentence = []\n",
    "target = []\n",
    "for i, data in enumerate(overfit_dataset):\n",
    "    if i == 5:\n",
    "        input_sentence = data[0]\n",
    "        target = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder(input_sentence, len(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
